{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large ML Models Market Analysis Agent\n",
    "\n",
    "This notebook demonstrates the automated analysis of the most popular large ML models (such as LLMs) and their competitive aspects.\n",
    "\n",
    "The implementation is kept simple, leaving room for several potential improvements, such as:\n",
    "- Using reasoning steps (with LLM) to decide which information to gather rather than using a fixed workflow\n",
    "- Increasing the number of information providers\n",
    "- Engineering improvements (such as converting the notebook to a script, using requirements.txt for dependencies, Python type hints, etc)\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get `TAVILY_API_KEY` & `OPENAI_API_KEY` from .env file.\n",
    "%reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (0.27.1)\n",
      "Requirement already satisfied: tavily-python in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (1.59.3)\n",
      "Requirement already satisfied: arxiv in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: Jinja2 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: markdown in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (3.7)\n",
      "Requirement already satisfied: graphviz in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from tavily-python) (0.8.0)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from tavily-python) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from Jinja2) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from httpx->tavily-python) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from httpx->tavily-python) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub tavily-python openai arxiv beautifulsoup4 Jinja2 markdown graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from tavily import TavilyClient\n",
    "from openai import OpenAI\n",
    "import arxiv\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "from IPython.display import Markdown, Image, display\n",
    "from inspect import signature\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api = HfApi()\n",
    "tavily_client = TavilyClient(os.getenv(\"TAVILY_API_KEY\"))\n",
    "arxiv_client = arxiv.Client()\n",
    "llm_client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a directed graph\n",
    "dot = Digraph(comment='Large Models Market Analysis Agent', format='png')\n",
    "\n",
    "# Set default shape for all nodes to 'rectangle'\n",
    "dot.attr('node', shape='rect')\n",
    "\n",
    "# Define nodes\n",
    "dot.node('A', '< <b>Start</b>: <br/> <i>large_models_market_analysis_agent</i> >')\n",
    "dot.node('B', '< Fetch top trending models from HuggingFace Hub: <br/> <i>get_top_models</i> >')\n",
    "dot.node('C', '< Get model information from HuggingFace Hub: <br/> <i>get_model_info_from_hugging_face</i> >')\n",
    "dot.node('D', '< Search web by model name: <br/> <i>get_model_info_on_the_web</i> >')\n",
    "dot.node('E', '< Extract web links: <br/> <i>get_web_links_from_search_results</i> >')\n",
    "dot.node('F', '< Search web for arxiv papers mentioning the model: <br/> <i>get_model_info_on_arxiv</i> >')\n",
    "dot.node('G', '< Extract arxiv IDs: <br/> <i>get_arxiv_ids_from_search_results</i> >')\n",
    "dot.node('H', '< Get paper titles and URLs: <br/> <i>get_arxiv_links_by_id</i> >')\n",
    "dot.node('I', '< Call LLM to summarize and produce competitive points: <br/> <i>get_model_competitive_overview</i> >')\n",
    "dot.node('J', '< <b>Aggregate Results</b>: <br/> - Summary in tabular format <br/> - Detailed competitive information for each model >')\n",
    "\n",
    "# Define edges (workflow)\n",
    "dot.edge('A', 'B')\n",
    "dot.edge('B', 'C', label='For each model')\n",
    "dot.edge('B', 'D', label='For each model')\n",
    "dot.edge('D', 'E', label='Snippes from web pages')\n",
    "dot.edge('B', 'F', label='For each model')\n",
    "dot.edge('F', 'G', label='Snippes from web pages')\n",
    "dot.edge('G', 'H', label='Arxiv IDs')\n",
    "dot.edge('C', 'I', label='README file from model card')\n",
    "dot.edge('D', 'I', label='Snippets from web pages')\n",
    "\n",
    "dot.edge('C', 'J', label='model info from HuggingFace')\n",
    "dot.edge('E', 'J', label='web links (title+URL)')\n",
    "dot.edge('H', 'J', label='Arxiv papers (title+URL)')\n",
    "dot.edge('I', 'J', label='competitive summary')\n",
    "\n",
    "# Render the diagram to a byte stream and display it\n",
    "png_bytes = dot.pipe(format='png')\n",
    "display(Image(png_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tool_request(request):\n",
    "    print(f\"< REQ {request}\")\n",
    "\n",
    "def log_tool_response(response):\n",
    "    print(f\"> RSP {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tool_invocation(description=None):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Extract argument names and values\n",
    "            func_sig = signature(func)\n",
    "            bound_args = func_sig.bind(*args, **kwargs)\n",
    "            bound_args.apply_defaults()\n",
    "            args_str = \", \".join(f\"{key}={value}\" for key, value in bound_args.arguments.items())\n",
    "            \n",
    "            log_tool_request(f\"{description} {args_str}\")\n",
    "            \n",
    "            result = func(*args, **kwargs)\n",
    "            log_tool_response(result)\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving top large models from HuggingFace Hub\")\n",
    "def get_top_models(n = 10):\n",
    "    models = hf_api.list_models(sort=\"trending_score\", limit=n)\n",
    "    return [model.modelId for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_datetime_readable(dt):\n",
    "    return dt.strftime('%d %B %Y at %H:%M:%S %Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving model information from HuggingFace Hub\")\n",
    "def get_model_info_from_hugging_face(model_id):\n",
    "    model_info = hf_api.model_info(repo_id=model_id, expand=[\"createdAt\", \"downloads\", \"likes\", \"trendingScore\"])\n",
    "\n",
    "    description = read_file(hf_api.hf_hub_download(model_id, 'README.md'))\n",
    "\n",
    "    model_info = {\n",
    "        \"model_id\": model_id,\n",
    "        \"created_at\": format_datetime_readable(model_info.created_at),\n",
    "        \"downloads\": model_info.downloads,\n",
    "        \"likes\": model_info.likes,\n",
    "        \"trending_score\": model_info.trending_score,\n",
    "        \"description\": description\n",
    "        }\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving model information on the web using Tavily\")\n",
    "def get_model_info_on_the_web(model_id):\n",
    "    response = tavily_client.search(model_id)\n",
    "    return response['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving model information on arxiv documents using Tavily\")\n",
    "def get_model_info_on_arxiv(model_id):\n",
    "    response = tavily_client.search(model_id, include_domains=[\"arxiv.org\"], max_results=10)\n",
    "    return response['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arxiv_ids_from_search_results(search_results):\n",
    "    arxiv_ids = set()\n",
    "    for result in search_results:\n",
    "        url = result['url']\n",
    "\n",
    "        # Extract the ArXiv ID from the URL\n",
    "        match = re.search(r'arxiv\\.org/(abs|pdf|html)/([\\d.]+)(v\\d+)?', url)\n",
    "        if match:\n",
    "            arxiv_id = match.group(2)\n",
    "            arxiv_ids.add(arxiv_id)\n",
    "    return arxiv_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving paper information using Arxiv API\")\n",
    "def get_arxiv_links_by_id(arxiv_id):\n",
    "    search = arxiv.Search(id_list=[arxiv_id])\n",
    "    for paper in arxiv_client.results(search):\n",
    "        return f\"[{paper.title}]({paper})\" # assume there is only one paper\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Calling OpenAI gpt-4o-mini model\")\n",
    "def call_llm(system_prompt, message):\n",
    "    completion = llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]}\n",
    "        ])\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_markdown_formatting(markdown_content):\n",
    "    html_content = markdown.markdown(markdown_content)\n",
    "    \n",
    "    # Use BeautifulSoup to remove HTML tags and get plain text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text(separator='\\n', strip=True)\n",
    "    return text[:50_000] # trim so that LLM calls don't fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT = \"\"\"You are a helpful assistant that performs short and concise summarization of the large machine learning models.\n",
    "# Below you would see the contents of the README FILE (MODEL CARD), as well as the WEB SEARCH RESULTS for this model.\n",
    "# Use both of these sources to construct the summary, but prioritize web search results.\n",
    "# Use bullets to summarize the model competitive characteristics, and pay special attention to mention those characteristics which best differenciate this model from any others.\"\"\"\n",
    "\n",
    "# SYSTEM_PROMPT = \"\"\"Summarize using bullets.\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
    "\n",
    "Example of the required output:\n",
    "* Characteristic 1\n",
    "* Characteristic 2\n",
    "* Characteristic 3\n",
    "...\"\"\"\n",
    "\n",
    "def get_model_competitive_overview(model_description, model_web_info):\n",
    "    model_info = f\"# WEB SEARCH RESULTS\\n{model_web_info}\\n\\n# README FILE (MODEL CARD)\\n{remove_markdown_formatting(model_description)}\"\n",
    "    return call_llm(SYSTEM_PROMPT, model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_links_from_search_results(search_results):\n",
    "    return set([f\"[{result['title']}]({result['url']})\" for result in search_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the information from individual tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_models_market_analysis_agent():\n",
    "    top_models = get_top_models()\n",
    "\n",
    "    models_info = {}\n",
    "    for model_id in top_models:\n",
    "        models_info[model_id] = get_model_info_from_hugging_face(model_id)\n",
    "\n",
    "        models_info[model_id]['web_info'] = get_model_info_on_the_web(model_id)\n",
    "        models_info[model_id]['web_links'] = get_web_links_from_search_results(models_info[model_id]['web_info'])\n",
    "\n",
    "        arxiv_search_results = get_model_info_on_arxiv(model_id)\n",
    "        arxiv_ids = get_arxiv_ids_from_search_results(arxiv_search_results)\n",
    "        models_info[model_id]['papers'] = [get_arxiv_links_by_id(arxiv_id) for arxiv_id in arxiv_ids]\n",
    "\n",
    "        models_info[model_id]['competitive_overview'] = get_model_competitive_overview(models_info[model_id]['description'], models_info[model_id]['web_info'])\n",
    "\n",
    "    return models_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison_table(models_info):\n",
    "    required_columns = {\n",
    "        \"created_at\": \"Created At\",\n",
    "        \"downloads\": \"Total Downloads\",\n",
    "        \"likes\": \"Total Likes\",\n",
    "        \"trending_score\": \"Trending Score\",\n",
    "    }\n",
    "    \n",
    "    # Extract only the required columns\n",
    "    filtered_data = [\n",
    "        {**{\"Model Name\": model}, **{required_columns[key]: details.get(key) for key in required_columns}}\n",
    "        for model, details in models_info.items()\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_overview(models_info):\n",
    "    md_content = \"\"\n",
    "\n",
    "    for model_id, details in models_info.items():\n",
    "        md_content += f\"\\n# {model_id}\\n\\n\"\n",
    "        md_content += details['competitive_overview']\n",
    "        md_content += f\"\\n\\nMentioned in the following web pages:\\n\"\n",
    "        for web_link in details['web_links']:\n",
    "            md_content += f\"* {web_link}\\n\"\n",
    "        md_content += f\"\\nMentioned in the following papers:\\n\"\n",
    "        for paper in details['papers']:\n",
    "            md_content += f\"* {paper}\\n\"\n",
    "    return md_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the market analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< REQ Retrieving top large models from HuggingFace Hub n=10\n",
      "> RSP ['deepseek-ai/DeepSeek-V3', 'PowerInfer/SmallThinker-3B-Preview', 'deepseek-ai/DeepSeek-V3-Base', 'black-forest-labs/FLUX.1-dev', 'hexgrad/Kokoro-82M', 'meta-llama/Llama-3.3-70B-Instruct', 'StephanST/WALDO30', 'nomic-ai/modernbert-embed-base', 'cognitivecomputations/Dolphin3.0-Llama3.1-8B', 'stabilityai/stable-diffusion-3.5-large']\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=deepseek-ai/DeepSeek-V3\n",
      "> RSP {'model_id': 'deepseek-ai/DeepSeek-V3', 'created_at': '25 December 2024 at 12:52:23 UTC', 'downloads': 74084, 'likes': 1412, 'trending_score': 612, 'description': '<!-- markdownlint-disable first-line-h1 -->\\n<!-- markdownlint-disable html -->\\n<!-- markdownlint-disable no-duplicate-header -->\\n\\n<div align=\"center\">\\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\\n</div>\\n<hr>\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n\\n<p align=\"center\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>👁️</a>\\n</p>\\n\\n\\n## 1. Introduction\\n\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \\nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \\nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \\nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \\nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\\nIn addition, its training process is remarkably stable. \\nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/benchmark.png\">\\n</p>\\n\\n## 2. Model Summary\\n\\n---\\n\\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\\n\\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \\n    It can also be used for speculative decoding for inference acceleration. \\n\\n---\\n\\n**Pre-Training: Towards Ultimate Training Efficiency**\\n\\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \\n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \\n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \\n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\\n\\n---\\n\\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\\n\\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\\n\\n---\\n\\n\\n## 3. Model Downloads\\n\\n<div align=\"center\">\\n\\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\\n| :------------: | :------------: | :------------: | :------------: | :------------: |\\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\\n\\n</div>\\n\\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\\n\\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\\n\\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\\n\\n## 4. Evaluation Results\\n### Base Model\\n#### Standard Benchmarks\\n\\n<div align=\"center\">\\n\\n\\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\\n|---|-------------------|----------|--------|-------------|---------------|---------|\\n| | Architecture | - | MoE | Dense | Dense | MoE |\\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\\n| | # Total Params | - | 236B | 72B | 405B | 671B |\\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\\n\\n</div>\\n\\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\\nFor more evaluation details, please check our paper. \\n\\n#### Context Window\\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/niah.png\">\\n</p>\\n\\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \\n\\n### Chat Model\\n#### Standard Benchmarks (Models larger than 67B)\\n<div align=\"center\">\\n\\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\\n\\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\\n\\n</div>\\n\\n\\n####  Open Ended Generation Evaluation\\n\\n<div align=\"center\">\\n\\n\\n\\n| Model | Arena-Hard | AlpacaEval 2.0 |\\n|-------|------------|----------------|\\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\\n| LLaMA-3.1 405B | 69.3 | 40.5 |\\n| GPT-4o-0513 | 80.4 | 51.1 |\\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\\n| DeepSeek-V3 | **85.5** | **70.0** |\\n\\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\\n</div>\\n\\n\\n## 5. Chat Website & API Platform\\nYou can chat with DeepSeek-V3 on DeepSeek\\'s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\\n\\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\\n\\n## 6. How to Run Locally\\n\\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\\n\\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\\n\\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\\n\\nHere is an example of converting FP8 weights to BF16:\\n\\n```shell\\ncd inference\\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\\n```\\n\\n**NOTE: Huggingface\\'s Transformers has not been directly supported yet.**\\n\\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\\n\\n#### Model Weights & Demo Code Preparation\\n\\nFirst, clone our DeepSeek-V3 GitHub repository:\\n\\n```shell\\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\\n```\\n\\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\\n\\n```shell\\ncd DeepSeek-V3/inference\\npip install -r requirements.txt\\n```\\n\\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\\n\\n#### Model Weights Conversion\\n\\nConvert HuggingFace model weights to a specific format:\\n\\n```shell\\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\\n```\\n\\n#### Run\\n\\nThen you can chat with DeepSeek-V3:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\\n```\\n\\nOr batch inference on a given file:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\\n```\\n\\n### 6.2 Inference with SGLang (recommended)\\n\\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\\n\\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\\n\\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\\n\\n### 6.3 Inference with LMDeploy (recommended)\\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\\n\\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\\n\\n\\n### 6.4 Inference with TRT-LLM (recommended)\\n\\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \\n\\n### 6.5 Inference with vLLM (recommended)\\n\\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\\n\\n### 6.6 Recommended Inference Functionality with AMD GPUs\\n\\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\\n\\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\\n\\n\\n## 7. License\\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\\n\\n## 8. Citation\\n```\\n@misc{deepseekai2024deepseekv3technicalreport,\\n      title={DeepSeek-V3 Technical Report}, \\n      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},\\n      year={2024},\\n      eprint={2412.19437},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.19437}, \\n}\\n```\\n\\n## 9. Contact\\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\\n'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=deepseek-ai/DeepSeek-V3\n",
      "> RSP [{'title': 'DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat', 'url': 'https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/', 'content': 'DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch | VentureBeat DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B.', 'score': 0.8727061, 'raw_content': None}, {'title': \"DeepSeek's new AI model appears to be one of the best 'open ...\", 'url': 'https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/', 'content': \"DeepSeek's new AI model appears to be one of the best 'open' challengers yet | TechCrunch DeepSeek's new AI model appears to be one of the best 'open' challengers yet | TechCrunch DeepSeek’s new AI model appears to be one of the best ‘open’ challengers yet The model, DeepSeek V3, was developed by the AI firm DeepSeek and was released on Wednesday under a permissive license that allows developers to download and modify it for most applications, including commercial ones. According to DeepSeek’s internal benchmark testing, DeepSeek V3 outperforms both downloadable, “openly” available models and “closed” AI models that can only be accessed through an API.\", 'score': 0.86108357, 'raw_content': None}, {'title': 'DeepSeek V3: New Open AI Model Surpasses Rivals and ... - WinBuzzer', 'url': 'https://winbuzzer.com/2024/12/27/deepseek-v3-new-open-ai-model-surpasses-rivals-and-challenges-gpt-4o-xcxwbn/', 'content': 'DeepSeek V3: New Open AI Model Surpasses Rivals and Challenges GPT-4o - WinBuzzer DeepSeek V3: New Open AI Model Surpasses Rivals and Challenges GPT-4o DeepSeek V3 is an open-source AI model that outperformes Meta’s Llama 3.1 and comes close to OpenAI’s GPT-4o in key benchmarks. DeepSeek V3’s technical advancements place it among the most powerful AI systems to, rivaling both open-source competitors like Meta’s Llama 3.1 and proprietary models like OpenAI’s GPT-4o. DeepSeek V3’s benchmark results showcase its exceptional capabilities across a broad spectrum of tasks, solidifying its position as a leader among open-source AI models. Related: DeepSeek AI Open Sources VL2 Series of Vision Language Models', 'score': 0.8593928, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-', 'score': 0.8479807, 'raw_content': None}, {'title': 'Introducing DeepSeek-V3 | DeepSeek API Docs', 'url': 'https://api-docs.deepseek.com/news/news1226', 'content': '🚀 Introducing DeepSeek-V3 | DeepSeek API Docs DeepSeek API Docs DeepSeek Platform Your First API Call Introducing DeepSeek-V3 2024/12/26 DeepSeek-V2.5-1210 Release 2024/12/10 DeepSeek-V2.5 Release 2024/09/05 Context Caching is Available 2024/08/02 New API Features 2024/07/25 API Reference API Guides Context Caching API Status Page Introducing DeepSeek-V3 2024/12/26 🚀 Introducing DeepSeek-V3 ⚡ 60 tokens/second (3x faster than V2!) Model 👉 https://github.com/deepseek-ai/DeepSeek-V3 Paper 👉 https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf 💰 API Pricing Update\\u200b 🎉 Until Feb 8: same as V2! | Input (cache miss) | Input (cache hit) | Output | Look forward to multimodal support and other cutting-edge features in the DeepSeek ecosystem. Previous Error CodesNext 🚀 DeepSeek V2.5: The Grand Finale 🎉 🎉 What’s new in V3 💰 API Pricing Update Copyright © 2024 DeepSeek, Inc.', 'score': 0.780947, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=deepseek-ai/DeepSeek-V3\n",
      "> RSP [{'title': '[2412.19437] DeepSeek-V3 Technical Report - export.arxiv.org', 'url': 'http://export.arxiv.org/abs/2412.19437', 'content': 'cs > arXiv:2412.19437 cs.CL cs cs.AI We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite\\xa0as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) Which authors of this paper are endorsers?', 'score': 0.72907686, 'raw_content': None}, {'title': '[2412.19437] DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/abs/2412.19437', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. arXiv:2412.19437 arXiv author ID DeepSeek-V3 Technical Report We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.19437 Bibliographic and Citation Tools Connected Papers Toggle\", 'score': 0.65252984, 'raw_content': None}, {'title': 'arXiv:2401.02954v1 [cs.CL] 5 Jan 2024', 'url': 'https://arxiv.org/pdf/2401.02954', 'content': 'n\\n0007315414 00000 n\\n0007315565 00000 n\\n0005088444 00000 n\\n0007315714 00000 n\\n0007315777 00000 n\\n0007315840 00000 n\\n0007316008 00000 n\\n0007316159 00000 n\\n0005091379 00000 n\\n0007316300 00000 n\\n0005095039 00000 n\\n0005095316 00000 n\\n0005095595 00000 n\\n0005095874 00000 n\\n0005096153 00000 n\\n0005096431 00000 n\\n0005096709 00000 n\\n0005096987 00000 n\\n0005097266 00000 n\\n0005097544 00000 n\\n0005097822 00000 n\\n0005098100 00000 n\\n0005098378 00000 n\\n0005098656 00000 n\\n0005098934 00000 n\\n0005099213 00000 n\\n0005099492 00000 n\\n0005099769 00000 n\\n0005100048 00000 n\\n0005100327 00000 n\\n0005100605 00000 n\\n0005100882 00000 n\\n0005101160 00000 n\\n0005101438 00000 n\\n0005101716 00000 n\\n0005101994 00000 n\\n0005102270 00000 n\\n0005102548 00000 n\\n0005102826 00000 n\\n0005103103 00000 n\\n0005103381 00000 n\\n0005103660 00000 n\\n0007316363 00000 n\\n0007316531 00000 n\\n0007316699 00000 n\\n0007317326 00000 n\\n0005103938 00000 n\\n0007317475 00000 n\\n0007317538 00000 n\\n0007317601 00000 n\\n0007317766 00000 n\\n0007317932 00000 n\\n0007318097 00000 n\\n0007318262 00000 n\\n0007318429 00000 n\\n0007318594 00000 n\\n0007318759 00000 n\\n0007318924 00000 n\\n0007319089 00000 n\\n0007319254 00000 n\\n0007319420 00000 n\\n0007319586 00000 n\\n0007319751 00000 n\\n0007319918 00000 n\\n0007320083 00000 n\\n0007320250 00000 n\\n0007320417 00000 n\\n0007320582 00000 n\\n0007320748 00000 n\\n0007320914 00000 n\\n0007321080 00000 n\\n0007321246 00000 n\\n0007321412 00000 n\\n0007321577 00000 n\\n0007321742 00000 n\\n0007321938 00000 n\\n0007322102 00000 n\\n0007322268 00000 n\\n0007322433 00000 n\\n0007322598 00000 n\\n0007322763 00000 n\\n0007322930 00000 n\\n0007323097 00000 n\\n0005149229 00000 n\\n0007323160 00000 n\\n0007323330 00000 n\\n0007323496 00000 n\\n0007323662 00000 n\\n0007323830 00000 n\\n0007323995 00000 n\\n0007324159 00000 n\\n0007324323 00000 n\\n0007324485 00000 n\\n0007324648 00000 n\\n0007324811 00000 n\\n0007324975 00000 n\\n0007325139 00000 n\\n0007325304 00000 n\\n0005111289 00000 n\\n0007325533 00000 n\\n0007325596 00000 n\\n0007325659 00000 n\\n0007325722 00000 n\\n0007325785 00000 n\\n0007325848 00000 n\\n0007326013 00000 n\\n0007326178 00000 n\\n0007326340 00000 n\\n0007326502 00000 n\\n0007326663 00000 n\\n0007326824 00000 n\\n0007326991 00000 n\\n0005114771 00000 n\\n0005115049 00000 n\\n0005115327 00000 n\\n0005115605 00000 n\\n0005115884 00000 n\\n0005116163 00000 n\\n0005116440 00000 n\\n0005116718 00000 n\\n0005116997 00000 n\\n0005117275 00000 n\\n0005117554 00000 n\\n0005117831 00000 n\\n0005118109 00000 n\\n0005118388 00000 n\\n0005118667 00000 n\\n0005118945 00000 n\\n0005119223 00000 n\\n0005119501 00000 n\\n0005119779 00000 n\\n0005120058 00000 n\\n0005120336 00000 n\\n0005120614 00000 n\\n0005120891 00000 n\\n0005121169 00000 n\\n0007327159 00000 n\\n0007327328 00000 n\\n0007327479 00000 n\\n0005121447 00000 n\\n0007327676 00000 n\\n0007327739 00000 n\\n0007327802 00000 n\\n0007327922 00000 n\\n0007327985 00000 n\\n0007328155 00000 n\\n0007328325 00000 n\\n0007328495 00000 n\\n0007328665 00000 n\\n0007328834 00000 n\\n0007329003 00000 n\\n0007329812 00000 n\\n0005124485 00000 n\\n0007329985 00000 n\\n0007330048 00000 n\\n0007330111 00000 n\\n0007330277 00000 n\\n0007330443 00000 n\\n0007330609 00000 n\\n0007330775 00000 n\\n0007330942 00000 n\\n0007331108 00000 n\\n0007331274 00000 n\\n0007331438 00000 n\\n0007331604 00000 n\\n0007331771 00000 n\\n0007331937 00000 n\\n0007332103 00000 n\\n0007332268 00000 n\\n0007332434 00000 n\\n0007332600 00000 n\\n0007332766 00000 n\\n0007332932 00000 n\\n0007333098 00000 n\\n0007333263 00000 n\\n0007333430 00000 n\\n0007333596 00000 n\\n0007333763 00000 n\\n0007333929 00000 n\\n0007334094 00000 n\\n0007334157 00000 n\\n0007334220 00000 n\\n0007334390 00000 n\\n0007334560 00000 n\\n0007334711 00000 n\\n0005129191 00000 n\\n0007334876 00000 n\\n0007334939 00000 n\\n0007335002 00000 n\\n0007335064 00000 n\\n0007335232 00000 n\\n0007335397 00000 n\\n0005132062 00000 n\\n0007335538 00000 n\\n0007335601 00000 n\\n0007335663 00000 n\\n0007335882 00000 n\\n0007336091 00000 n\\n0007336242 00000 n\\n0005135187 00000 n\\n0007336391 00000 n\\n0007336454 00000 n\\n0007336646 00000 n\\n0007336864 00000 n\\n0007337065 00000 n\\n0007337264 00000 n\\n0007337482 00000 n\\n0007337684 00000 n\\n0007337821 00000 n\\n0005138801 00000 n\\n0007337994 00000 n\\n0007338057 00000 n\\n0007338290 00000 n\\n0005144461 00000 n\\n0005144739 00000 n\\n0005145018 00000 n\\n0007338500 00000 n\\n0007338710 00000 n\\n0007338920 00000 n\\n0007339130 00000 n\\n0007339340 00000 n\\n0007339659 00000 n\\n0005145296 00000 n\\n0007339872 00000 n\\n0007339935 00000 n\\n0007340167 00000 n\\n0007340230 00000 n\\n0007340436 00000 n\\n0007340602 00000 n\\n0007340767 00000 n\\n0007340934 00000 n\\n0007341143 00000 n\\n0007341206 00000 n\\n0007341415 00000 n\\n0006175493 00000 n\\n0007341535 00000 n\\n0007341737 00000 n\\n0007341938 00000 n\\n0007342143 00000 n\\n0007342348 00000 n\\n0007342556 00000 n\\n0007342693 00000 n\\n0005152288 00000 n\\n0007342874 00000 n\\n0007342937 00000 n\\n0007343141 00000 n\\n0007343341 00000 n\\n0007343478 00000 n\\n0005157208 00000 n\\n0007343619 00000 n\\n0007343682 00000 n\\n0007343745 00000 n\\n0007343955 00000 n\\n0007344163 00000 n\\n0007344438 00000 n\\n0007344654 00000 n\\n0007344886 00000 n\\n0007345023 00000 n\\n0005161745 00000 n\\n0007345204 00000 n\\n0007345267 00000 n\\n0007345475 00000 n\\n0007345750 00000 n\\n0007345959 00000 n\\n0007346164 00000 n\\n0007346369 00000 n\\n0007346578 00000 n\\n0007346715 00000 n\\n0005166165 00000 n\\n0007346904 00000 n\\n0007346967 00000 n\\n0007347030 00000 n\\n0007347238 00000 n\\n0007347442 00000 n\\n0007347607 00000 n\\n0005173001 00000 n\\n0005510940 00000 n\\n0005853150 00000 n\\n0007347778 00000 n\\n0007348022 00000 n\\n0005170325 00000 n\\n0007348186 00000 n\\n0007348249 00000 n\\n0007348312 00000 n\\n0007348375 00000 n\\n0007348438 00000 n\\n0005501997 00000 n\\n0005844207 00000 n\\n0006166550 00000 n\\n0007348501 00000 n\\n0006563966 00000 n\\n0006179210 00000 n\\n0007348535 00000 n\\n0007348705 00000 n\\n0007348874 00000 n\\n0007349076 00000 n\\n0006177718 00000 n\\n0007349232 00000 n\\n0007349295 00000 n\\n0007349357 00000 n\\n0007349420 00000 n\\n0006526370 00000 n\\n0007349540 00000 n\\n0007349710 00000 n\\n0007349874 00000 n\\n0007350038 00000 n\\n0007350219 00000 n\\n0007350400 00000 n\\n0007350563 00000 n\\n0007350726 00000 n\\n0007350897 00000 n\\n0007351068 00000 n\\n0007351231 00000 n\\n0007351391 00000 n\\n0007351558 00000 n\\n0007351725 00000 n\\n0007351894 00000 n\\n0007352063 00000 n\\n0007352225 00000 n\\n0007352387 00000 n\\n0007352554 00000 n\\n0007352721 00000 n\\n0006531289 00000 n\\n0006531568 00000 n\\n0006531847 00000 n\\n0006532126 00000 n\\n0006532403 00000 n\\n0006532681 00000 n\\n0006532959 00000 n\\n0006533238 00000 n\\n0006533517 00000 n\\n0006533796 00000 n\\n0006534075 00000 n\\n0006534353 00000 n\\n0006534631 00000 n\\n0006534909 00000 n\\n0006535188 00000 n\\n0006535466 00000 n\\n0006535745 00000 n\\n0006536024 00000 n\\n0007352889 00000 n\\n0007353040 00000 n\\n0006536303 00000 n\\n0007353341 00000 n\\n0007353404 00000 n\\n0007353467 00000 n\\n0007353530 00000 n\\n0007353592 00000 n\\n0007353656 00000 n\\n0007354325 00000 n\\n0006538595 00000 n\\n0007354446 00000 n\\n0007354509 00000 n\\n0007354675 00000 n\\n0007354841 00000 n\\n0007355007 00000 n\\n0007355171 00000 n\\n0007355336 00000 n\\n0007355503 00000 n\\n0007355669 00000 n\\n0007355836 00000 n\\n0007356003 00000 n\\n0007356066 00000 n\\n0007356217 00000 n\\n0006540424 00000 n\\n0007356338 00000 n\\n0007356401 00000 n\\n0007356464 00000 n\\n0007357427 00000 n\\n0006541782 00000 n\\n0007357548 00000 n\\n0007357611 00000 n\\n0007357672 00000 n\\n0007357838 00000 n\\n0007358004 00000 n\\n0007358883 00000 n\\n0006544062 00000 n\\n0007359004 00000 n\\n0007359067 00000 n\\n0007359130 00000 n\\n0007359297 00000 n\\n0007359359 00000 n\\n0007359525 00000 n\\n0007359692 00000 n\\n0007360557 00000 n\\n0006545580 00000 n\\n0007360678 00000 n\\n0007360741 00000 n\\n0007360804 00000 n\\n0007360969 00000 n\\n0007361136 00000 n\\n0007361256 00000 n\\n0007362037 00000 n\\n0006549276 00000 n\\n0007362158 00000 n\\n0007362221 00000 n\\n0007362284 00000 n\\n0007362449 00000 n\\n0007362616 00000 n\\n0007363425 00000 n\\n0006550661 00000 n\\n0007363546 00000 n\\n0007363609 00000 n\\n0007363672 00000 n\\n0007363735 00000 n\\n0007364740 00000 n\\n0006552618 00000 n\\n0007364861 00000 n\\n0007364924 00000 n\\n0007364986 00000 n\\n0007365137 00000 n\\n0006554967 00000 n\\n0007365258 00000 n\\n0007365321 00000 n\\n0007365384 00000 n\\n0007365535 00000 n\\n0006557886 00000 n\\n0007365656 00000 n\\n0007365719 00000 n\\n0007365782 00000 n\\n0007365845 00000 n\\n0007365996 00000 n\\n0006558584 00000 n\\n0007366117 00000 n\\n0007366180 00000 n\\n0007366243 00000 n\\n0007366364 00000 n\\n0007366515 00000 n\\n0006560251 00000 n\\n0007366636 00000 n\\n0007366699 00000 n\\n0007366761 00000 n\\n0007366912 00000 n\\n0006561609 00000 n\\n0007367033 00000 n\\n0007367096 00000 n\\n0007367159 00000 n\\n0007367310 00000 n\\n0006562717 00000 n\\n0007367431 00000 n\\n0007367494 00000 n\\n0007367557 00000 n\\n0007367620 00000 n\\n0006571237 00000 n\\n0007367683 00000 n\\n0007367834 00000 n\\n0006566959 00000 n\\n0007367955 00000 n\\n0007368019 00000 n\\n0007368083 00000 n\\n0007368235 00000 n\\n0006569526 00000 n\\n0007368359 00000 n\\n0007368424 00000 n\\n0007368489 00000 n\\n0007368516 00000 n\\n0007369045 00000 n\\n0007369172 00000 n\\n0007369199 00000 n\\n0007369226 00000 n\\n0007369578 00000 n\\n0007369709 00000 n\\n0007370384 00000 n\\n0007370949 00000 n\\n0007370976 00000 n\\n0007371003 00000 n\\n0007371548 00000 n\\n0007371979 00000 n\\n0007372596 00000 n\\n0007372664 00000 n\\n0007373193 00000 n\\n0007373405 00000 n\\n0007373986 00000 n\\n0007374328 00000 n\\n0007375017 00000 n\\n0007375453 00000 n\\n0007375768 00000 n\\n0007376102 00000 n\\n0007376129 00000 n\\n0007376824 00000 n\\n0007376851 00000 n\\n0007377594 00000 n\\n0007377991 00000 n\\n0007378857 00000 n\\n0007379237 00000 n\\n0007379903 00000 n\\n0007380512 00000 n\\n0007380914 00000 n\\n0007381531 00000 n\\n0007381957 00000 n\\n0007382716 00000 n\\n0007383019 00000 n\\n0007383425 00000 n\\n0007383856 00000 n\\n0007384140 00000 n\\n0007384340 00000 n\\n0007384985 00000 n\\n0007385429 00000 n\\n0007385928 00000 n\\n0007386644 00000 n\\n0007387193 00000 n\\n0007387775 00000 n\\n0007388221 00000 n\\n0007388612 00000 n\\n0007389441 00000 n\\n0007390074 00000 n\\n0007390487 00000 n\\n0007391014 00000 n\\n0007391156 00000 n\\n0007391879 00000 n\\n0007392233 00000 n\\n0007393130 00000 n\\n0007393355 00000 n\\n0007394241 00000 n\\n0007395035 00000 n\\n0007395760 00000 n\\n0007396434 00000 n\\n0007397126 00000 n\\n0007398036 00000 n\\n0007398494 00000 n\\n0007398793 00000 n\\n0007399611 00000 n\\n0007400210 00000 n\\n0007401046 00000 n\\n0007401647 00000 n\\n0007402270 00000 n\\n0007403090 00000 n\\n0007403779 00000 n\\n0007404582 00000 n\\n0007404609 00000 n\\n0007404925 00000 n\\n0007405399 00000 n\\n0007406036 00000 n\\n0007406745 00000 n\\n0007407353 00000 n\\n0007408323 00000 n\\n0007408642 00000 n\\n0007408668 00000 n\\n0007408966 00000 n\\n0007182892 00000 n\\n0007409746 00000 n\\n0007410688 00000 n\\n0006574950 00000 n\\n0007411168 00000 n\\n0006581515 00000 n\\n0007411510 00000 n\\n0006583679 00000 n\\n0007411757 00000 n\\n0006599029 00000 n\\n0007412314 00000 n\\n0006608793 00000 n\\n0007412661 00000 n\\n0006615140 00000 n\\n0007412932 00000 n\\n0006625783 00000 n\\n0007413333 00000 n\\n0006636954 00000 n\\n0007413706 00000 n\\n0006650045 00000 n\\n0007414169 00000 n\\n0006659549 00000 n\\n0007414530 00000 n\\n0006663331 00000 n\\n0007414767 00000 n\\n0006668489 00000 n\\n0007415036 00000 n\\n0006678438 00000 n\\n0007415379 00000 n\\n0006681647 00000 n\\n0007415606 00000 n\\n0006688918 00000 n\\n0007415935 00000 n\\n0006692082 00000 n\\n0007416162 00000 n\\n0006701373 00000 n\\n0007416509 00000 n\\n0006706867 00000 n\\n0007416796 00000 n\\n0006711402 00000 n\\n0007417073 00000 n\\n0006719918 00000 n\\n0007417398 00000 n\\n0006730409 00000 n\\n0007417755 00000 n\\n0006739484 00000 n\\n0007418082 00000 n\\n0006742031 00000 n\\n0007418305 00000 n\\n0006751174 00000 n\\n0007418642 00000 n\\n0006763129 00000 n\\n0007418981 00000 n\\n0006765762 00000 n\\n0007419204 00000 n\\n0006775354 00000 n\\n0007419547 00000 n\\n0006781094 00000 n\\n0007419824 00000 n\\n0006788534 00000 n\\n0007420143 00000 n\\n0006794642 00000 n\\n0007420414 00000 n\\n0006797269 00000 n\\n0007420635 00000 n\\n0006804430 00000 n\\n0007420936 00000 n\\n0006809669 00000 n\\n0007421217 00000 n\\n0006815038 00000 n\\n0007421480 00000 n\\n0006818724 00000 n\\n0007421715 00000 n\\n0006822371 00000 n\\n0007421942 00000 n\\n0006826448 00000 n\\n0007422193 00000 n\\n0006828908 00000 n\\n0007422414 00000 n\\n0006834494 00000 n\\n0007422693 00000 n\\n0006839535 00000 n\\n0007422954 00000 n\\n0006843919 00000 n\\n0007423199 00000 n\\n0006850711 00000 n\\n0007423502 00000 n\\n0006856978 00000 n\\n0007423793 00000 n\\n0006862452 00000 n\\n0007424048 00000 n\\n0006865493 00000 n\\n0007424275 00000 n\\n0006871906 00000 n\\n0007424562 00000 n\\n0006878567 00000 n\\n0007424839 00000 n\\n0006886398 00000 n\\n0007425134 00000 n\\n0006891532 00000 n\\n0007425393 00000 n\\n0006904155 00000 n\\n0007425748 00000 n\\n0006910039 00000 n\\n0007426013 00000 n\\n0006919394 00000 n\\n0007426360 00000 n\\n0006925164 00000 n\\n0007426631 00000 n\\n0006931399 00000 n\\n0007426916 00000 n\\n0006935136 00000 n\\n0007427153 00000 n\\n0006939056 00000 n\\n0007427388 00000 n\\n0006941901 00000 n\\n0007427613 00000 n\\n0006944271 00000 n\\n0007427834 00000 n\\n0006946924 00000 n\\n0007428057 00000 n\\n0006952453 00000 n\\n0007428318 00000 n\\n0006958951 00000 n\\n0007428601 00000 n\\n0006961863 00000 n\\n0007428832 00000 n\\n0006973749 00000 n\\n0007429211 00000 n\\n0006978457 00000 n\\n0007429458 00000 n\\n0006987794 00000 n\\n0007429777 00000 n\\n0006990670 00000 n\\n0007430008 00000 n\\n0007000068 00000 n\\n0007430343 00000 n\\n0007009521 00000 n\\n0007430654 00000 n\\n0007013227 00000 n\\n0007430899 00000 n\\n0007016335 00000 n\\n0007431126 00000 n\\n0007021225 00000 n\\n0007431381 00000 n\\n0007028221 00000 n\\n0007431662 00000 n\\n0007032889 00000 n\\n0007431905 00000 n\\n0007037012 00000 n\\n0007432158 00000 n\\n0007039971 00000 n\\n0007432389 00000 n\\n0007045905 00000 n\\n0007432650 00000 n\\n0007048304 00000 n\\n0007432873 00000 n\\n0007051312 00000 n\\n0007433100 00000 n\\n0007053807 00000 n\\n0007433371 00000 n\\n0007081427 00000 n\\n0007433774 00000 n\\n0007091000 00000 n\\n0007434003 00000 n\\n0007096507 00000 n\\n0007434318 00000 n\\n0007119773 00000 n\\n0007434772 00000 n\\n0007132638 00000 n\\n0007435091 00000 n\\n0007160339 00000 n\\n0007435841 00000 n\\n0007176421 00000 n\\n0007177166 00000 n\\n0007177643 00000 n\\n0007178415 00000 n\\n0007179198 00000 n\\n0007179858 00000 n\\n0007180594 00000 n\\n0007181319 00000 n\\n0007182097 00000 n\\n0007189258 00000 n\\n0007240991 00000 n\\n0007190039 00000 n\\n0007190725 00000 n\\n0007191445 00000 n\\n0007192097 00000 n\\n0007192864 00000 n\\n0007193536 00000 n\\n0007194229 00000 n\\n0007194972 00000 n\\n0007195599 00000 n\\n0007196309 00000 n\\n0007197107 00000 n\\n0007197880 00000 n\\n0007198513 00000 n\\n0007199283 00000 n\\n0007200058 00000 n\\n0007200766 00000 n\\n0007201508 00000 n\\n0007202237 00000 n\\n0007203044 00000 n\\n0007203761 00000 n\\n0007204333 00000 n\\n0007204996 00000 n\\n0007205796 00000 n\\n0007206597 00000 n\\n0007207337 00000 n\\n0007207996 00000 n\\n0007208650 00000 n\\n0007209284 00000 n\\n0007209992 00000 n\\n0007210694 00000 n\\n0007211359 00000 n\\n0007212136 00000 n\\n0007212895 00000 n\\n0007213627 00000 n\\n0007214355 00000 n\\n0007215048 00000 n\\n0007215724 00000 n\\n0007216501 00000 n\\n0007217140 00000 n\\n0007217656 00000 n\\n0007218361 00000 n\\n0007219113 00000 n\\n0007219823 00000 n\\n0007220598 00000 n\\n0007221345 00000 n\\n0007222057 00000 n\\n0007222692 00000 n\\n0007223356 00000 n\\n0007224088 00000 n\\n0007224809 00000 n\\n0007225400 00000 n\\n0007225849 00000 n\\n0007226362 00000 n\\n0007226890 00000 n\\n0007227545 00000 n\\n0007228194 00000 n\\n0007228804 00000 n\\n0007229586 00000 n\\n0007230224 00000 n\\n0007230738 00000 n\\n0007231294 00000 n\\n0007232020 00000 n\\n0007232671 00000 n\\n0007233180 00000 n\\n0007233702 00000 n\\n0007234272 00000 n\\n0007234970 00000 n\\n0007235498 00000 n\\n0007235929 00000 n\\n0007236701 00000 n\\n0007237474 00000 n\\n0007238246 00000 n\\n0007239019 00000 n\\n0007239937 00000 n\\n0007436223 00000 n\\n0007436345 00000 n\\n0007436435 00000 n\\n0007436510 00000 n\\n0007244520 00000 n\\n0007436585 00000 n\\n0007436787 00000 n\\n0007437007 00000 n\\n0007437238 00000 n\\n0007437478 00000 n\\n0007437736 00000 n\\n0007437971 00000 n\\n0007438240 00000 n\\n0007438473 00000 n\\n0007438693 00000 n\\n0007438922 00000 n\\n0007439190 00000 n\\n0007439447 00000 n\\n0007439686 00000 n\\n0007439923 00000 n\\n0007440172 00000 n\\n0007440345 00000 n\\n0007440518 00000 n\\n0007440693 00000 n\\n0007440867 00000 n\\n0007441042 00000 n\\n0007441216 00000 n\\n0007441392 00000 n\\n0007441562 00000 n\\n0007441746 00000 n\\n0007441969 00000 n\\n0007442192 00000 n\\n0007442416 00000 n\\n0007442641 00000 n\\n0007442870 00000 n\\n0007443119 00000 n\\n0007443366 00000 n\\n0007443612 00000 n\\n0007443859 00000 n\\n0007444106 00000 n\\n0007444353 00000 n\\n0007444602 00000 n\\n0007444765 00000 n\\n0007444907 00000 n\\n0007445053 00000 n\\n0007445176 00000 n\\n0007445298 00000 n\\n0007445432 00000 n\\n0007445570 00000 n\\n0007445662 00000 n\\n0007445793 00000 n\\n0007445885 00000 n\\n0007445979 00000 n\\n0007446019 00000 n\\n0007244205 00000 n\\n0007247361 00000 n\\n0007446152 00000 n\\n0007446329 00000 n\\ntrailer\\n<< /ID /arXivStAmP >>\\nendobj\\nxref\\n0 1408\\n0000000000 65535 f\\n0007251237 00000 n\\n0007252151 00000 n\\n0007252173 00000 n\\n0007252195 00000 n\\n0007252249 00000 n\\n0000008852 00000 n\\n0007252294 00000 n\\n0007252354 00000 n\\n0007252427 00000 n\\n0007252471 00000 n\\n0007252517 00000 n\\n0007252579 00000 n\\n0007252704 00000 n\\n0007252749 00000 n\\n0007252800 00000 n\\n0007252862 00000 n\\n0007252936 00000 n\\n0007252965 00000 n\\n0007253016 00000 n\\n0007253078 00000 n\\n0007253165 00000 n\\n0007253210 00000 n\\n0007253261 00000 n\\n0007253323 00000 n\\n0007253410 00000 n\\n0007253461 00000 n\\n0007253512 00000 n\\n0007253574 00000 n\\n0007253648 00000 n\\n0007253699 00000 n\\n0007253745 00000 n\\n0007253807 00000 n\\n0007253933 00000 n\\n0007253978 00000 n\\n0007254029 00000 n\\n0007254090 00000 n\\n0007254164 00000 n\\n0007254249 00000 n\\n0007254300 00000 n\\n0007254362 00000 n\\n0007254449 00000 n\\n0007254552 00000 n\\n0007254603 00000 n\\n0007254665 00000 n\\n0007254739 00000 n\\n0007254824 00000 n\\n0007254870 00000 n\\n0007254932 00000 n\\n0007255021 00000 n\\n0007255060 00000 n\\n0007255106 00000 n\\n0007255168 00000 n\\n0007255294 00000 n\\n0007255335 00000 n\\n0007255386 00000 n\\n0007255448 00000 n\\n0007255559 00000 n\\n0007255634 00000 n\\n0007255690 00000 n\\n0007255752 00000 n\\n0007255826 00000 n\\n0007255867 00000 n\\n0007255923 00000 n\\n0007255985 00000 n\\n0007256059 00000 n\\n0007256100 00000 n\\n0007256151 00000 n\\n0007256213 00000 n\\n0007256337 00000 n\\n0007256400 00000 n\\n0007256456 00000 n\\n0007256518 00000 n\\n0007256592 00000 n\\n0007256671 00000 n\\n0007256727 00000 n\\n0007256789 00000 n\\n0007256863 00000 n\\n0007256942 00000 n\\n0007256993 00000 n\\n0007257055 00000 n\\n0007257142 00000 n\\n0007257201 00000 n\\n0007257252 00000 n\\n0007257314 00000 n\\n0007257401 00000 n\\n0007257456 00000 n\\n0007257507 00000 n\\n0007257569 00000 n\\n0007257643 00000 n\\n0007257684 00000 n\\n0007257730 00000 n\\n0007257792 00000 n\\n0007257881 00000 n\\n0007257980 00000 n\\n0007258027 00000 n\\n0007258088 00000 n\\n0007258203 00000 n\\n0007258240 00000 n\\n0007258291 00000 n\\n0007258353 00000 n\\n0007258430 00000 n\\n0007258482 00000 n\\n0007258534 00000 n\\n0007258596 00000 n\\n0007258688 00000 n\\n0007258784 00000 n\\n0007258836 00000 n\\n0007258899 00000 n\\n0007258991 00000 n\\n0007259061 00000 n\\n0007259113 00000 n\\n0007259176 00000 n\\n0007259268 00000 n\\n0007259378 00000 n\\n0007259430 00000 n\\n0007259493 00000 n\\n0007259585 00000 n\\n0007259667 00000 n\\n0007259719 00000 n\\n0007259782 00000 n\\n0007259860 00000 n\\n0007259918 00000 n\\n0007259970 00000 n\\n0000000012 00000 n\\n0000000671 00000 n\\n0000003778 00000 n\\n0007260119 00000 n\\n0000001514 00000 n\\n0007260305 00000 n\\n0007260368 00000 n\\n0007260444 00000 n\\n0007260506 00000 n\\n0007260568 00000 n\\n0007260763 00000 n\\n0007260958 00000 n\\n0007261078 00000 n\\n0000005542 00000 n\\n0007261354 00000 n\\n0007261376 00000 n\\n0007261438 00000 n\\n0007261598 00000 n\\n0007261759 00000 n\\n0007261926 00000 n\\n0007262093 00000 n\\n0007262260 00000 n\\n0007262427 00000 n\\n0007262588 00000 n\\n0007262753 00000 n\\n0007262920 00000 n\\n0007263087 00000 n\\n0007263248 00000 n\\n0007263408 00000 n\\n0007263574 00000 n\\n0007263747 00000 n\\n0007263920 00000 n\\n0007264087 00000 n\\n0007264260 00000 n\\n0007264433 00000 n\\n0007264600 00000 n\\n0007264767 00000 n\\n0007264934 00000 n\\n0007265094 00000 n\\n0007265256 00000 n\\n0007265422 00000 n\\n0007265587 00000 n\\n0007265754 00000 n\\n0007265921 00000 n\\n0007266087 00000 n\\n0007266253 00000 n\\n0007266390 00000 n\\n0000007802 00000 n\\n0007266755 00000 n\\n0007266818 00000 n\\n0007266986 00000 n\\n0007267156 00000 n\\n0007267322 00000 n\\n0007267488 00000 n\\n0007267653 00000 n\\n0007267818 00000 n\\n0007267980 00000 n\\n0007268143 00000 n\\n0007268305 00000 n\\n0007268464 00000 n\\n0007268625 00000 n\\n0007268786 00000 n\\n0007268951 00000 n\\n0007269116 00000 n\\n0007269279 00000 n\\n0007269442 00000 n\\n0007269606 00000 n\\n0007269773 00000 n\\n0007269940 00000 n\\n0007270102 00000 n\\n0007270264 00000 n\\n0007270427 00000 n\\n0007270595 00000 n\\n0007270763 00000 n\\n0007270931 00000 n\\n0007271099 00000 n\\n0007271268 00000 n\\n0007271436 00000 n\\n0007271605 00000 n\\n0007271774 00000 n\\n0007271951 00000 n\\n0007272125 00000 n\\n0007272287 00000 n\\n0007272448 00000 n\\n0007272585 00000 n\\n0000011279 00000 n\\n0007273006 00000 n\\n0007273069 00000 n\\n0007273232 00000 n\\n0001665152 00000 n\\n0007273398 00000 n\\n0007273461 00000 n\\n0007273524 00000 n\\n0007273587 00000 n\\n0007273650 00000 n\\n0007273713 00000 n\\n0007273776 00000 n\\n0007273839 00000 n\\n0007273902 00000 n\\n0007273965 00000 n\\n0007274027 00000 n\\n0007274090 00000 n\\n0007274152 00000 n\\n0007274215 00000 n\\n0007274278 00000 n\\n0007274440 00000 n\\n0007274603 00000 n\\n0007274766 00000 n\\n0007274929 00000 n\\n0007275092 00000 n\\n0007275260 00000 n\\n0007275428 00000 n\\n0007275591 00000 n\\n0007275751 00000 n\\n0007275919 00000 n\\n0007276088 00000 n\\n0007276252 00000 n\\n0007276416 00000 n\\n0007276585 00000 n\\n0007276754 00000 n\\n0007276923 00000 n\\n0007277085 00000 n\\n0007277247 00000 n\\n0007277410 00000 n\\n0007277571 00000 n\\n0007277735 00000 n\\n0007277886 00000 n\\n0000014903 00000 n\\n0007278147 00000 n\\n0007278210 00000 n\\n0007278273 00000 n\\n0007278437 00000 n\\n0007278500 00000 n\\n0007278563 00000 n\\n0007278626 00000 n\\n0007278689 00000 n\\n0007278851 00000 n\\n0007279015 00000 n\\n0007279172 00000 n\\n0007279331 00000 n\\n0007279499 00000 n\\n0007279667 00000 n\\n0007279840 00000 n\\n0007280012 00000 n\\n0007280182 00000 n\\n0007280355 00000 n\\n0007280527 00000 n\\n0007280700 00000 n\\n0007280869 00000 n\\n0007281032 00000 n\\n0007281195 00000 n\\n0007281360 00000 n\\n0007281527 00000 n\\n0007281706 00000 n\\n0000018136 00000 n\\n0007281999 00000 n\\n0007282062 00000 n\\n0007282125 00000 n\\n0007282298 00000 n\\n0007282463 00000 n\\n0007282526 00000 n\\n0007282589 00000 n\\n0007282652 00000 n\\n0007282715 00000 n\\n0007282778 00000 n\\n0000024330 00000 n\\n0007282841 00000 n\\n0000836114 00000 n\\n0007282917 00000 n\\n0007283085 00000 n\\n0007283250 00000 n\\n0007283415 00000 n\\n0007283582 00000 n\\n0007283749 00000 n\\n0007283917 00000 n\\n0007284084 00000 n\\n0007284251 00000 n\\n0007284418 00000 n\\n0007284599 00000 n\\n0007284780 00000 n\\n0007284960 00000 n\\n0007285140 00000 n\\n0007285302 00000 n\\n0007285464 00000 n\\n0007285664 00000 n\\n0000021529 00000 n\\n0007285948 00000 n\\n0007286011 00000 n\\n0007286074 00000 n\\n0007286136 00000 n\\n0000821926 00000 n\\n0001650964 00000 n\\n0007286200 00000 n\\n0007286263 00000 n\\n0007286326 00000 n\\n0007286389 00000 n\\n0005076832 00000 n\\n0007286452 00000 n\\n0007286515 00000 n\\n0007286578 00000 n\\n0007286641 00000 n\\n0007286804 00000 n\\n0007286967 00000 n\\n0007287141 00000 n\\n0007287315 00000 n\\n0007287493 00000 n\\n0007287671 00000 n\\n0007287840 00000 n\\n0007288008 00000 n\\n0007288176 00000 n\\n0007288345 00000 n\\n0007288521 00000 n\\n0007288697 00000 n\\n0007288860 00000 n\\n0007289023 00000 n\\n0007289199 00000 n\\n0007289372 00000 n\\n0007289544 00000 n\\n0007289716 00000 n\\n0007289885 00000 n\\n0007290054 00000 n\\n0007290222 00000 n\\n0007290391 00000 n\\n0007290560 00000 n\\n0007290728 00000 n\\n0007290904 00000 n\\n0007291080 00000 n\\n0007291262 00000 n\\n0007291444 00000 n\\n0007291621 00000 n\\n0007291800 00000 n\\n0007291971 00000 n\\n0007292142 00000 n\\n0007292319 00000 n\\n0007292498 00000 n\\n0007292667 00000 n\\n0007292835 00000 n\\n0007293000 00000 n\\n0001668071 00000 n\\n0007293429 00000 n\\n0007293492 00000 n\\n0007293658 00000 n\\n0007293778 00000 n\\n0007293841 00000 n\\n0007293904 00000 n\\n0007293967 00000 n\\n0007294030 00000 n\\n0007294093 00000 n\\n0007294156 00000 n\\n0007294219 00000 n\\n0007294282 00000 n\\n0007294345 00000 n\\n0007294408 00000 n\\n0007294471 00000 n\\n0007294534 00000 n\\n0001674060 00000 n\\n0002285572 00000 n\\n0007294701 00000 n\\n0007294871 00000 n\\n0007295099 00000 n\\n0001671550 00000 n\\n0007295255 00000 n\\n0007295318 00000 n\\n0007295381 00000 n\\n0007295444 00000 n\\n0002273140 00000 n\\n0002952301 00000 n\\n0002967587 00000 n\\n0003419178 00000 n\\n0007295508 00000 n\\n0007295675 00000 n\\n0007295844 00000 n\\n0007296013 00000 n\\n0007296195 00000 n\\n0007296377 00000 n\\n0007296546 00000 n\\n0007296715 00000 n\\n0007296883 00000 n\\n0007297052 00000 n\\n0007297280 00000 n\\n0002964733 00000 n\\n0007297476 00000 n\\n0007297539 00000 n\\n0007297603 00000 n\\n0007297666 00000 n\\n0007297728 00000 n\\n0003404990 00000 n\\n0003828834 00000 n\\n0007297792 00000 n\\n0007297961 00000 n\\n0007298129 00000 n\\n0007298297 00000 n\\n0007298465 00000 n\\n0007298631 00000 n\\n0007298796 00000 n\\n0003843022 00000 n\\n0007298993 00000 n\\n0007299056 00000 n\\n0007299119 00000 n\\n0007299182 00000 n\\n0007299245 00000 n\\n0003849689 00000 n\\n0004316376 00000 n\\n0004572233 00000 n\\n0004805422 00000 n\\n0007299414 00000 n\\n0007299583 00000 n\\n0007299825 00000 n\\n0003847768 00000 n\\n0007299997 00000 n\\n0007300060 00000 n\\n0007300123 00000 n\\n0007300185 00000 n\\n0007300249 00000 n\\n0007300313 00000 n\\n0007300377 00000 n\\n0004307433 00000 n\\n0004563290 00000 n\\n0004796479 00000 n\\n0005062644 00000 n\\n0005108449 00000 n\\n0007300440 00000 n\\n0007300609 00000 n\\n0007300777 00000 n\\n0007300945 00000 n\\n0007301108 00000 n\\n0007301270 00000 n\\n0007301433 00000 n\\n0007301596 00000 n\\n0007301757 00000 n\\n0007301918 00000 n\\n0007302083 00000 n\\n0005079569 00000 n\\n0007302248 00000 n\\n0007302311 00000 n\\n0007302477 00000 n\\n0007302639 00000 n\\n0007302801 00000 n\\n0007302963 00000 n\\n0007303126 00000 n\\n0007303289 00000 n\\n0007303453 00000 n\\n0007303617 00000 n\\n0007303781 00000 n\\n0007303949 00000 n\\n0007304117 00000 n\\n0007304280 00000 n\\n0007304443 00000 n\\n0007304605 00000 n\\n0007304767 00000 n\\n0007304942 00000 n\\n0007305117 00000 n\\n0007305279 00000 n\\n0007305441 00000 n\\n0007305624 00000 n\\n0007305807 00000 n\\n0007305982 00000 n\\n0007306157 00000 n\\n0007306318 00000 n\\n0007306481 00000 n\\n0007306644 00000 n\\n0007306803 00000 n\\n0007306981 00000 n\\n0007307159 00000 n\\n0007307341 00000 n\\n0007307522 00000 n\\n0007307685 00000 n\\n0007307848 00000 n\\n0007308011 00000 n\\n0007308174 00000 n\\n0007308336 00000 n\\n0007308497 00000 n\\n0007308662 00000 n\\n0007308827 00000 n\\n0007308990 00000 n\\n0007309153 00000 n\\n0007309333 00000 n\\n0007309513 00000 n\\n0007309684 00000 n\\n0007309855 00000 n\\n0007309992 00000 n\\n0005082652 00000 n\\n0007310485 00000 n\\n0007310548 00000 n\\n0007310707 00000 n\\n0007310827 00000 n\\n0007310890 00000 n\\n0007310953 00000 n\\n0007311016 00000 n\\n0007311079 00000 n\\n0007311142 00000 n\\n0007311205 00000 n\\n0007311268 00000 n\\n0007311331 00000 n\\n0007311394 00000 n\\n0007311457 00000 n\\n0007311520 00000 n\\n0007311583 00000 n\\n0007311646 00000 n\\n0007311709 00000 n\\n0007311772 00000 n\\n0007311835 00000 n\\n0007311897 00000 n\\n0007311960 00000 n\\n0007312022 00000 n\\n0007312186 00000 n\\n0007312348 00000 n\\n0007312511 00000 n\\n0007312674 00000 n\\n0007312840 00000 n\\n0007313005 00000 n\\n0007313180 00000 n\\n0007313356 00000 n\\n0007313524 00000 n\\n0007313692 00000 n\\n0007313859 00000 n\\n0007314026 00000 n\\n0007314177 00000 n\\n0005085553 00000 n\\n0007314454 00000 n\\n0007314517 00000 n\\n0007314717 00000 n\\n0007314779 00000 n\\n0007314842 00000 n\\n0007314905 00000 n\\n0007314968 00000 n\\n0007315031 00000 n\\n0007315094 00000 n\\n0007315255 00000 ã™é½³\\x12\\nÑÈ\\nh·Ûö\\x07\\x01\\xa0²k×®k�Ö××\\x17\\x0fÊÝ»w\\x0fv™˜˜ˆï8444Øc||<\\x0eŒŒŒô\\x0eŒ��Å�}ûöõ\\x0eŒŽŽÆ�ðvï@x¯8\\x10>Zï@øwã@ø|z\\x07Âg\\n\\x07ÂW4X\\'\\x0eLNNÖ\\x0eÄ:wàÀ�Ú�åååj`jjªv`ii©\\x1a8xð`íÀââb50==];°°°P\\nÌÌÌÔ\\x0e\\n:t¨\\x1a˜��\\xad\\n˜ŸŸ¯\\x06æææj\\x07â/ã…7j\\x07Â;V\\x03áCÕ\\x0e„\\x7fº\\x1a\\x08ŸLí@øä«�ðåÔ\\x0e„/¿\\x1a\\x08/Hí@x\\x01«�ð’Ö\\x0e„oA5\\x10¾)µ\\x03á›\\x18{uí@x\\nâ#Q; \\x1a¢!\\x1a¢!\\x1a¢±V4öîÝ\\x1bïµÂÛ¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑènVñµ\\x12\\nÑ\\x10\\n§†hˆ†hˆ†h4ˆÆððplV\\x03\\x03\\x03¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑèÞ\\x1f¬~\\x1a(\\x1a¢!\\x1a[üÔ¨Ý\\x1f\\x14\\nÑ\\x10\\n§Æ–�ÆØôkŸûÑ\\x7fü—»\\x7f\\x7fÙÿøým�üÇÿ}áŸÇˆÆªß³\\x12\\nÑÈ�F\\x7f\\x7f¿ýA¨lß¾}ÛÚì\\x0fªjþ/Æÿà‹†hˆ†hˆFN4ì\\x0fŠ†h85DC4DC4Dc£¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1a\\x1b\\x15\\nûƒ¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑ°?(\\x1a¢áÔØ¤h,Tí@ø§«�ðÉÔ\\x0e„O¾\\x1a\\x08_Ní@øò«�ð‚Ô\\x0e„\\x17°\\x1a\\x08/ií@ø\\x16T\\x03á›R;\\x10¾‰±W×\\x0e„Ç >\\x12µ\\x03¢!\\x1a¢!\\x1a¢!\\x1akEcïÞ½ñ^+¼-\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îf\\x15_+Ñ\\x10\\nÑpjˆ†hˆ†hˆFƒh\\n\\x0f\\x0fÇf500 \\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îýÁê§�¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�fÑXõ{V¢!\\x1aùÑèïï·?\\x08•íÛ·o[›ýAUÍÿÅø\\x1f|Ñ\\x10\\nÑ\\x10\\nÑÈ‰†ýAÑ\\x10\\n§†hˆ†hˆ†hlT4ì\\x0fŠ†h85DC4DC4Dc£¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4NúhØ\\x1f\\x04€èØ\\x7f\\x7f°Õjù+êþTô\\x1bþŠº\\'_4DÃ“/\\x1a¢Ñ4\\x1a�N\\'Þk…·EC4DÃ“/\\x1a¢áÉ\\x17\\nÑh\\n�îf\\x15\\x7fº$\\x1a¢!\\x1a\\x06DC4<ù¢!\\x1a\\n‰Ú;+Ñ\\x10\\nÑ0 \\x1a¢a@4D£Á3Ó½?¸ªY‰†h85œ\\x1a¢!\\x1a\\x06DC4ÖõÌôÞY‰†hd\\x0e´Ûmûƒ°–p8ÆƒrÏž=^\\x10€Æº\\x7fb\\x18\\x7fË\\nÍ\\n@³\\x02Ð¬Ð¬Ø<Ýk\\x11ö\\x07\\x01à\\x18\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xP¶Z\\xad#G‹cGÖ°E\\x06VVV6j ¼a`S\\x07â74\\x7fÀ“/\\x1a¢áÉ\\x17\\nÑXo4:�N¼×\\no‹†hˆ†\\'_4DÃ“/\\x1a¢Ñ8\\x1aÝÍ*þÄP4DC4\\nˆ†hxòEC4\\x1a<\\x12µwV¢!\\x1a¢a@4DÃ€hˆFƒg¦û·ÜW5+Ñ\\x10\\n§†SC4DÃ€hˆÆºž™Þ;+Ñ\\x10�Ì�v»m\\x7f\\x10*»víºöh}}}ñ\\xa0Ü½{÷`—‰‰‰øŽCCCƒ=ÆÇÇãÀÈÈHïÀØØX\\nØ·o_ïÀèèh\\n\\x08o÷\\x0e„÷Š\\x03á£õ\\x0e„\\x7f7\\x0e„Ï§w |æq |EƒuâÀäädí@¬s\\x07\\x0e\\n¨\\nX^^®\\x06¦¦¦j\\x07–––ª�ƒ\\x07\\x0fÖ\\x0e,..V\\x03ÓÓÓµ\\x03\\nÕÀÌÌLíÀ¡C‡ª�ÙÙÙÚ�ùùùj`nn®v þ2^x£v ¼c5\\x10>Tí@ø§«�ðÉÔ\\x0e„O¾\\x1a\\x08_Ní@øò«�ð‚Ô\\x0e„\\x17°\\x1a\\x08/ií@ø\\x16T\\x03á›R;\\x10¾‰±W×\\x0e„Ç >\\x12µ\\x03¢!\\x1a¢!\\x1a¢!\\x1akEcïÞ½ñ^+¼-\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îf\\x15_+Ñ\\x10\\nÑpjˆ†hˆ†hˆFƒh\\n\\x0f\\x0fÇf500 \\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îýÁê§�¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�fÑXõ{V¢!\\x1aùÑèïï·?\\x08•íÛ·o[›ýAUÍÿÅø\\x1f|Ñ\\x10\\nÑ\\x10\\nÑÈ‰†ýAÑ\\x10\\n§†hˆ†hˆ†hlT4ì\\x0fŠ†h85DC4DC4Dc£¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4NúhØ\\x1f\\x04€èØ\\x7f\\x7f°Õjù+êþTô\\x1bþŠº\\'_4DÃ“/\\x1a¢Ñ4\\x1a�N\\'Þk…·EC4DÃ“/\\x1a¢áÉ\\x17\\nÑh\\n�îf\\x15\\x7fº$\\x1a¢!\\x1a\\x06DC4<ù¢!\\x1a\\n‰Ú;+Ñ\\x10\\nÑ0 \\x1a¢a@4D£Á3Ó½?¸ªY‰†h85œ\\x1a¢!\\x1a\\x06DC4ÖõÌôÞY‰†hd\\x0e´Ûmûƒ°–p8ÆƒrÏž=^\\x10€Æº\\x7fb\\x18\\x7fË\\nÍ\\n@³\\x02Ð¬Ð¬Ø<Ýk\\x11ö\\x07\\x01à\\x18\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xP¶Z\\xad#G‹cGÖ°E\\x06VVV6j ¼a`S\\x07â74\\x7fÀ“/\\x1a¢áÉ\\x17\\nÑXo4:�N¼×\\no‹†hˆ†\\'_4DÃ“/\\x1a¢Ñ8\\x1aÝÍ*þÄP4DC4\\nˆ†hxòEC4\\x1a<\\x12µwV¢!\\x1a¢a@4DÃ€hˆFƒg¦û·ÜW5+Ñ\\x10\\n§†SC4DÃ€hˆÆºž™Þ;+Ñ\\x10�Ì�v»m\\x7f\\x10*»víºöh}}}ñ\\xa0Ü½{÷`—‰‰‰øŽCCCƒ=ÆÇÇãÀÈÈHïÀØØX\\nØ·o_ïÀèèh\\n\\x08o÷\\x0e„÷Š\\x03á£õ\\x0e„\\x7f7\\x0e„Ï§w |æq |EƒuâÀäädí@¬s\\x07\\x0e\\n¨\\nX^^®\\x06¦¦¦j\\x07–––ª�ƒ\\x07\\x0fÖ\\x0e,..V\\x03ÓÓÓµ\\x03\\nÕÀÌÌLíÀ¡C‡ª�ÙÙÙÚ�ùùùj`nn®v þ2^x£v ¼c5\\x10>Tí@ø§«�ðÉÔ\\x0e„O¾\\x1a\\x08_Ní@øò«�ð‚Ô\\x0e„\\x17°\\x1a\\x08/ií@ø\\x16T\\x03á›R;\\x10¾‰±W×\\x0e„Ç >\\x12µ\\x03¢!\\x1a¢!\\x1a¢!\\x1akEcïÞ½ñ^+¼-\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îf\\x15_+Ñ\\x10\\nÑpjˆ†hˆ†hˆFƒh\\n\\x0f\\x0fÇf500 \\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îýÁê§�¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�fÑXõ{V¢!\\x1aùÑèïï·?\\x08•íÛ·o[›ýAUÍÿÅø\\x1f|Ñ\\x10\\nÑ\\x10\\nÑÈ‰†ýAÑ\\x10\\n§†hˆ†hˆ†hlT4ì\\x0fŠ†h85DC4DC4Dc£¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4NúhØ\\x1f\\x04€èØ\\x7f\\x7f°Õjù+êþTô\\x1bþŠº\\'_4DÃ“/\\x1a¢Ñ4\\x1a�N\\'Þk…·EC4DÃ“/\\x1a¢áÉ\\x17\\nÑh\\n�îf\\x15\\x7fº$\\x1a¢!\\x1a\\x06DC4<ù¢!\\x1a\\n‰Ú;+Ñ\\x10\\nÑ0 \\x1a¢a@4D£Á3Ó½?¸ªY‰†h85œ\\x1a¢!\\x1a\\x06DC4ÖõÌôÞY‰†hd\\x0e´Ûmûƒ°–p8ÆƒrÏž=^\\x10€Æº\\x7fb\\x18\\x7fË\\nÍ\\n@³\\x02Ð¬Ð¬Ø<Ýk\\x11ö\\x07\\x01à\\x18\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07e«Õ:r´8vd\\n[d`eee£\\x06Â\\x1b\\x066u ~Có\\x07<ù¢!\\x1až|Ñ\\x10�õF£ÓéÄ{\\xadð¶hˆ†hxòEC4<ù¢!\\x1a�£ÑÝ¬âO\\nEC4DÃ€hˆ†\\'_4D£Á#Q{g%\\x1a¢!\\x1a\\x06DC4\\nˆ†h4xfº\\x7fË}U³\\x12\\nÑpj85DC4\\nˆ†h¬ë™é½³\\x12\\nÑÈ\\nh·Ûö\\x07\\x01\\xa0²k×®k�Ö××\\x17\\x0fÊÝ»w\\x0fv™˜˜ˆï8444Øc||<\\x0eŒŒŒô\\x0eŒ��Å�}ûöõ\\x0eŒŽŽÆ�ðvï@x¯8\\x10>Zï@øwã@ø|z\\x07Âg\\n\\x07ÂW4X\\'\\x0eLNNÖ\\x0eÄ:wàÀ�Ú�åååj`jjªv`ii©\\x1a8xð`íÀââb50==];°°°P\\nÌÌÌÔ\\x0e\\n:t¨\\x1a˜��\\xad\\n˜ŸŸ¯\\x06æææj\\x07â/ã…7j\\x07Â;V\\x03áCÕ\\x0e„\\x7fº\\x1a\\x08ŸLí@øä«�ðåÔ\\x0e„/¿\\x1a\\x08/Hí@x\\x01«�ð’Ö\\x0e„oA5\\x10¾)µ\\x03á›\\x18{uí@x\\nâ#Q; \\x1a¢!\\x1a¢!\\x1a¢±V4öîÝ\\x1bïµÂÛ¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑènVñµ\\x12\\nÑ\\x10\\n§†hˆ†hˆ†h4ˆÆððplV\\x03\\x03\\x03¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑèÞ\\x1f¬~\\x1a(\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\x16�U¿g%\\x1a¢‘\\x1f�þþ~ûƒPÙ¾}û¶µÙ\\x1fTÕü_ŒÿÁ\\x17\\nÑ\\x10\\nÑ\\x10�œhØ\\x1f\\x14\\nÑpjˆ†hˆ†hˆÆFEÃþ\\xa0hˆ†SC4DC4DC46*\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07Eã¤�†ýAˆŽý÷\\x07[\\xad–¿¢îOE¿á¯¨{òEC4<ù¢!\\x1aM£Ñétâ½Vx[4DC4<ù¢!\\x1až|Ñ\\x10�ÆÑènVñ§K¢!\\x1a¢a@4DÃ“/\\x1a¢Ñà‘¨½³\\x12\\nÑ\\x10\\n\\x03¢!\\x1a\\x06DC4\\x1a<3Ýûƒ«š•hˆ†SÃ©!\\x1a¢a@4Dc]ÏLï�•hˆFæ@»Ý¶?\\x08k\\t‡c<(÷ìÙã\\x05\\x01h¬û\\'†ñ·Ü\\x01Ð¬4+Í\\nÍŠÍÓ½\\x16a\\x7f\\x10ŽqPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07e«Õ:r´8vd\\n[d`eee£\\x06Â\\x1b\\x066u ~Có\\x07<ù¢!\\x1až|Ñ\\x10�õF£ÓéÄ{\\xadð¶hˆ†hxòEC4<ù¢!\\x1a�£ÑÝ¬âO\\nEC4DÃ€hˆ†\\'_4D£Á#Q{g%\\x1a¢!\\x1a\\x06DC4\\nˆ†h4xfº\\x7fË}U³\\x12\\nÑpj85DC4\\n ˆ†h¬ë™é½³\\x12\\nÑÈ\\nh·Ûö\\x07\\x01\\xa0²k×®k�Ö××\\x17\\x0fÊÝ»w\\x0fv™˜˜ˆï8444Øc||<\\x0eŒŒŒô\\x0eŒ��Å�}ûöõ\\x0eŒŽŽÆ�ðvï@x¯8\\x10>Zï@øwã@ø|z\\x07Âg\\n\\x07ÂW4X\\'\\x0eLNNÖ\\x0eÄ:wàÀ�Ú�åååj`jjªv`ii©\\x1a8xð`íÀââb50==];°°°P\\nÌÌÌÔ\\x0e\\n:t¨\\x1a˜��\\xad\\n˜ŸŸ¯\\x06æææj\\x07â/ã…7j\\x07Â;V\\x03áCÕ\\x0e„\\x7fº\\x1a\\x08ŸLí@øä«�ðåÔ\\x0e„/¿\\x1a\\x08/Hí@x\\x01«�ð’Ö\\x0e„oA5\\x10¾)µ\\x03á›\\x18{uí@x\\nâ#Q; \\x1a¢!\\x1a¢!\\x1a¢±V4öîÝ\\x1bïµÂÛ¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑènVñµ\\x12\\nÑ\\x10\\n§†hˆ†hˆ†h4ˆÆððplV\\x03\\x03\\x03¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑèÞ\\x1f¬~\\x1a(\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\x16�U¿g%\\x1a¢‘\\x1f�þþ~ûƒPÙ¾}û¶µÙ\\x1fTÕü_ŒÿÁ\\x17\\nÑ\\x10\\nÑ\\x10�œhØ\\x1f\\x14\\nÑpjˆ†hˆ†hˆÆFEÃþ\\xa0hˆ†SC4DC4DC46*\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07Eã¤�†ýAˆŽý÷\\x07[\\xad–¿¢îOE¿á¯¨{òEC4<ù¢!\\x1aM£Ñétâ½Vx[4DC4<ù¢!\\x1až|Ñ\\x10�ÆÑènVñ§K¢!\\x1a¢a@4DÃ“/\\x1a¢Ñà‘¨½³\\x12\\nÑ\\x10\\n\\x03¢!\\x1a\\x06DC4\\x1a<3Ýûƒ«š•hˆ†SÃ©!\\x1a¢a@4Dc]ÏLï�•hˆFæ@»Ý¶?\\x08k\\t‡c<(÷ìÙã\\x05\\x01h¬û\\'†ñ·Ü\\x01Ð¬4+Í\\nÍŠÍÓ½\\x16a\\x7f\\x10ŽqPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07e«Õ:r´8vd\\n[d`eee£\\x06Â\\x1b\\x066u ~Có\\x07<ù¢!\\x1až|Ñ\\x10�õF£ÓéÄ{\\xadð¶hˆ†hxòEC4<ù¢!\\x1a�£ÑÝ¬âO\\nEC4DÃ€hˆ†\\'_4D£Á#Q{g%\\x1a¢!\\x1a\\x06DC4\\nˆ†h4xfº\\x7fË}U³\\x12\\nÑpj85DC4\\nˆ†h¬ë™é½³\\x12\\nÑÈ\\nh·Ûö\\x07\\x01\\xa0²k×®k�Ö××\\x17\\x0fÊÝ»w\\x0fv™˜˜ˆï8444Øc||<\\x0eŒŒŒô\\x0eŒ��Å�}ûöõ\\x0eŒŽŽÆ�ðvï@x¯8\\x10>Zï@øwã@ø|z\\x07Âg\\n\\x07ÂW4X\\'\\x0eLNNÖ\\x0eÄ:wàÀ�Ú�åååj`jjªv`ii©\\x1a8xð`íÀââb50==];°°°P\\nÌÌÌÔ\\x0e\\n:t¨\\x1a˜��\\xad\\n˜ŸŸ¯\\x06æææj\\x07â/ã…7j\\x07Â;V\\x03áCÕ\\x0e„\\x7fº\\x1a\\x08ŸLí@øä«�ðåÔ\\x0e„/¿\\x1a\\x08/Hí@x\\x01«�ð’Ö\\x0e„oA5\\x10¾)µ\\x03á›\\x18{uí@x\\nâ#Q; \\x1a¢!\\x1a¢!\\x1a¢±V4öîÝ\\x1bïµÂÛ¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑènVñµ\\x12\\nÑ\\x10\\n§†hˆ†hˆ†h4ˆÆððplV\\x03\\x03\\x03¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑèÞ\\x1f¬~\\x1a(\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\x16�U¿g%\\x1a¢‘\\x1f�þþ~ûƒPÙ¾}û¶µÙ\\x1fTÕü_ŒÿÁ\\x17\\nÑ\\x10\\nÑ\\x10�œhØ\\x1f\\x14\\nÑpjˆ†hˆ†hˆÆFEÃþ\\xa0hˆ†SC4DC4DC46*\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07Eã¤�†ýAˆŽý÷\\x07[\\xad–¿¢îOE¿á¯¨{òEC4<ù¢!\\x1aM£Ñétâ½Vx[4DC4<ù¢!\\x1až|Ñ\\x10�ÆÑènVñ§K¢!\\x1a¢a@4DÃ“/\\x1a¢Ñà‘¨½³\\x12\\nÑ\\x10\\n\\x03¢!\\x1a\\x06DC4\\x1a<3Ýûƒ«š•hˆ†SÃ©!\\x1a¢a@4Dc]ÏLï�•hˆFæ@»Ý¶?\\x08k\\t‡c<(÷ìÙã\\x05\\x01h¬û\\'†ñ·Ü\\x01Ð¬4+Í\\nÍŠÍÓ½\\x16a\\x7f\\x10ŽqPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08°Êìììï~÷»\\x1füà\\x07_þò—¯¼òÊw¿ûÝç�wÞ\\x19gœqÚi§�sÎ9\\x17\\\\pÁ%—\\\\Ò××wÃ\\n7|ë[ßzðÁ\\x07Ÿzê [MÈðÈH“Wyñæ]ò\\x06gÆô“^ú%0¶vŒ®‡Ëfao±cú6§ë•’[R\\'?850Ö®¾Í.åY “›2î\\x01åùÁÜbE«x÷ª_Uæ\\x07—�wÈÊ\\nŽ‰ü g\\x13\\x12\\x12’——×týçææ^wÝu&3qíÚµãZHÖ¯_¯$n™””d\\x0f\\x1b%ùAsp_\\n€Ê\\n€Ê\\n˜…ü ••:\\x05—È\\x0f²rk$\\x1f\\x17\\nÉ>Ÿf5Ú[þY÷\\x04ý\\x06ÛæH\\x0e×ó4®ŠbÛ�RQ–…óƒ«4â\\x0e«tN¾ú#l—\\x1fŒŒŽ1y•—î<,o\\x103öQé¥Kyu’tÛ«4·\\n\\x1a�y¨™]²9\\x07â�gýÖì£…e•vµwd\\\\)3¿ãKãz)\\n÷õôØ¥W–tÞ½f†Êüàò\\t\\x0eYYÁ1‘\\x1f\\x04‹{óÍ7•dâŠ‹‹\\x19+i\\x10®½öZ“cµ\\x7fÿ~{Ø(É\\x0f\\x02˜ƒûZTVTVÀ,ä\\x07©¬T\\n\\\\*ùAVn\\ná“(ýG4«ÑN\\t\\xad{‚g6ÚæHŽÔó4®Š\\x12Û�RqŽ…óƒ«½Ä\\nÚð\\x19‹ö!pòÇ¶ZþgãâM^åUûÃå\\n.Œû-\\x12èwô¢1I×ËsW±æNyãè±�4¿«\\x16�veyHâÁØ¬²J›\\x06`E—OáÓ\\x1bvylO…á¾AS\\x0e(ìs÷ºÙ*óƒ+&9de\\x05ÇD~\\x10,nÆŒ\\x19JòƒçÎ�c¬$\\x03\\x06\\n09VK–,±‡�’ü €9¸¯\\x05@e\\x05@e\\x05ÌB~�ÊJ�Â\\nòƒ¬Ü\\x1aÚ\\nÁ‘¬ù¨¹\\nøÑi\\x7fœ]ÀO6‹°\\n�.¾ô•e¶\\x1f¢’\\\\ËNËSkÆÉ{Ózµfõÿ8wS�aÙí1È:\\ns1ÅäUÞp,VÞàâ¸\\x07�\\n.f\\x17¯\\nM>–�\\x13�v%[ÓAÞøìØ~\\\\ekî\\nW,ñ<ÄÔ±÷É{Þçñt�¿™å>lØ\"¥¿7¾gý|uùÁ½+\\x7fvÈÊ\\nŽ‰ü XÜ‘#G”ä\\x07÷îÝËXI>ûì3“c5mÚ4{Ø(É\\x0f\\x02˜ƒûZTVTVÀ,ä\\x07©¬Ô)Ê\\x12Ìœ\\x15osÕœqåÊ�dï˜f8æ…\\x19Õ1\\x015Ï[´¡à™âK¯\\xad°ýø”æ[øùƒk½å½UxÝÎê\\x1f¶èx€Ç\\nÆ1)×Üþ�Ûdë|tüå\\n“WÙÿôeyƒ”q½å½]Ê+Iñ¼WÞøÌØÇ¸ÊÖÜ;J+,ðHÄ´±=ä={Œþ®ö?Vin\\nè²Ôec¤Â>\\x037-R™\\x1f\\\\5Õ!++8&òƒ`q©©©JòƒÒnÎXIFŽ\\nir¬|}}ía£$?\\x08`\\x0eîk\\x01PY\\x01PY\\x01³�\\x1f¤²R§8‡ç\\x0f²r\\x7f³æ£ºG’\\nÅ%j\\x12Çæˆ/½®ÒöÇVvÅÂÏ\\x1f\\\\ç+\\x08:yÝÅ,ødñ‰{]¶N\\x18ýÕ\\x11��\\x1b=\\x06¿ã:¥³Ë\\x0eë|tbFžÉ«\\n\\x14-È\\x18^öî%ï-§¨<Öó~yã¨q\\x03¸ÊÖÜ;´º*ó;N\\x1bÛ]Þs\\x17\\x17ÿ�\\n¯þ\\x16þÕ´\\n9z”4W§\\x07Å)ìsï\\x16¿†s‚åšÛÄùÁÕ¿:de\\x05ÇD~\\x10,\\x7f³¡¸XI~põêÕŒ•ÄÃÃÃäXMš4É\\n6Jòƒæà¾\\x16•\\x15•\\x150\\nùA*+uJóÈ\\x0f²r\\x7f“x´zÜ�µ¦Á›Õz=—¨I\\nŸ\\'¾ôU:Û\\x1f[E±…óƒëÇË{+òjÇ,øtñ‰Î.;êüXç£/å\\x16™¼ÊÁñÙò\\x06\\x19Þ=å½\\x15—k#<\\x1f’7Ž\\n7�«lÍ½Co‰oìŒ±Ýä=\\x1b&ç“®K†¹�\\x7fÈeµá\\x1f7†]RØgÐ¶åõ%\\x07ÿÏÍ»¿Ë²>.ë.iº\\nòƒk¦;de\\x05ÇD~\\x10,N§Ó)É\\x0f®ZµŠ±’üðÃ\\x0f&ÇjÑ¢Eö°Q’\\x1f\\x040\\x07÷µ¨¬¨¬€YÈ\\x0fRY©SV ˜9+Þæª9éÊM>^½á‹êE/Vï÷\\xad®(áú4•\\x13\\nêÉ\\x0fVÙþØ*K-;-Oo˜(ïíÊ˜»™\\x056Ì\\x0ff\\\\)3y•O§äË\\x1bdŽë.ïMW¥?æÙ_Þø´÷S\\\\e‡Û;2Çt\\xad/?Xç\\'$>GaŸAþ«êË\\x0f~âækè-ÉS�[Ü»n¦CVVpLä\\x07\\x01ÀâŠŠŠ”ä\\x07ýýý\\x19+ÉçŸ\\x7fnr¬¶lÙb\\x0f\\x1b%ùAsp_\\n€Ê\\n€Ê\\n˜…ü ••:å…ä\\x07Y¹°¶ÐEâKo\\x0f\\x0f|Ô–[8?¸i²¼·¼1�˜\\x05®›¢l•\\x1fÌ-®0y•c3\\x04»CÚ¸žÂ\\x0e\\x0fz>-o|Êûo\\\\e‡Û;²\\x14ç\\x07“s”fÌ÷\\x05¬\\xad/?8Ôm¢¡·‹žÝå¯\\x06\\xad›å�•\\x15\\n\\x13ùA°¸¤¤$%ùÁàà`ÆJ2pà@“c\\x15\\x16\\x16f\\x0f\\x1b%ùAsp_\\n€Ê\\n€Ê\\n˜…\\x14\\x12••:\\x15%ä\\x07Y¹°¶“Kì÷Òë*EÇv«êþNoš\"ï0gl\\x17fABVQ�@Ö—KC\\xad´+•Ušœ�)¹%{=ž©Óàç©ã…\\n\\x06z½(ï-Üç\\x19®²Ãí\\nÙcîQ˜\\x1f,×ê\\x14ö¹\\x7f×Æúòƒï»þlè-Þó>ù«û6ÌqÈÊ\\nŽ‰ü X\\\\PP�’ü`vv6cUYYÙªU«†\\x07Jj\\xa0Óélu„ä\\x07\\x01,…ûZTVTVÀ,\\x16�{À‰*«Ê2òƒv¶rÉ\\x0f:�ðeòë®ÓØÇ—v•N0\\'ÇüUu\\x7f§·ü\"ï0kì½Ì\\x02½^ÿ…_¨1�ÕmtÀ¡Ø,ë|ti…Îä—ONQù÷n.µ_½¢i3túNa‡;Æ¾!ï-Ìç9®²Ãí\\n9ŠóƒÊû<°gk}ùÁ·]§\\x1az‹ó¼_ðüÁ�ó\\n²²‚c\"?\\x08\\x167aÂ\\x04“áÁ»îº‹�’ìÚµËäX½øâ‹v²Q’\\x1f\\x040\\x07÷µ¨¬¨¬€YŽN«û{×§V0*TV¦i+È\\x0fÚ\\x12ùAç$}?Ë®{¥¦µ]\\n›^/˜“coSÝ_Ä–_å\\nfŒëÁ,�”kuS\\x03cßš}ôŸ+Ã�^°ÞƒW´º*“_>¥\\x15ºÎ.Û\\'�\\n^¨i#½”èÙý\\n×i¯N?,ìp«÷ûòÞNú>Ï%v¸½#wL\\'‹ç\\x07\\x0f\\x05m¯/?8Äuš¡·\\x18Ï\\x07\\x04Ï\\x1fÜ´À!++8&òƒ`q/¾ø¢ÉLÜûï¿Ï@IÞyç\\n“c5uêT;Ù(É\\x0f\\x02˜ƒûZTVTVÀ,\\x05—«§ôüã—®§?\\\\]’Ë¨PY™¦Ó’\\x1f´%òƒÎéô*ùu¯°“ü\\xa0pZŽ»Cug‘ÛfÈ;L\\x1bw?³À†ôÂ”èŸ¿|¤6†`WW—m�»,7üùï¿\\n\\x12v¸aü§òÞB}_d¨\\nnïÈ\\nÓÑòùÁý;ëË\\x0f¾æ:ÃÐ[´goA~pó\"‡¬¬à˜È\\x0f\\x02€eeff^sÍ5&3q\\n\\x17.d¬ÂÃÃ[¶lÙð@Ýxã�yyyv²Q’\\x1f\\x040\\x07÷µ¨¬¨¬€¹r\\x13ª·\\x7f_½ðùê�ÿ\\xad¾’ÊxPY)\"L‘�\\x1f´\\x1aòƒÎ)b\\xadüº—kn·ßiéÝFug‘þ3å\\n^ö~€Y`ÿ_>òÈØ\\xa0)\\x07„�\\xad�ôµ¼·\\x13ã_b˜\\nnïÈ÷ê\\xa0$?ØÛk·ò>�\\nÜS_~ð\\x15×Y†\\x0eÏxö•¿º\\x7fë\\x12‡¬¬à˜È\\x0f\\x02€eùúúš\\n\\x0f^wÝu¹¹Îþ¿|U^^Þ§O\\x1f“cõõ×_ÛÏFI~\\x10À\\nÜ×\\x02\\xa0²\\x02\\xa0²\\x02\\xa0²²\\nòƒv5øä\\x07�Aäzùu/³çü\\xa0O;õçº}¶¼Ã\\x14Ÿ¾Ì\\x02ûÿò‘§Æž™¼_ØÙê)ßË{;>á\\x15†¹‰Ìuÿ¨ÎhoõxÙ\"=\\x17xÝ-¿”ÿY}ªÎLØrê²ò>�\\n\\x0eª/?ø¢ë\\\\C‡‘ž\\x0f\\nòƒÛ–:je\\x05\\x07D~\\x10,¨¨¨èöÛo7™‰{ï½÷œ|\\xa0ôzýG\\x1f}dr\\xa0Zµj•˜˜h?\\x1b%ùAsp_\\n€Ê\\n€Ê\\n€ÊÊ6È\\x0fÚÕà“\\x1ft\\x06g6Ê¯{‰æNû�–¾íUw\\x16\\x190WÞa²ïÃÌ\\x02ûÿò‘ç\\x07Ÿœ¸OØÙÊénòÞŽMx•an\"¯¹Î¨3Ú\\x1f»M°LÅâÕ^~)�Äewu\\n0NƒG½÷–Vè”÷\\x19|ô@}ùÁA®ó\\n}žö|Xþê\\x01ÿeŽZYÁ\\x01‘\\x1f\\x04\\nš8qb\\n\\x05BBBœy”´Zí—_~©d\\xa0æÌ™cW\\x1b%ùAsp_\\n€Ê\\n€Ê\\n€ÊÊ6\\x04ùÁw¸j6\\x1b|òƒÎàìfùu/¶çüàø\\x0eª;‹Ú9_Þa¢o?f�ý\\x7fùÈóƒ\\x1f/\\x14ÿªðŠÙãä½…L|�an\"Òµ\\x185zdÍCK½n©Ð´ö\\x19ý�ô7\\x16éùÀ¸¿Ë/¥^¯ßu&íõ™Gz{íþdñ‰”Ü’FõyìØ‘úòƒO».2L\\xadpÏG\\x04ùÁí+\\nµ²‚#îÌä\\x07\\x01ÀB²³³ï¸ã\\x0e“™¸gžyÆ™G)99ù¹çžS\\x12\\n|á…\\x17¤bÌ®6Jòƒæà¾\\x16•\\x15•\\x15••m�\\x1f´«Á\\'?è\\nÎm•_÷BM\\x1bû�–\\x13:ªîìÌ®…ò\\x0e/NèÏ,°ÿ/Ÿ\\x1f×EÔÉ\\x0f\\n½�-ìlù‚Ÿå½\\x05O\\x1aÂ07\\x11Ãåèá²ù5×\\x19=]6\\x1aþÑ\"=ÿc|Ý\\x07†ÎqÿØøªºß]?\\nz¬¾üà“®~†ƒ\\x0fõì\\'È\\x0fîXå¨•\\x15\\n\\x10ùA°”¡C‡*‰Å\\n9rÄ9Ç\\'##ÃÅÅåæ›oV2J½zõÊËË³·�’ü €9¸¯\\x05@e\\x05@e\\x05@ee\\x1bä\\x07íjðÉ\\x0f:ƒèíòë~EÓÖ~§åÄÎª;;³{‘¼Ãø‰O0\\nìÿË\\',)·»ûNcxðåi‡+´UÂÎ–ûÍ‘÷vtÒ[\\ns\\x13‘?\\x1aÒRùÁ\\'ÇïÝêñ²ñ\"&{Þ;Àe™™}††‡Ö—\\n9\\x15q(6+(:ã¸çãòW\\x0f\\x06¬qÔÊ\\n\\x0eˆü X„¿¿¿’XÜ�!N÷?4Q^^\\n\\x18\\x18øÍ7ß\\\\\\x7fýõ-”¹ûî»SRRìp£$?\\x08`\\x0eîk\\x01PY\\x01PY\\x01PYÙ\\x06ùA\\x1bšÐ©îà¯û„Qiþb\\x02äë®Àžóƒ“º¨îìL\\xa0Ÿ¼Ã\\n\\x13ÿÆ,°»«,\\n/\\x1fKÈùrièóS\\x0fºmŽÊ/', 'score': 0.64719695, 'raw_content': None}, {'title': '[2412.19437v1] DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/abs/2412.19437v1', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. arXiv:2412.19437v1 arXiv author ID DeepSeek-V3 Technical Report We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.19437 Bibliographic and Citation Tools Connected Papers Toggle\", 'score': 0.6466616, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-', 'score': 0.62303346, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437v1', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total ... DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022 Figure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.', 'score': 0.5425973, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/html/2412.19437v1', 'content': 'Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention\\xa0(MLA)\\xa0(DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE\\xa0(Dai et\\xa0al., 2024) for cost-effective training. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy\\xa0(Wang et\\xa0al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models.', 'score': 0.47446132, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/html/2412.19437', 'content': 'Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention\\xa0(MLA)\\xa0(DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE\\xa0(Dai et\\xa0al., 2024) for cost-effective training. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy\\xa0(Wang et\\xa0al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models.', 'score': 0.47446132, 'raw_content': None}, {'title': 'DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open ...', 'url': 'https://arxiv.org/abs/2402.03300', 'content': 'cs arXiv:2402.03300 Help | Advanced Search arXiv author ID DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2402.03300 [cs.CL] (or arXiv:2402.03300v3 [cs.CL] for this version) From: Zhihong Shao [view email] Access Paper: cs.CL cs cs.AI References & Citations Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? Help', 'score': 0.40098447, 'raw_content': None}, {'title': 'DeepSeek LLM: Scaling Open-Source Language Models with Longtermism', 'url': 'https://arxiv.org/abs/2401.02954', 'content': 'cs arXiv:2401.02954 arXiv author ID DeepSeek LLM: Scaling Open-Source Language Models with Longtermism The rapid development of open-source large language models (LLMs) has been truly remarkable. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2401.02954 [cs.CL] (or arXiv:2401.02954v1 [cs.CL] for this version) cs.CL cs cs.AI Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle', 'score': 0.36765668, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2402.03300\n",
      "> RSP [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/abs/2402.03300v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.19437\n",
      "> RSP [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2401.02954\n",
      "> RSP [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/abs/2401.02954v1)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat', 'url': 'https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/', 'content': 'DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch | VentureBeat DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B.', 'score': 0.8727061, 'raw_content': None}, {'title': \"DeepSeek's new AI model appears to be one of the best 'open ...\", 'url': 'https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/', 'content': \"DeepSeek's new AI model appears to be one of the best 'open' challengers yet | TechCrunch DeepSeek's new AI model appears to be one of the best 'open' challengers yet | TechCrunch DeepSeek’s new AI model appears to be one of the best ‘open’ challengers yet The model, DeepSeek V3, was developed by the AI firm DeepSeek and was released on Wednesday under a permissive license that allows developers to download and modify it for most applications, including commercial ones. According to DeepSeek’s internal benchmark testing, DeepSeek V3 outperforms both downloadable, “openly” available models and “closed” AI models that can only be accessed through an API.\", 'score': 0.86108357, 'raw_content': None}, {'title': 'DeepSeek V3: New Open AI Model Surpasses Rivals and ... - WinBuzzer', 'url': 'https://winbuzzer.com/2024/12/27/deepseek-v3-new-open-ai-model-surpasses-rivals-and-challenges-gpt-4o-xcxwbn/', 'content': 'DeepSeek V3: New Open AI Model Surpasses Rivals and Challenges GPT-4o - WinBuzzer DeepSeek V3: New Open AI Model Surpasses Rivals and Challenges GPT-4o DeepSeek V3 is an open-source AI model that outperformes Meta’s Llama 3.1 and comes close to OpenAI’s GPT-4o in key benchmarks. DeepSeek V3’s technical advancements place it among the most powerful AI systems to, rivaling both open-source competitors like Meta’s Llama 3.1 and proprietary models like OpenAI’s GPT-4o. DeepSeek V3’s benchmark results showcase its exceptional capabilities across a broad spectrum of tasks, solidifying its position as a leader among open-source AI models. Related: DeepSeek AI Open Sources VL2 Series of Vision Language Models', 'score': 0.8593928, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-', 'score': 0.8479807, 'raw_content': None}, {'title': 'Introducing DeepSeek-V3 | DeepSeek API Docs', 'url': 'https://api-docs.deepseek.com/news/news1226', 'content': '🚀 Introducing DeepSeek-V3 | DeepSeek API Docs DeepSeek API Docs DeepSeek Platform Your First API Call Introducing DeepSeek-V3 2024/12/26 DeepSeek-V2.5-1210 Release 2024/12/10 DeepSeek-V2.5 Release 2024/09/05 Context Caching is Available 2024/08/02 New API Features 2024/07/25 API Reference API Guides Context Caching API Status Page Introducing DeepSeek-V3 2024/12/26 🚀 Introducing DeepSeek-V3 ⚡ 60 tokens/second (3x faster than V2!) Model 👉 https://github.com/deepseek-ai/DeepSeek-V3 Paper 👉 https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf 💰 API Pricing Update\\u200b 🎉 Until Feb 8: same as V2! | Input (cache miss) | Input (cache hit) | Output | Look forward to multimodal support and other cutting-edge features in the DeepSeek ecosystem. Previous Error CodesNext 🚀 DeepSeek V2.5: The Grand Finale 🎉 🎉 What’s new in V3 💰 API Pricing Update Copyright © 2024 DeepSeek, Inc.', 'score': 0.780947, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "Paper Link\n",
      "👁️\n",
      "1. Introduction\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \n",
      "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \n",
      "Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \n",
      "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \n",
      "Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\n",
      "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\n",
      "In addition, its training process is remarkably stable. \n",
      "Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "2. Model Summary\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n",
      "We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n",
      "    It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.\n",
      "This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.\n",
      "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n",
      "3. Model Downloads\n",
      "| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n",
      "| :------------: | :------------: | :------------: | :------------: | :------------: |\n",
      "| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n",
      "| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n",
      "NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.\n",
      "To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6:\n",
      "How_to Run_Locally\n",
      ".\n",
      "For developers looking to dive deeper, we recommend exploring\n",
      "README_WEIGHTS.md\n",
      "for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n",
      "4. Evaluation Results\n",
      "Base Model\n",
      "Standard Benchmarks\n",
      "|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n",
      "|---|-------------------|----------|--------|-------------|---------------|---------|\n",
      "| | Architecture | - | MoE | Dense | Dense | MoE |\n",
      "| | # Activated Params | - | 21B | 72B | 405B | 37B |\n",
      "| | # Total Params | - | 236B | 72B | 405B | 671B |\n",
      "| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n",
      "| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n",
      "| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n",
      "| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n",
      "| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n",
      "| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n",
      "| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n",
      "| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n",
      "| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n",
      "| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n",
      "| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n",
      "| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n",
      "| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n",
      "| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n",
      "| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n",
      "| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n",
      "| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n",
      "| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n",
      "| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n",
      "| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n",
      "| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n",
      "| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n",
      "| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n",
      "| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n",
      "| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n",
      "| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n",
      "| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n",
      "| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n",
      "| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n",
      "| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n",
      "| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n",
      "| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n",
      "Note: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\n",
      "For more evaluation details, please check our paper.\n",
      "Context Window\n",
      "Evaluation results on the\n",
      "Needle In A Haystack\n",
      "(NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to\n",
      "128K\n",
      ".\n",
      "Chat Model\n",
      "Standard Benchmarks (Models larger than 67B)\n",
      "| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n",
      "|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n",
      "| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n",
      "| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n",
      "| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n",
      "| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n",
      "| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n",
      "| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n",
      "| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n",
      "| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n",
      "| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n",
      "| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n",
      "| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n",
      "| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n",
      "| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n",
      "| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n",
      "| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n",
      "| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n",
      "| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n",
      "| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n",
      "| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n",
      "| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n",
      "| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n",
      "| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n",
      "| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n",
      "| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n",
      "| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n",
      "\n",
      "Note: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n",
      "Open Ended Generation Evaluation\n",
      "| Model | Arena-Hard | AlpacaEval 2.0 |\n",
      "|-------|------------|----------------|\n",
      "| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n",
      "| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n",
      "| LLaMA-3.1 405B | 69.3 | 40.5 |\n",
      "| GPT-4o-0513 | 80.4 | 51.1 |\n",
      "| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n",
      "| DeepSeek-V3 | **85.5** | **70.0** |\n",
      "\n",
      "Note: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n",
      "5. Chat Website & API Platform\n",
      "You can chat with DeepSeek-V3 on DeepSeek's official website:\n",
      "chat.deepseek.com\n",
      "We also provide OpenAI-Compatible API at DeepSeek Platform:\n",
      "platform.deepseek.com\n",
      "6. How to Run Locally\n",
      "DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n",
      "DeepSeek-Infer Demo\n",
      ": We provide a simple and lightweight demo for FP8 and BF16 inference.\n",
      "SGLang\n",
      ": Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n",
      "LMDeploy\n",
      ": Enables efficient FP8 and BF16 inference for local and cloud deployment.\n",
      "TensorRT-LLM\n",
      ": Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n",
      "vLLM\n",
      ": Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n",
      "AMD GPU\n",
      ": Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n",
      "Huawei Ascend NPU\n",
      ": Supports running DeepSeek-V3 on Huawei Ascend devices.\n",
      "Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n",
      "Here is an example of converting FP8 weights to BF16:\n",
      "shell\n",
      "cd inference\n",
      "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n",
      "NOTE: Huggingface's Transformers has not been directly supported yet.\n",
      "6.1 Inference with DeepSeek-Infer Demo (example only)\n",
      "Model Weights & Demo Code Preparation\n",
      "First, clone our DeepSeek-V3 GitHub repository:\n",
      "shell\n",
      "git clone https://github.com/deepseek-ai/DeepSeek-V3.git\n",
      "Navigate to the\n",
      "inference\n",
      "folder and install dependencies listed in\n",
      "requirements.txt\n",
      ".\n",
      "shell\n",
      "cd DeepSeek-V3/inference\n",
      "pip install -r requirements.txt\n",
      "Download the model weights from HuggingFace, and put them into\n",
      "/path/to/DeepSeek-V3\n",
      "folder.\n",
      "Model Weights Conversion\n",
      "Convert HuggingFace model weights to a specific format:\n",
      "shell\n",
      "python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n",
      "Run\n",
      "Then you can chat with DeepSeek-V3:\n",
      "shell\n",
      "torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n",
      "Or batch inference on a given file:\n",
      "shell\n",
      "torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n",
      "6.2 Inference with SGLang (recommended)\n",
      "SGLang\n",
      "currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n",
      "Notably,\n",
      "SGLang v0.4.1\n",
      "fully supports running DeepSeek-V3 on both\n",
      "NVIDIA and AMD GPUs\n",
      ", making it a highly versatile and robust solution.\n",
      "Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n",
      "6.3 Inference with LMDeploy (recommended)\n",
      "LMDeploy\n",
      ", a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n",
      "For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n",
      "6.4 Inference with TRT-LLM (recommended)\n",
      "TensorRT-LLM\n",
      "now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3.\n",
      "6.5 Inference with vLLM (recommended)\n",
      "vLLM\n",
      "v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers\n",
      "pipeline parallelism\n",
      "allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the\n",
      "vLLM instructions\n",
      ". Please feel free to follow\n",
      "the enhancement plan\n",
      "as well.\n",
      "6.6 Recommended Inference Functionality with AMD GPUs\n",
      "In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the\n",
      "SGLang instructions\n",
      ".\n",
      "6.7 Recommended Inference Functionality with Huawei Ascend NPUs\n",
      "The\n",
      "MindIE\n",
      "framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the\n",
      "instructions here\n",
      ".\n",
      "7. License\n",
      "This code repository is licensed under\n",
      "the MIT License\n",
      ". The use of DeepSeek-V3 Base/Chat models is subject to\n",
      "the Model License\n",
      ". DeepSeek-V3 series (including Base and Chat) supports commercial use.\n",
      "8. Citation\n",
      "@misc{deepseekai2024deepseekv3technicalreport,\n",
      "      title={DeepSeek-V3 Technical Report}, \n",
      "      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},\n",
      "      year={2024},\n",
      "      eprint={2412.19437},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.CL},\n",
      "      url={https://arxiv.org/abs/2412.19437}, \n",
      "}\n",
      "9. Contact\n",
      "If you have any questions, please raise an issue or contact us at\n",
      "service@deepseek.com\n",
      ".\n",
      "> RSP * Outperforms leading open-source models and rivals closed models.\n",
      "* Utilizes a unique Mixture-of-Experts architecture with 671B parameters.\n",
      "* Features an economical training process requiring only 2.788M GPU hours.\n",
      "* Implements innovative Multi-head Latent Attention for efficient inference.\n",
      "* Introduces an auxiliary-loss-free load balancing strategy.\n",
      "* Achieves remarkable performance across extensive benchmarks, especially in math and code tasks.\n",
      "* Supports a context window length of up to 128K tokens.\n",
      "* Open-source with permissive licensing for commercial use.\n",
      "* Allows for easy local deployment using various optimized frameworks.\n",
      "* Pioneers advancements in FP8 mixed precision training for large-scale models.\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=PowerInfer/SmallThinker-3B-Preview\n",
      "> RSP {'model_id': 'PowerInfer/SmallThinker-3B-Preview', 'created_at': '12 December 2024 at 11:56:09 UTC', 'downloads': 6996, 'likes': 288, 'trending_score': 217, 'description': \"---\\ndatasets:\\n- PowerInfer/QWQ-LONGCOT-500K\\n- PowerInfer/LONGCOT-Refine-500K\\nbase_model:\\n- Qwen/Qwen2.5-3B-Instruct\\npipeline_tag: text-generation\\nlanguage:\\n- en\\nlibrary_name: transformers\\n---\\n# SmallThinker-3B-preview\\n\\nWe introduce **SmallThinker-3B-preview**, a new model fine-tuned from the [Qwen2.5-3b-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) model. \\n\\n## Benchmark Performance\\n\\n| Model | AIME24 | AMC23 | GAOKAO2024_I | GAOKAO2024_II | MMLU_STEM | AMPS_Hard | math_comp |\\n|---------|--------|-------|--------------|---------------|-----------|-----------|-----------|\\n| Qwen2.5-3B-Instruct | 6.67 | 45 | 50 | 35.8 | 59.8 | - | - |\\n| SmallThinker | 16.667 | 57.5 | 64.2 | 57.1 | 68.2 | 70 | 46.8 |\\n| GPT-4o | 9.3 | - | - | - | 64.2 | 57 | 50 |\\n\\nLimitation: Due to SmallThinker's current limitations in instruction following, for math_comp we adopt a more lenient evaluation method where only correct answers are required, without constraining responses to follow the specified AAAAA format.\\n\\nColab Link: [Colab](https://colab.research.google.com/drive/182q600at0sVw7uX0SXFp6bQI7pyjWXQ2?usp=sharing)\\n## Intended Use Cases\\n\\nSmallThinker is designed for the following use cases:\\n\\n1.  **Edge Deployment:** Its small size makes it ideal for deployment on resource-constrained devices.\\n2.  **Draft Model for QwQ-32B-Preview:** SmallThinker can serve as a fast and efficient draft model for the larger QwQ-32B-Preview model. From my test, in llama.cpp we can get 70% speedup (from 40 tokens/s to 70 tokens/s).\\n\\n## Training Details\\n\\nThe model was trained using 8 H100 GPUs with a global batch size of 16. The specific configuration is as follows:\\n\\nThe SFT (Supervised Fine-Tuning) process was conducted in two phases:\\n\\n1. First Phase:\\n   - Used only the PowerInfer/QWQ-LONGCOT-500K dataset\\n   - Trained for 1.5 epochs\\n```\\n### model\\nmodel_name_or_path: /home/syx/Qwen2.5-3B-Instruct\\n\\n### method\\nstage: sft\\ndo_train: true\\nfinetuning_type: full\\ndeepspeed: examples/deepspeed/ds_z3_config.json\\n\\n### dataset\\ndataset: o1-v2\\ntemplate: qwen\\nneat_packing: true\\ncutoff_len: 16384\\noverwrite_cache: true\\npreprocessing_num_workers: 16\\n\\n### output\\noutput_dir: saves/qwen2-01-qat/full/sft\\nlogging_steps: 1\\nsave_steps: 1000\\nplot_loss: true\\noverwrite_output_dir: true\\n```\\n2. Second Phase:\\n   - Combined training with PowerInfer/QWQ-LONGCOT-500K and PowerInfer/LONGCOT-Refine datasets\\n   - Continued training for 2 additional epochs\\n```\\n### model\\nmodel_name_or_path: saves/qwen2-01-qat/full/sft/checkpoint-24000\\n\\n### method\\nstage: sft\\ndo_train: true\\nfinetuning_type: full\\ndeepspeed: examples/deepspeed/ds_z3_config.json\\n\\n### dataset\\ndataset: o1-v2, o1-v3\\ntemplate: qwen\\nneat_packing: true\\ncutoff_len: 16384\\noverwrite_cache: true\\npreprocessing_num_workers: 16\\n\\n### output\\noutput_dir: saves/qwen2-01-qat/full/sft\\nlogging_steps: 1\\nsave_steps: 1000\\nplot_loss: true\\noverwrite_output_dir: true\\n```\\n\\n## Limitations & Disclaimer\\n\\nPlease be aware of the following limitations:\\n\\n*   **Language Limitation:** The model has only been trained on English-language datasets, hence its capabilities in other languages are still lacking.\\n*   **Limited Knowledge:** Due to limited SFT data and the model's relatively small scale, its reasoning capabilities are constrained by its knowledge base.\\n*   **Unpredictable Outputs:** The model may produce unexpected outputs due to its size and probabilistic generation paradigm. Users should exercise caution and validate the model's responses.\\n*   **Repetition Issue:** The model tends to repeat itself when answering high-difficulty questions. Please increase the `repetition_penalty` to mitigate this issue.\"}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=PowerInfer/SmallThinker-3B-Preview\n",
      "> RSP [{'title': 'SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency', 'url': 'https://pub.towardsai.net/smallthinker-3b-a-small-thinking-model-revolutionizing-ai-efficiency-f528cf7d6906', 'content': 'SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency | by Md Monsur ali | Jan, 2025 | Towards AI SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency How SmallThinker 3B delivers big results with minimal resources, making it the perfect choice for edge computing and mobile AI applications. Published in Towards AI With the introduction of the SmallThinker-3B Preview, PowerInfer seeks to address this balance by offering a compact yet potent model tailored for diverse applications. SmallThinker-3B-Preview is a compact yet powerful AI model developed by PowerInfer, designed to deliver high-quality inference while minimizing computational overhead. Hosted on Hugging Face, the model is easily accessible to developers, researchers, and AI enthusiasts. Follow Published in Towards AI ----------------------- Follow Follow 368 Followers Follow', 'score': 0.90771407, 'raw_content': None}, {'title': 'SmallThinker 3B Preview By PowerInfer: Benchmarks, Features and ...', 'url': 'https://llm.extractum.io/model/PowerInfer/SmallThinker-3B-Preview,6YvWQRdkYbb2o0HqvU7LTJ', 'content': 'SmallThinker 3B Preview by PowerInfer »\\xa0 All LLMs \\xa0»\\xa0 PowerInfer \\xa0»\\xa0 SmallThinker 3B Preview \\xa0\\xa0URL Share it on  Base model:qwen/qwen2.5-3b-ins... Model Card on HF 🤗: https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\\xa0 SmallThinker 3B Preview Benchmarks LLM Name    SmallThinker 3B Preview Repository 🤗    https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\\xa0 Base Model(s)   \\xa0\\xa0Qwen/Qwen2.5-3B-Instruct \\xa0\\xa0Qwen/Qwen2.5-3B-Instruct Model Size  3b Best Alternatives to SmallThinker 3B Preview Qwen2.5 3B Instruct 32K / 6.2\\u2009GB    558811  130 Calme 3.2 Instruct 3B   32K / 11\\u2009GB 3210    0 Qwen2.5 Coder 3B Instruct   32K / 6.2\\u2009GB    66636   24 Qwen2.5 3B RP Thinker V2    32K / 6.8\\u2009GB    98  1 Calme 3.1 Instruct 3B   32K / 11\\u2009GB 2677    2 \"73.2\") means that the model is better than PowerInfer/SmallThinker-3B-Preview. Rank the SmallThinker 3B Preview Capabilities', 'score': 0.8302782, 'raw_content': None}, {'title': 'Testing SmallThinker 3B Preview by PowerInfer - YouTube', 'url': 'https://www.youtube.com/watch?v=OVNnXQp_wNU', 'content': \"In this video, I tested SmallThinker 3B Preview by PowerInfer, an AI model which is currently trending on HuggingFace.It has a good concept, but it's very in\", 'score': 0.81097376, 'raw_content': None}, {'title': 'PowerInfer/SmallThinker-3B-Preview at main - Hugging Face', 'url': 'https://huggingface.co/PowerInfer/SmallThinker-3B-Preview/tree/main', 'content': 'PowerInfer/SmallThinker-3B-Preview at main *   config.json *   generation_config.json 16 Bytes LFSupdate weight 14 days ago 1.67 MB LFSupdate weight 14 days ago 4.96 GB LFSupdate weight 14 days ago *   model.safetensors.index.json 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago Pickle imports What is a pickle import? 1.06 kB LFSupdate weight 14 days ago *   tokenizer.json *   tokenizer_config.json *   trainer_state.json \"transformers.training_args.OptimizerNames\", \"transformers.trainer_utils.HubStrategy\", \"transformers.trainer_pt_utils.AcceleratorConfig\", \"transformers.trainer_utils.SchedulerType\", \"transformers.trainer_utils.IntervalStrategy\" 6.58 kB LFSupdate weight 14 days ago', 'score': 0.7343678, 'raw_content': None}, {'title': 'bartowski/SmallThinker-3B-Preview-GGUF - Hugging Face', 'url': 'https://huggingface.co/bartowski/SmallThinker-3B-Preview-GGUF', 'content': 'Filename Quant type File Size Split Description; SmallThinker-3B-Preview-f32.gguf: f32: 13.59GB: false: Full F32 weights. SmallThinker-3B-Preview-f16.gguf: f16', 'score': 0.572078, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=PowerInfer/SmallThinker-3B-Preview\n",
      "> RSP [{'title': 'PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU', 'url': 'https://arxiv.org/pdf/2312.12456', 'content': 'PowerInfer introduces a high-speed Large Language Model inference engine on a PC with a consumer-grade GPU, reducing memory and data transfer costs.', 'score': 0.41634628, 'raw_content': None}, {'title': 'PDF', 'url': 'https://arxiv.org/pdf/2312.12456v1.pdf', 'content': 'arXiv:2312.12456v1 [cs.LG] 16 Dec 2023 PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU Yixin Song, Zeyu Mi∗, Haotong Xie and Haibo Chen Institute of Parallel and Distributed Systems (IPADS),ShanghaiJiao Tong University', 'score': 0.3864398, 'raw_content': None}, {'title': '[2312.12456] PowerInfer: Fast Large Language Model Serving with a ...', 'url': 'http://export.arxiv.org/abs/2312.12456', 'content': 'cs > arXiv:2312.12456 cs.LG cs cs.OS This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. For the OPT-30B model, PowerInfer achieves performance comparable to that of a high-end server-grade A100 GPU, reaching 82% of its token generation rate on a single consumer-grade RTX 4090 GPU. Subjects:   Machine Learning (cs.LG); Operating Systems (cs.OS) Cite\\xa0as:    arXiv:2312.12456 [cs.LG] (or arXiv:2312.12456v2 [cs.LG] for this version)', 'score': 0.331672, 'raw_content': None}, {'title': 'PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU', 'url': 'https://arxiv.org/html/2312.12456v1', 'content': 'PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. However, these improvements are smaller compared to those on a higher-end PC (PC-High), primarily due to the 11GB GPU memory limitation of PC-Low. This limitation affects the number of neurons that can be allocated to the GPU, particularly for models with around 30B parameters or more, leading to a greater dependence on the CPU for processing a larger number of activated neurons.', 'score': 0.3156732, 'raw_content': None}, {'title': 'PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU', 'url': 'https://arxiv.org/pdf/2312.12456v2', 'content': 'PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU Yixin Song, Zeyu Mi, Haotong Xie and Haibo Chen Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University', 'score': 0.27899086, 'raw_content': None}, {'title': '[2312.12456] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU', 'url': 'https://arxiv.org/abs/2312.12456', 'content': 'cs arXiv:2312.12456 Help | Advanced Search arXiv author ID Help pages This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. Subjects:   Machine Learning (cs.LG); Operating Systems (cs.OS) Cite as:    arXiv:2312.12456 [cs.LG] (or arXiv:2312.12456v1 [cs.LG] for this version) From: Zeyu Mi [view email] Access Paper: cs.LG cs Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? arXiv Operational Status ', 'score': 0.25215882, 'raw_content': None}, {'title': 'PowerInfer-2 : Fast Large Language Model Inference on a Smartphone', 'url': 'https://arxiv.org/html/2406.06282v3', 'content': 'In contrast, high-end PCs’ discrete GPUs efficiently handle both dense and sparse matrix operations compared to CPUs. This hardware limitation significantly impacts the effectiveness of PowerInfer and LLMFlash’s sparse computation approaches on smartphones, as sparse computations are forced to run on CPUs. Such CPU-only execution not only underutilizes the NPU’s computational capabilities but also fails to fully exploit the high memory bandwidth available in smartphones’ unified memory architecture (UMA). (a) The prefill phase uses an NPU-centric workflow that leverages NPU for computation; (b) The decoding phase employs a CPU-NPU hybrid workflow for FFN computation where NPU handles dense computations for hot neurons while CPU cores process sparse computations for cold neurons, with their processing ratio automatically adjusting to match the dynamic sparsity patterns caused by varying batch sizes.', 'score': 0.21727978, 'raw_content': None}, {'title': 'PowerInfer-2 : Fast Large Language Model Inference on a Smartphone', 'url': 'https://arxiv.org/html/2406.06282', 'content': 'In contrast, high-end PCs’ discrete GPUs efficiently handle both dense and sparse matrix operations compared to CPUs. This hardware limitation significantly impacts the effectiveness of PowerInfer and LLMFlash’s sparse computation approaches on smartphones, as sparse computations are forced to run on CPUs. Such CPU-only execution not only underutilizes the NPU’s computational capabilities but also fails to fully exploit the high memory bandwidth available in smartphones’ unified memory architecture (UMA). (a) The prefill phase uses an NPU-centric workflow that leverages NPU for computation; (b) The decoding phase employs a CPU-NPU hybrid workflow for FFN computation where NPU handles dense computations for hot neurons while CPU cores process sparse computations for cold neurons, with their processing ratio automatically adjusting to match the dynamic sparsity patterns caused by varying batch sizes.', 'score': 0.21727978, 'raw_content': None}, {'title': 'PowerInfer-2: Fast Large Language Model Inference on a Smartphone', 'url': 'https://arxiv.org/pdf/2406.06282v3', 'content': 'PowerInfer-2: Fast Large Language Model Inference on a Smartphone Zhenliang Xue*, Yixin Song*, Zeyu Mi , Xinrui Zheng, Yubin Xia, and Haibo Chen', 'score': 0.20032328, 'raw_content': None}, {'title': 'PowerInfer-2: Fast Large Language Model Inference on a Smartphone', 'url': 'https://arxiv.org/pdf/2406.06282v1', 'content': 'PowerInfer-2: Fast Large Language Model Inference on a Smartphone Zhenliang Xue*, Yixin Song*, Zeyu Mi , Le Chen, Yubin Xia, and Haibo Chen Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University', 'score': 0.1839162, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2406.06282\n",
      "> RSP [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](http://arxiv.org/abs/2406.06282v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2312.12456\n",
      "> RSP [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](http://arxiv.org/abs/2312.12456v2)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency', 'url': 'https://pub.towardsai.net/smallthinker-3b-a-small-thinking-model-revolutionizing-ai-efficiency-f528cf7d6906', 'content': 'SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency | by Md Monsur ali | Jan, 2025 | Towards AI SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency How SmallThinker 3B delivers big results with minimal resources, making it the perfect choice for edge computing and mobile AI applications. Published in Towards AI With the introduction of the SmallThinker-3B Preview, PowerInfer seeks to address this balance by offering a compact yet potent model tailored for diverse applications. SmallThinker-3B-Preview is a compact yet powerful AI model developed by PowerInfer, designed to deliver high-quality inference while minimizing computational overhead. Hosted on Hugging Face, the model is easily accessible to developers, researchers, and AI enthusiasts. Follow Published in Towards AI ----------------------- Follow Follow 368 Followers Follow', 'score': 0.90771407, 'raw_content': None}, {'title': 'SmallThinker 3B Preview By PowerInfer: Benchmarks, Features and ...', 'url': 'https://llm.extractum.io/model/PowerInfer/SmallThinker-3B-Preview,6YvWQRdkYbb2o0HqvU7LTJ', 'content': 'SmallThinker 3B Preview by PowerInfer »\\xa0 All LLMs \\xa0»\\xa0 PowerInfer \\xa0»\\xa0 SmallThinker 3B Preview \\xa0\\xa0URL Share it on  Base model:qwen/qwen2.5-3b-ins... Model Card on HF 🤗: https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\\xa0 SmallThinker 3B Preview Benchmarks LLM Name    SmallThinker 3B Preview Repository 🤗    https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\\xa0 Base Model(s)   \\xa0\\xa0Qwen/Qwen2.5-3B-Instruct \\xa0\\xa0Qwen/Qwen2.5-3B-Instruct Model Size  3b Best Alternatives to SmallThinker 3B Preview Qwen2.5 3B Instruct 32K / 6.2\\u2009GB    558811  130 Calme 3.2 Instruct 3B   32K / 11\\u2009GB 3210    0 Qwen2.5 Coder 3B Instruct   32K / 6.2\\u2009GB    66636   24 Qwen2.5 3B RP Thinker V2    32K / 6.8\\u2009GB    98  1 Calme 3.1 Instruct 3B   32K / 11\\u2009GB 2677    2 \"73.2\") means that the model is better than PowerInfer/SmallThinker-3B-Preview. Rank the SmallThinker 3B Preview Capabilities', 'score': 0.8302782, 'raw_content': None}, {'title': 'Testing SmallThinker 3B Preview by PowerInfer - YouTube', 'url': 'https://www.youtube.com/watch?v=OVNnXQp_wNU', 'content': \"In this video, I tested SmallThinker 3B Preview by PowerInfer, an AI model which is currently trending on HuggingFace.It has a good concept, but it's very in\", 'score': 0.81097376, 'raw_content': None}, {'title': 'PowerInfer/SmallThinker-3B-Preview at main - Hugging Face', 'url': 'https://huggingface.co/PowerInfer/SmallThinker-3B-Preview/tree/main', 'content': 'PowerInfer/SmallThinker-3B-Preview at main *   config.json *   generation_config.json 16 Bytes LFSupdate weight 14 days ago 1.67 MB LFSupdate weight 14 days ago 4.96 GB LFSupdate weight 14 days ago *   model.safetensors.index.json 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago Pickle imports What is a pickle import? 1.06 kB LFSupdate weight 14 days ago *   tokenizer.json *   tokenizer_config.json *   trainer_state.json \"transformers.training_args.OptimizerNames\", \"transformers.trainer_utils.HubStrategy\", \"transformers.trainer_pt_utils.AcceleratorConfig\", \"transformers.trainer_utils.SchedulerType\", \"transformers.trainer_utils.IntervalStrategy\" 6.58 kB LFSupdate weight 14 days ago', 'score': 0.7343678, 'raw_content': None}, {'title': 'bartowski/SmallThinker-3B-Preview-GGUF - Hugging Face', 'url': 'https://huggingface.co/bartowski/SmallThinker-3B-Preview-GGUF', 'content': 'Filename Quant type File Size Split Description; SmallThinker-3B-Preview-f32.gguf: f32: 13.59GB: false: Full F32 weights. SmallThinker-3B-Preview-f16.gguf: f16', 'score': 0.572078, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "datasets:\n",
      "- PowerInfer/QWQ-LONGCOT-500K\n",
      "- PowerInfer/LONGCOT-Refine-500K\n",
      "base_model:\n",
      "- Qwen/Qwen2.5-3B-Instruct\n",
      "pipeline_tag: text-generation\n",
      "language:\n",
      "- en\n",
      "library_name: transformers\n",
      "SmallThinker-3B-preview\n",
      "We introduce\n",
      "SmallThinker-3B-preview\n",
      ", a new model fine-tuned from the\n",
      "Qwen2.5-3b-Instruct\n",
      "model.\n",
      "Benchmark Performance\n",
      "| Model | AIME24 | AMC23 | GAOKAO2024_I | GAOKAO2024_II | MMLU_STEM | AMPS_Hard | math_comp |\n",
      "|---------|--------|-------|--------------|---------------|-----------|-----------|-----------|\n",
      "| Qwen2.5-3B-Instruct | 6.67 | 45 | 50 | 35.8 | 59.8 | - | - |\n",
      "| SmallThinker | 16.667 | 57.5 | 64.2 | 57.1 | 68.2 | 70 | 46.8 |\n",
      "| GPT-4o | 9.3 | - | - | - | 64.2 | 57 | 50 |\n",
      "Limitation: Due to SmallThinker's current limitations in instruction following, for math_comp we adopt a more lenient evaluation method where only correct answers are required, without constraining responses to follow the specified AAAAA format.\n",
      "Colab Link:\n",
      "Colab\n",
      "Intended Use Cases\n",
      "SmallThinker is designed for the following use cases:\n",
      "Edge Deployment:\n",
      "Its small size makes it ideal for deployment on resource-constrained devices.\n",
      "Draft Model for QwQ-32B-Preview:\n",
      "SmallThinker can serve as a fast and efficient draft model for the larger QwQ-32B-Preview model. From my test, in llama.cpp we can get 70% speedup (from 40 tokens/s to 70 tokens/s).\n",
      "Training Details\n",
      "The model was trained using 8 H100 GPUs with a global batch size of 16. The specific configuration is as follows:\n",
      "The SFT (Supervised Fine-Tuning) process was conducted in two phases:\n",
      "First Phase:\n",
      "Used only the PowerInfer/QWQ-LONGCOT-500K dataset\n",
      "Trained for 1.5 epochs\n",
      "```\n",
      "model\n",
      "model_name_or_path: /home/syx/Qwen2.5-3B-Instruct\n",
      "method\n",
      "stage: sft\n",
      "do_train: true\n",
      "finetuning_type: full\n",
      "deepspeed: examples/deepspeed/ds_z3_config.json\n",
      "dataset\n",
      "dataset: o1-v2\n",
      "template: qwen\n",
      "neat_packing: true\n",
      "cutoff_len: 16384\n",
      "overwrite_cache: true\n",
      "preprocessing_num_workers: 16\n",
      "output\n",
      "output_dir: saves/qwen2-01-qat/full/sft\n",
      "logging_steps: 1\n",
      "save_steps: 1000\n",
      "plot_loss: true\n",
      "overwrite_output_dir: true\n",
      "2. Second Phase:\n",
      "   - Combined training with PowerInfer/QWQ-LONGCOT-500K and PowerInfer/LONGCOT-Refine datasets\n",
      "   - Continued training for 2 additional epochs\n",
      "model\n",
      "model_name_or_path: saves/qwen2-01-qat/full/sft/checkpoint-24000\n",
      "method\n",
      "stage: sft\n",
      "do_train: true\n",
      "finetuning_type: full\n",
      "deepspeed: examples/deepspeed/ds_z3_config.json\n",
      "dataset\n",
      "dataset: o1-v2, o1-v3\n",
      "template: qwen\n",
      "neat_packing: true\n",
      "cutoff_len: 16384\n",
      "overwrite_cache: true\n",
      "preprocessing_num_workers: 16\n",
      "output\n",
      "output_dir: saves/qwen2-01-qat/full/sft\n",
      "logging_steps: 1\n",
      "save_steps: 1000\n",
      "plot_loss: true\n",
      "overwrite_output_dir: true\n",
      "```\n",
      "Limitations & Disclaimer\n",
      "Please be aware of the following limitations:\n",
      "Language Limitation:\n",
      "The model has only been trained on English-language datasets, hence its capabilities in other languages are still lacking.\n",
      "Limited Knowledge:\n",
      "Due to limited SFT data and the model's relatively small scale, its reasoning capabilities are constrained by its knowledge base.\n",
      "Unpredictable Outputs:\n",
      "The model may produce unexpected outputs due to its size and probabilistic generation paradigm. Users should exercise caution and validate the model's responses.\n",
      "Repetition Issue:\n",
      "The model tends to repeat itself when answering high-difficulty questions. Please increase the\n",
      "repetition_penalty\n",
      "to mitigate this issue.\n",
      "> RSP * Compact size ideal for resource-constrained devices\n",
      "* High-quality inference with minimal computational overhead\n",
      "* Designed for edge deployment and mobile applications\n",
      "* Accelerated performance compared to larger models (up to 70% speedup)\n",
      "* Fine-tuned on specialized datasets for improved accuracy\n",
      "* Strong benchmark performance surpassing some larger models\n",
      "* Versatile applications as a draft model for larger models\n",
      "* Trained with advanced techniques like Supervised Fine-Tuning (SFT)\n",
      "* Accessible on platforms like Hugging Face for easy integration\n",
      "* Holds potential for further enhancement with more extensive datasets and training\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=deepseek-ai/DeepSeek-V3-Base\n",
      "> RSP {'model_id': 'deepseek-ai/DeepSeek-V3-Base', 'created_at': '25 December 2024 at 12:52:06 UTC', 'downloads': 8663, 'likes': 1180, 'trending_score': 188, 'description': '<!-- markdownlint-disable first-line-h1 -->\\n<!-- markdownlint-disable html -->\\n<!-- markdownlint-disable no-duplicate-header -->\\n\\n<div align=\"center\">\\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\\n</div>\\n<hr>\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n\\n<p align=\"center\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>👁️</a>\\n</p>\\n\\n\\n## 1. Introduction\\n\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \\nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \\nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \\nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \\nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\\nIn addition, its training process is remarkably stable. \\nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/benchmark.png\">\\n</p>\\n\\n## 2. Model Summary\\n\\n---\\n\\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\\n\\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \\n    It can also be used for speculative decoding for inference acceleration. \\n\\n---\\n\\n**Pre-Training: Towards Ultimate Training Efficiency**\\n\\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \\n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \\n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \\n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\\n\\n---\\n\\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\\n\\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\\n\\n---\\n\\n\\n## 3. Model Downloads\\n\\n<div align=\"center\">\\n\\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\\n| :------------: | :------------: | :------------: | :------------: | :------------: |\\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\\n\\n</div>\\n\\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\\n\\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\\n\\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\\n\\n## 4. Evaluation Results\\n### Base Model\\n#### Standard Benchmarks\\n\\n<div align=\"center\">\\n\\n\\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\\n|---|-------------------|----------|--------|-------------|---------------|---------|\\n| | Architecture | - | MoE | Dense | Dense | MoE |\\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\\n| | # Total Params | - | 236B | 72B | 405B | 671B |\\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\\n\\n</div>\\n\\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\\nFor more evaluation details, please check our paper. \\n\\n#### Context Window\\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/niah.png\">\\n</p>\\n\\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \\n\\n### Chat Model\\n#### Standard Benchmarks (Models larger than 67B)\\n<div align=\"center\">\\n\\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\\n\\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\\n\\n</div>\\n\\n\\n####  Open Ended Generation Evaluation\\n\\n<div align=\"center\">\\n\\n\\n\\n| Model | Arena-Hard | AlpacaEval 2.0 |\\n|-------|------------|----------------|\\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\\n| LLaMA-3.1 405B | 69.3 | 40.5 |\\n| GPT-4o-0513 | 80.4 | 51.1 |\\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\\n| DeepSeek-V3 | **85.5** | **70.0** |\\n\\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\\n</div>\\n\\n\\n## 5. Chat Website & API Platform\\nYou can chat with DeepSeek-V3 on DeepSeek\\'s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\\n\\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\\n\\n## 6. How to Run Locally\\n\\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\\n\\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\\n\\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\\n\\nHere is an example of converting FP8 weights to BF16:\\n\\n```shell\\ncd inference\\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\\n```\\n\\n**NOTE: Huggingface\\'s Transformers has not been directly supported yet.**\\n\\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\\n\\n#### Model Weights & Demo Code Preparation\\n\\nFirst, clone our DeepSeek-V3 GitHub repository:\\n\\n```shell\\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\\n```\\n\\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\\n\\n```shell\\ncd DeepSeek-V3/inference\\npip install -r requirements.txt\\n```\\n\\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\\n\\n#### Model Weights Conversion\\n\\nConvert HuggingFace model weights to a specific format:\\n\\n```shell\\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\\n```\\n\\n#### Run\\n\\nThen you can chat with DeepSeek-V3:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\\n```\\n\\nOr batch inference on a given file:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\\n```\\n\\n### 6.2 Inference with SGLang (recommended)\\n\\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\\n\\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\\n\\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\\n\\n### 6.3 Inference with LMDeploy (recommended)\\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\\n\\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\\n\\n\\n### 6.4 Inference with TRT-LLM (recommended)\\n\\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \\n\\n### 6.5 Inference with vLLM (recommended)\\n\\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\\n\\n### 6.6 Recommended Inference Functionality with AMD GPUs\\n\\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\\n\\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\\n\\n\\n## 7. License\\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\\n\\n## 8. Citation\\n```\\n@misc{deepseekai2024deepseekv3technicalreport,\\n      title={DeepSeek-V3 Technical Report}, \\n      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},\\n      year={2024},\\n      eprint={2412.19437},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.19437}, \\n}\\n```\\n\\n## 9. Contact\\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\\n'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=deepseek-ai/DeepSeek-V3-Base\n",
      "> RSP [{'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3-Base achieves the best performance on most benchmarks, especially on math and code tasks. 4.4.2.Evaluation Results. In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B', 'score': 0.7463059, 'raw_content': None}, {'title': 'Paper page - DeepSeek-V3 Technical Report - Hugging Face', 'url': 'https://huggingface.co/papers/2412.19437', 'content': 'Paper page - DeepSeek-V3 Technical Report DeepSeek-V3 Technical Report Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. Comment· Sign up or log in to comment Models citing this paper 3 #### deepseek-ai/DeepSeek-V3-Base Updated 4 days ago • 6.64k • 1.1k #### deepseek-ai/DeepSeek-V3 Updated 4 days ago • 45.5k • 988 #### bullerwins/DeepSeek-V3-split Updated about 9 hours ago Datasets citing this paper 0 Spaces citing this paper 7 #### Deepseek Papers Collection Deepseek papers collection • 14 items • Updated 4 days ago • 8 #### LLM Technical Report Collection 22 items • Updated 3 days ago • 2 #### Papers Collection 40 items • Updated 1 day ago Models Datasets Spaces Pricing Docs', 'score': 0.73868835, 'raw_content': None}, {'title': 'deepseek-ai/DeepSeek-V3-Base', 'url': 'https://simonwillison.net/2024/Dec/25/deepseek-v3/', 'content': 'deepseek-ai/DeepSeek-V3-Base deepseek-ai/DeepSeek-V3-Base (via) No model card or announcement yet, but this new model release from Chinese AI lab DeepSeek (an arm of Chinese hedge fund High-Flyer) looks very significant. The new model is apparently available to some people via both chat.deepseek.com and the DeepSeek API as part of a staged rollout. I never know if I can believe models or not (the first time I asked \"what model are you?\" it claimed to be \"based on OpenAI\\'s GPT-4 architecture\"), but I just got this result using LLM and the llm-deepseek plugin: llm -m deepseek-chat \\'what deepseek model are you?\\' Trying out QvQ - Qwen\\'s new visual reasoning model - 24th December 2024', 'score': 0.7190094, 'raw_content': None}, {'title': 'DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat', 'url': 'https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/', 'content': 'DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch | VentureBeat DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B.', 'score': 0.7161595, 'raw_content': None}, {'title': 'deepseek-ai/DeepSeek-V3 - GitHub', 'url': 'https://github.com/deepseek-ai/DeepSeek-V3', 'content': 'At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights. SGLang: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes. AMD GPU: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes. LMDeploy, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. The use of DeepSeek-V3 Base/Chat models is subject to the Model License.', 'score': 0.68344116, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=deepseek-ai/DeepSeek-V3-Base\n",
      "> RSP [{'title': '[2412.19437] DeepSeek-V3 Technical Report - export.arxiv.org', 'url': 'http://export.arxiv.org/abs/2412.19437', 'content': 'cs > arXiv:2412.19437 cs.CL cs cs.AI We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite\\xa0as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) Which authors of this paper are endorsers?', 'score': 0.6055431, 'raw_content': None}, {'title': '[2412.19437] DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/abs/2412.19437', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. arXiv:2412.19437 arXiv author ID DeepSeek-V3 Technical Report We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.19437 Bibliographic and Citation Tools Connected Papers Toggle\", 'score': 0.5097406, 'raw_content': None}, {'title': '[2412.19437v1] DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/abs/2412.19437v1', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. arXiv:2412.19437v1 arXiv author ID DeepSeek-V3 Technical Report We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.19437 Bibliographic and Citation Tools Connected Papers Toggle\", 'score': 0.50388235, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total ... on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the', 'score': 0.49392182, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/html/2412.19437v1', 'content': 'Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention\\xa0(MLA)\\xa0(DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE\\xa0(Dai et\\xa0al., 2024) for cost-effective training. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy\\xa0(Wang et\\xa0al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models.', 'score': 0.43842506, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/html/2412.19437', 'content': 'Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention\\xa0(MLA)\\xa0(DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE\\xa0(Dai et\\xa0al., 2024) for cost-effective training. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy\\xa0(Wang et\\xa0al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models.', 'score': 0.43842506, 'raw_content': None}, {'title': 'DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open ...', 'url': 'https://arxiv.org/abs/2402.03300', 'content': 'cs arXiv:2402.03300 Help | Advanced Search arXiv author ID DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2402.03300 [cs.CL] (or arXiv:2402.03300v3 [cs.CL] for this version) From: Zhihong Shao [view email] Access Paper: cs.CL cs cs.AI References & Citations Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? Help', 'score': 0.31459826, 'raw_content': None}, {'title': 'DeepSeek LLM: Scaling Open-Source Language Models with Longtermism', 'url': 'https://arxiv.org/abs/2401.02954', 'content': 'cs arXiv:2401.02954 arXiv author ID DeepSeek LLM: Scaling Open-Source Language Models with Longtermism The rapid development of open-source large language models (LLMs) has been truly remarkable. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2401.02954 [cs.CL] (or arXiv:2401.02954v1 [cs.CL] for this version) cs.CL cs cs.AI Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle', 'score': 0.30731693, 'raw_content': None}, {'title': 'DeepSeek-VL: Towards Real-World Vision-Language Understanding', 'url': 'http://export.arxiv.org/abs/2403.05525', 'content': 'cs > arXiv:2403.05525 cs.AI cs DeepSeek-VL: Towards Real-World Vision-Language Understanding We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a vision-language chatbot in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language benchmarks at the same model size while maintaining robust performance on language-centric benchmarks. Subjects:   Artificial Intelligence (cs.AI) Cite\\xa0as:    arXiv:2403.05525 [cs.AI] (or arXiv:2403.05525v2 [cs.AI] for this version) Which authors of this paper are endorsers?', 'score': 0.28048873, 'raw_content': None}, {'title': 'DeepSeek-VL2: Mixture-of-Experts Vision-Language Models', 'url': 'https://web3.arxiv.org/pdf/2412.10302', 'content': 'DeepSeek-AI Abstract We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two ... The pre-trained SigLIP operates at a base resolution of 384 ×384. To', 'score': 0.27226803, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.10302\n",
      "> RSP [DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding](http://arxiv.org/abs/2412.10302v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2401.02954\n",
      "> RSP [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/abs/2401.02954v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2403.05525\n",
      "> RSP [DeepSeek-VL: Towards Real-World Vision-Language Understanding](http://arxiv.org/abs/2403.05525v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.19437\n",
      "> RSP [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2402.03300\n",
      "> RSP [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/abs/2402.03300v3)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3-Base achieves the best performance on most benchmarks, especially on math and code tasks. 4.4.2.Evaluation Results. In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B', 'score': 0.7463059, 'raw_content': None}, {'title': 'Paper page - DeepSeek-V3 Technical Report - Hugging Face', 'url': 'https://huggingface.co/papers/2412.19437', 'content': 'Paper page - DeepSeek-V3 Technical Report DeepSeek-V3 Technical Report Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. Comment· Sign up or log in to comment Models citing this paper 3 #### deepseek-ai/DeepSeek-V3-Base Updated 4 days ago • 6.64k • 1.1k #### deepseek-ai/DeepSeek-V3 Updated 4 days ago • 45.5k • 988 #### bullerwins/DeepSeek-V3-split Updated about 9 hours ago Datasets citing this paper 0 Spaces citing this paper 7 #### Deepseek Papers Collection Deepseek papers collection • 14 items • Updated 4 days ago • 8 #### LLM Technical Report Collection 22 items • Updated 3 days ago • 2 #### Papers Collection 40 items • Updated 1 day ago Models Datasets Spaces Pricing Docs', 'score': 0.73868835, 'raw_content': None}, {'title': 'deepseek-ai/DeepSeek-V3-Base', 'url': 'https://simonwillison.net/2024/Dec/25/deepseek-v3/', 'content': 'deepseek-ai/DeepSeek-V3-Base deepseek-ai/DeepSeek-V3-Base (via) No model card or announcement yet, but this new model release from Chinese AI lab DeepSeek (an arm of Chinese hedge fund High-Flyer) looks very significant. The new model is apparently available to some people via both chat.deepseek.com and the DeepSeek API as part of a staged rollout. I never know if I can believe models or not (the first time I asked \"what model are you?\" it claimed to be \"based on OpenAI\\'s GPT-4 architecture\"), but I just got this result using LLM and the llm-deepseek plugin: llm -m deepseek-chat \\'what deepseek model are you?\\' Trying out QvQ - Qwen\\'s new visual reasoning model - 24th December 2024', 'score': 0.7190094, 'raw_content': None}, {'title': 'DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat', 'url': 'https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/', 'content': 'DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch | VentureBeat DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B.', 'score': 0.7161595, 'raw_content': None}, {'title': 'deepseek-ai/DeepSeek-V3 - GitHub', 'url': 'https://github.com/deepseek-ai/DeepSeek-V3', 'content': 'At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights. SGLang: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes. AMD GPU: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes. LMDeploy, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. The use of DeepSeek-V3 Base/Chat models is subject to the Model License.', 'score': 0.68344116, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "Paper Link\n",
      "👁️\n",
      "1. Introduction\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \n",
      "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \n",
      "Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \n",
      "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \n",
      "Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\n",
      "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\n",
      "In addition, its training process is remarkably stable. \n",
      "Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "2. Model Summary\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n",
      "We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n",
      "    It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.\n",
      "This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.\n",
      "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n",
      "3. Model Downloads\n",
      "| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n",
      "| :------------: | :------------: | :------------: | :------------: | :------------: |\n",
      "| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n",
      "| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n",
      "NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.\n",
      "To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6:\n",
      "How_to Run_Locally\n",
      ".\n",
      "For developers looking to dive deeper, we recommend exploring\n",
      "README_WEIGHTS.md\n",
      "for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n",
      "4. Evaluation Results\n",
      "Base Model\n",
      "Standard Benchmarks\n",
      "|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n",
      "|---|-------------------|----------|--------|-------------|---------------|---------|\n",
      "| | Architecture | - | MoE | Dense | Dense | MoE |\n",
      "| | # Activated Params | - | 21B | 72B | 405B | 37B |\n",
      "| | # Total Params | - | 236B | 72B | 405B | 671B |\n",
      "| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n",
      "| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n",
      "| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n",
      "| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n",
      "| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n",
      "| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n",
      "| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n",
      "| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n",
      "| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n",
      "| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n",
      "| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n",
      "| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n",
      "| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n",
      "| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n",
      "| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n",
      "| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n",
      "| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n",
      "| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n",
      "| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n",
      "| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n",
      "| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n",
      "| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n",
      "| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n",
      "| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n",
      "| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n",
      "| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n",
      "| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n",
      "| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n",
      "| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n",
      "| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n",
      "| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n",
      "| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n",
      "Note: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\n",
      "For more evaluation details, please check our paper.\n",
      "Context Window\n",
      "Evaluation results on the\n",
      "Needle In A Haystack\n",
      "(NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to\n",
      "128K\n",
      ".\n",
      "Chat Model\n",
      "Standard Benchmarks (Models larger than 67B)\n",
      "| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n",
      "|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n",
      "| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n",
      "| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n",
      "| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n",
      "| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n",
      "| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n",
      "| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n",
      "| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n",
      "| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n",
      "| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n",
      "| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n",
      "| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n",
      "| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n",
      "| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n",
      "| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n",
      "| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n",
      "| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n",
      "| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n",
      "| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n",
      "| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n",
      "| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n",
      "| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n",
      "| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n",
      "| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n",
      "| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n",
      "| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n",
      "\n",
      "Note: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n",
      "Open Ended Generation Evaluation\n",
      "| Model | Arena-Hard | AlpacaEval 2.0 |\n",
      "|-------|------------|----------------|\n",
      "| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n",
      "| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n",
      "| LLaMA-3.1 405B | 69.3 | 40.5 |\n",
      "| GPT-4o-0513 | 80.4 | 51.1 |\n",
      "| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n",
      "| DeepSeek-V3 | **85.5** | **70.0** |\n",
      "\n",
      "Note: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n",
      "5. Chat Website & API Platform\n",
      "You can chat with DeepSeek-V3 on DeepSeek's official website:\n",
      "chat.deepseek.com\n",
      "We also provide OpenAI-Compatible API at DeepSeek Platform:\n",
      "platform.deepseek.com\n",
      "6. How to Run Locally\n",
      "DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n",
      "DeepSeek-Infer Demo\n",
      ": We provide a simple and lightweight demo for FP8 and BF16 inference.\n",
      "SGLang\n",
      ": Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n",
      "LMDeploy\n",
      ": Enables efficient FP8 and BF16 inference for local and cloud deployment.\n",
      "TensorRT-LLM\n",
      ": Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n",
      "vLLM\n",
      ": Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n",
      "AMD GPU\n",
      ": Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n",
      "Huawei Ascend NPU\n",
      ": Supports running DeepSeek-V3 on Huawei Ascend devices.\n",
      "Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n",
      "Here is an example of converting FP8 weights to BF16:\n",
      "shell\n",
      "cd inference\n",
      "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n",
      "NOTE: Huggingface's Transformers has not been directly supported yet.\n",
      "6.1 Inference with DeepSeek-Infer Demo (example only)\n",
      "Model Weights & Demo Code Preparation\n",
      "First, clone our DeepSeek-V3 GitHub repository:\n",
      "shell\n",
      "git clone https://github.com/deepseek-ai/DeepSeek-V3.git\n",
      "Navigate to the\n",
      "inference\n",
      "folder and install dependencies listed in\n",
      "requirements.txt\n",
      ".\n",
      "shell\n",
      "cd DeepSeek-V3/inference\n",
      "pip install -r requirements.txt\n",
      "Download the model weights from HuggingFace, and put them into\n",
      "/path/to/DeepSeek-V3\n",
      "folder.\n",
      "Model Weights Conversion\n",
      "Convert HuggingFace model weights to a specific format:\n",
      "shell\n",
      "python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n",
      "Run\n",
      "Then you can chat with DeepSeek-V3:\n",
      "shell\n",
      "torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n",
      "Or batch inference on a given file:\n",
      "shell\n",
      "torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n",
      "6.2 Inference with SGLang (recommended)\n",
      "SGLang\n",
      "currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n",
      "Notably,\n",
      "SGLang v0.4.1\n",
      "fully supports running DeepSeek-V3 on both\n",
      "NVIDIA and AMD GPUs\n",
      ", making it a highly versatile and robust solution.\n",
      "Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n",
      "6.3 Inference with LMDeploy (recommended)\n",
      "LMDeploy\n",
      ", a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n",
      "For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n",
      "6.4 Inference with TRT-LLM (recommended)\n",
      "TensorRT-LLM\n",
      "now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3.\n",
      "6.5 Inference with vLLM (recommended)\n",
      "vLLM\n",
      "v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers\n",
      "pipeline parallelism\n",
      "allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the\n",
      "vLLM instructions\n",
      ". Please feel free to follow\n",
      "the enhancement plan\n",
      "as well.\n",
      "6.6 Recommended Inference Functionality with AMD GPUs\n",
      "In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the\n",
      "SGLang instructions\n",
      ".\n",
      "6.7 Recommended Inference Functionality with Huawei Ascend NPUs\n",
      "The\n",
      "MindIE\n",
      "framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the\n",
      "instructions here\n",
      ".\n",
      "7. License\n",
      "This code repository is licensed under\n",
      "the MIT License\n",
      ". The use of DeepSeek-V3 Base/Chat models is subject to\n",
      "the Model License\n",
      ". DeepSeek-V3 series (including Base and Chat) supports commercial use.\n",
      "8. Citation\n",
      "@misc{deepseekai2024deepseekv3technicalreport,\n",
      "      title={DeepSeek-V3 Technical Report}, \n",
      "      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},\n",
      "      year={2024},\n",
      "      eprint={2412.19437},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.CL},\n",
      "      url={https://arxiv.org/abs/2412.19437}, \n",
      "}\n",
      "9. Contact\n",
      "If you have any questions, please raise an issue or contact us at\n",
      "service@deepseek.com\n",
      ".\n",
      "> RSP * Outperforms existing leading open-source models in benchmarks.\n",
      "* Strong performance on specialized tasks like math and code generation.\n",
      "* Efficient training requiring only 2.788M H800 GPU hours.\n",
      "* Utilizes a Mixture-of-Experts (MoE) architecture with 671B total parameters.\n",
      "* Features innovative Multi-Token Prediction (MTP) training objective.\n",
      "* Pioneers an auxiliary-loss-free load balancing strategy.\n",
      "* Demonstrates stability throughout the training process without significant loss spikes.\n",
      "* Offers competitive performance against closed-source models from major companies.\n",
      "* Supports both FP8 and BF16 inference modes for versatile deployment.\n",
      "* Capable of processing up to 128K context length for extensive input handling.\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=black-forest-labs/FLUX.1-dev\n",
      "> RSP {'model_id': 'black-forest-labs/FLUX.1-dev', 'created_at': '31 July 2024 at 21:13:44 UTC', 'downloads': 1166389, 'likes': 7800, 'trending_score': 182, 'description': '---\\nlanguage:\\n- en\\nlicense: other\\nlicense_name: flux-1-dev-non-commercial-license\\nlicense_link: LICENSE.md\\nextra_gated_prompt: By clicking \"Agree\", you agree to the [FluxDev Non-Commercial License Agreement](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)\\n  and acknowledge the [Acceptable Use Policy](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/POLICY.md).\\ntags:\\n- text-to-image\\n- image-generation\\n- flux\\n---\\n\\n![FLUX.1 [dev] Grid](./dev_grid.jpg)\\n\\n`FLUX.1 [dev]` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.\\nFor more information, please read our [blog post](https://blackforestlabs.ai/announcing-black-forest-labs/).\\n\\n# Key Features\\n1. Cutting-edge output quality, second only to our state-of-the-art model `FLUX.1 [pro]`.\\n2. Competitive prompt following, matching the performance of closed source alternatives .\\n3. Trained using guidance distillation, making `FLUX.1 [dev]` more efficient.\\n4. Open weights to drive new scientific research, and empower artists to develop innovative workflows.\\n5. Generated outputs can be used for personal, scientific, and commercial purposes as described in the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).\\n\\n# Usage\\nWe provide a reference implementation of `FLUX.1 [dev]`, as well as sampling code, in a dedicated [github repository](https://github.com/black-forest-labs/flux).\\nDevelopers and creatives looking to build on top of `FLUX.1 [dev]` are encouraged to use this as a starting point.\\n\\n## API Endpoints\\nThe FLUX.1 models are also available via API from the following sources\\n- [bfl.ml](https://docs.bfl.ml/) (currently `FLUX.1 [pro]`)\\n- [replicate.com](https://replicate.com/collections/flux)\\n- [fal.ai](https://fal.ai/models/fal-ai/flux/dev)\\n- [mystic.ai](https://www.mystic.ai/black-forest-labs/flux1-dev)\\n\\n## ComfyUI\\n`FLUX.1 [dev]` is also available in [Comfy UI](https://github.com/comfyanonymous/ComfyUI) for local inference with a node-based workflow.\\n\\n## Diffusers\\n\\nTo use `FLUX.1 [dev]` with the 🧨 diffusers python library, first install or upgrade diffusers\\n\\n```shell\\npip install -U diffusers\\n```\\n\\nThen you can use `FluxPipeline` to run the model\\n\\n```python\\nimport torch\\nfrom diffusers import FluxPipeline\\n\\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\\npipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\\n\\nprompt = \"A cat holding a sign that says hello world\"\\nimage = pipe(\\n    prompt,\\n    height=1024,\\n    width=1024,\\n    guidance_scale=3.5,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=torch.Generator(\"cpu\").manual_seed(0)\\n).images[0]\\nimage.save(\"flux-dev.png\")\\n```\\n\\nTo learn more check out the [diffusers](https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux) documentation\\n\\n---\\n# Limitations\\n- This model is not intended or able to provide factual information.\\n- As a statistical model this checkpoint might amplify existing societal biases.\\n- The model may fail to generate output that matches the prompts.\\n- Prompt following is heavily influenced by the prompting-style.\\n\\n# Out-of-Scope Use\\nThe model and its derivatives may not be used\\n\\n- In any way that violates any applicable national, federal, state, local or international law or regulation.\\n- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\\n- To generate or disseminate verifiably false information and/or content with the purpose of harming others.\\n- To generate or disseminate personal identifiable information that can be used to harm an individual.\\n- To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\\n- To create non-consensual nudity or illegal pornographic content.\\n- For fully automated decision making that adversely impacts an individual\\'s legal rights or otherwise creates or modifies a binding, enforceable obligation.\\n- Generating or facilitating large-scale disinformation campaigns.\\n\\n# License\\nThis model falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=black-forest-labs/FLUX.1-dev\n",
      "> RSP [{'title': 'Black Forest Labs - Frontier AI Lab', 'url': 'https://blackforestlabs.ai/', 'content': 'Black Forest Labs offers four variants of FLUX, a state-of-the-art image generation model. FLUX.1 [dev] is an open-weight model for non-commercial applications, available on HuggingFace and Replicate.', 'score': 0.89279664, 'raw_content': None}, {'title': 'FLUX 1.1 - BlackForestLabs', 'url': 'https://blackforestlabs.org/flux-1-1/', 'content': 'FLUX 1.1 - BlackForestLabs FLUX 1.1FLUX 1.1 Model FLUX 1.1 FLUX 1.1 [dev] is a 12-billion-parameter, rectified flow transformer designed to generate images from text prompts. High-quality output, just behind the performance of our flagship FLUX.1 [pro] model. Generated images are permitted for personal, scientific, and commercial use, as outlined in the FLUX.1 [dev] Non-Commercial License. FLUX 1.1 Model Usage FLUX.1 models are also accessible via API through: FLUX.1 [dev] is integrated with ComfyUI, offering a node-based workflow for local inference. To use FLUX.1 [dev] with the 🧨 diffusers library in Python, begin by installing or updating diffusers: pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16) image.save(\"flux-dev.png\") This model is covered by the FLUX.1 [dev] Non-Commercial License. FLUX 1.1FLUX 1.1 Model', 'score': 0.85509074, 'raw_content': None}, {'title': 'black-forest-labs/FLUX.1-dev - Hugging Face', 'url': 'https://huggingface.co/black-forest-labs/FLUX.1-dev', 'content': 'FLUX.1-dev is a 12 billion parameter rectified flow transformer that generates images from text descriptions. It is available via API, diffusers, and Comfy UI, and has a non-commercial license agreement.', 'score': 0.84858394, 'raw_content': None}, {'title': 'FLUX.1 Dev | FLUX Dev AI Image Generator by Black Forest Labs', 'url': 'https://flux1ai.com/dev', 'content': 'FLUX.1 Dev is a guidance-distilled text-to-image model that can create high-quality images from text prompts. It is free for non-commercial use and has an open-weight architecture for research and development.', 'score': 0.82425016, 'raw_content': None}, {'title': 'README.md · black-forest-labs/FLUX.1-dev at main - Hugging Face', 'url': 'https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/README.md', 'content': 'FLUX.1 [dev] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. For more information, please read our blog post.. Key Features Cutting-edge output quality, second only to our state-of-the-art model FLUX.1 [pro].; Competitive prompt following, matching the performance of closed source alternatives .', 'score': 0.81489426, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=black-forest-labs/FLUX.1-dev\n",
      "> RSP [{'title': 'I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow ...', 'url': 'https://arxiv.org/html/2410.07536v1', 'content': 'In this paper, we use a self-trained Lumina-Next-2K model and the open-source Flux.1-dev\\xa0Black Forest Labs (2024) model as representative rectified flow transformers (RFTs) to validate the general effectiveness of I-Max for RFTs. Lumina-Next-2K is a 2K generative model derived from the open-source Lumina-Next model\\xa0Zhuo et\\xa0al. In this section, using GPT-4o preference assessments, we compare the model’s generation results at its native resolution with high-resolution images generated using I-Max. As shown in Fig. 7, we can observe that for a certain range of scaling factors, the extrapolated results even achieve over ', 'score': 0.56517935, 'raw_content': None}, {'title': 'Abstract - arXiv.org', 'url': 'https://arxiv.org/pdf/2410.22775', 'content': 'FLUX: FLUX family models [21] are newly introduced T2I models developed by Black Forest Lab [20]. To the best of our knowledge, no formal technical report is available about this model. ... FLUX-Dev Aug 2024 - 512×512 - T5 & CLIP-based FLUX-Schnell Aug 2024 - 1024×1024 - T5 & CLIP-based', 'score': 0.511644, 'raw_content': None}, {'title': 'SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion ...', 'url': 'https://arxiv.org/html/2411.05007v2', 'content': 'To our knowledge, we are the first to successfully apply 4-bit post-training quantization to both the weights and activations of diffusion models, and achieve measured speedup on NVIDIA GPUs. On the latest 12B FLUX.1, we largely preserve the image quality and reduce the memory footprint of the original BF16 model by 3.5× and deliver a 3.0× speedup over the NF4 weight-only quantized baseline, measured on a 16GB laptop-level RTX4090 GPU. Among these works, only MixDQ\\xa0(Zhao et\\xa0al., 2024c) and ViDiT-Q\\xa0(Zhao et\\xa0al., 2024b) implement low-bit inference engines and report measured 8-bit speedup on GPUs. In this work, we push the boundary further by quantizing diffusion models to 4 bits, supporting both the integer or floating-point data types, compatible with the UNet backbone\\xa0(Ho et\\xa0al., 2020) and recent DiT architecture\\xa0(Peebles & Xie, 2023).', 'score': 0.36711195, 'raw_content': None}, {'title': 'SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion ...', 'url': 'https://arxiv.org/html/2411.05007v1', 'content': 'To our knowledge, we are the first to successfully apply 4-bit post-training quantization to both the weights and activations of diffusion models, and achieve measured speedup on NVIDIA GPUs. On the latest 12B FLUX.1, we largely preserve the image quality and reduce the memory footprint of the original BF16 model by 3.5× and deliver a 3.0× speedup over the NF4 weight-only quantized baseline, measured on a 16GB laptop-level RTX4090 GPU. Among these works, only MixDQ\\xa0(Zhao et\\xa0al., 2024c) and ViDiT-Q\\xa0(Zhao et\\xa0al., 2024b) implement low-bit inference engines and report measured 8-bit speedup on GPUs. In this work, we push the boundary further by quantizing diffusion models to 4 bits, supporting both the integer or floating-point data types, compatible with the UNet backbone\\xa0(Ho et\\xa0al., 2020) and recent DiT architecture\\xa0(Peebles & Xie, 2023).', 'score': 0.36711195, 'raw_content': None}, {'title': 'Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study', 'url': 'https://arxiv.org/html/2411.13588v1', 'content': 'While prior research has highlighted the presence of high similarity in activation values between adjacent diffusion steps (referred to as redundancy) and proposed various caching mechanisms to mitigate computational overhead, the exploration of redundancy in existing literature remains limited, with findings often not generalizable across different DiT models. Our experimental analysis reveals substantial variations in the distribution of redundancy across diffusion steps among different DiT models. We systematically explore redundancy across diffusion steps in a diverse range of prominent DiT models, including FLUX.1-dev, Pixart-Alpha, Stable-Diffusion-3, CogVideoX-5B, Open-Sora, Latte-1, and Mochi-1-preview. By contrast, in the domain of DiT models, L2C\\xa0[8], Tgate\\xa0[9], and PAB\\xa0[10] explores the redundancy distribution across diffusion steps and propose caching mechanism to accelerate the diffusion process. Consequently, we can infer that the number of diffusion steps does not significantly alter the redundancy distribution in DiT models.', 'score': 0.33541664, 'raw_content': None}, {'title': '1.58-bit FLUX - arXiv.org', 'url': 'https://arxiv.org/html/2412.18653v1', 'content': 'In this work, we focus on post-training 1.58-bit quantization of FLUX, a state-of-the-art open-source text-to-image (T2I) model. [2023a]   Yefei He, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang.Efficientdm: Efficient quantization-aware fine-tuning of low-bit diffusion models.arXiv preprint arXiv:2310.03270, 2023a. [2024a] Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et\\xa0al.Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation.arXiv preprint arXiv:2406.02540, 2024a. [2024b] Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, and Yu Wang.Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization.arXiv preprint arXiv:2405.17873, 2024b.', 'score': 0.33105537, 'raw_content': None}, {'title': 'Training-free Regional Prompting for Diffusion Transformers - arXiv.org', 'url': 'https://arxiv.org/html/2411.02395v1', 'content': 'In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. In addition to improving the base model, some recent studies\\xa0[16, 8, 17, 18, 19] have proposed to handle compositional control by providing spatial conditions (layout/box) and training a control module as a plugin on top of the base model, or to manipulate the attention score map using region masks in a training-free manner. RPG\\xa0[8] employs the Multi-modal Large Language Model (MLLM)\\xa0[20] as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions, and proposes complementary regional diffusion to enable region-wise compositional generation. We propose a training-free regional prompting method for FLUX.1, enabling fine-grained compositional generation for transformer-based models with swift and responsive image generation.', 'score': 0.2515378, 'raw_content': None}, {'title': 'Diffusion Beats Autoregressive: An Evaluation of Compositional ...', 'url': 'https://arxiv.org/html/2410.22775v1', 'content': 'Recently, a new open-source diffusion-based T2I model, FLUX, has been introduced, demonstrating strong performance in high-quality image generation. Recent advancements in computational resources and data scaling have led to the development of substantial text-to-image (T2I) models, from diffusion-based [16] models such as Stable Diffusion [37, 31] and DALL-E [34, 33, 4] to autoregressive-based ones such as LlamaGen [41], which are capable of producing high-quality and realistic images from textual prompts. While DALL-E1 [34] utilizes a discrete variational auto-encoder [43] model to generate image tokens from textual tokens, DALL-E2 [33] first uses a pre-trained CLIP-based model to prepare the text embeddings from the input prompt, which is then fed to a diffusion or autoregressive model to produce an image embedding.', 'score': 0.13990116, 'raw_content': None}, {'title': 'Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject ...', 'url': 'https://arxiv.org/html/2411.15466v1', 'content': 'In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Figure 1:Given a single reference image, our Diptych Prompting performs zero-shot subject-driven text-to-image generation through diptych inpainting. In our approach, we reinterpret the task as a diptych inpainting process: the left panel contains a reference image of the subject as a visual cue, and the right panel is generated through inpainting based on a text prompt describing the diptych with the desired context. In this paper, we proposed Diptych Prompting, an inpainting-based approach for zero-shot subject-driven text-to-image generation.', 'score': 0.02876438, 'raw_content': None}, {'title': 'OminiControl: Minimal and Universal Control for Diffusion Transformer', 'url': 'https://arxiv.org/html/2411.15098v1', 'content': 'We present a parameter-efficient method for enabling image-conditioned control in Diffusion Transformer (DiT) models, achieving both spatially aligned and non-spatially aligned control within a unified framework. We evaluate our method on two categories of conditional generation tasks: spatially aligned tasks (including Canny-to-image, depth-to-image, masked-based inpainting, and colorization) and subject-driven generation. effectively controls the generation process for both spatially-aligned tasks like canny-to-image generation and non-spatially-aligned tasks like subject-driven generation, enabling flexible control over the condition’s influence. As shown in Table\\xa03, our experiments show that increasing the LoRA rank generally improves model performance, with rank 16 achieving the best results across multiple aspects: image quality (measured by FID and SSIM), condition control capability (measured by F1 Score), while maintaining competitive text-image consistency (measured by CLIP-Score).', 'score': 0.027608924, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.05007\n",
      "> RSP [SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models](http://arxiv.org/abs/2411.05007v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.13588\n",
      "> RSP [Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study](http://arxiv.org/abs/2411.13588v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.15098\n",
      "> RSP [OminiControl: Minimal and Universal Control for Diffusion Transformer](http://arxiv.org/abs/2411.15098v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.15466\n",
      "> RSP [Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator](http://arxiv.org/abs/2411.15466v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2410.07536\n",
      "> RSP [I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow](http://arxiv.org/abs/2410.07536v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.02395\n",
      "> RSP [Training-free Regional Prompting for Diffusion Transformers](http://arxiv.org/abs/2411.02395v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2410.22775\n",
      "> RSP [Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models](http://arxiv.org/abs/2410.22775v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.18653\n",
      "> RSP [1.58-bit FLUX](http://arxiv.org/abs/2412.18653v1)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'Black Forest Labs - Frontier AI Lab', 'url': 'https://blackforestlabs.ai/', 'content': 'Black Forest Labs offers four variants of FLUX, a state-of-the-art image generation model. FLUX.1 [dev] is an open-weight model for non-commercial applications, available on HuggingFace and Replicate.', 'score': 0.89279664, 'raw_content': None}, {'title': 'FLUX 1.1 - BlackForestLabs', 'url': 'https://blackforestlabs.org/flux-1-1/', 'content': 'FLUX 1.1 - BlackForestLabs FLUX 1.1FLUX 1.1 Model FLUX 1.1 FLUX 1.1 [dev] is a 12-billion-parameter, rectified flow transformer designed to generate images from text prompts. High-quality output, just behind the performance of our flagship FLUX.1 [pro] model. Generated images are permitted for personal, scientific, and commercial use, as outlined in the FLUX.1 [dev] Non-Commercial License. FLUX 1.1 Model Usage FLUX.1 models are also accessible via API through: FLUX.1 [dev] is integrated with ComfyUI, offering a node-based workflow for local inference. To use FLUX.1 [dev] with the 🧨 diffusers library in Python, begin by installing or updating diffusers: pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16) image.save(\"flux-dev.png\") This model is covered by the FLUX.1 [dev] Non-Commercial License. FLUX 1.1FLUX 1.1 Model', 'score': 0.85509074, 'raw_content': None}, {'title': 'black-forest-labs/FLUX.1-dev - Hugging Face', 'url': 'https://huggingface.co/black-forest-labs/FLUX.1-dev', 'content': 'FLUX.1-dev is a 12 billion parameter rectified flow transformer that generates images from text descriptions. It is available via API, diffusers, and Comfy UI, and has a non-commercial license agreement.', 'score': 0.84858394, 'raw_content': None}, {'title': 'FLUX.1 Dev | FLUX Dev AI Image Generator by Black Forest Labs', 'url': 'https://flux1ai.com/dev', 'content': 'FLUX.1 Dev is a guidance-distilled text-to-image model that can create high-quality images from text prompts. It is free for non-commercial use and has an open-weight architecture for research and development.', 'score': 0.82425016, 'raw_content': None}, {'title': 'README.md · black-forest-labs/FLUX.1-dev at main - Hugging Face', 'url': 'https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/README.md', 'content': 'FLUX.1 [dev] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. For more information, please read our blog post.. Key Features Cutting-edge output quality, second only to our state-of-the-art model FLUX.1 [pro].; Competitive prompt following, matching the performance of closed source alternatives .', 'score': 0.81489426, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "language:\n",
      "- en\n",
      "license: other\n",
      "license_name: flux-1-dev-non-commercial-license\n",
      "license_link: LICENSE.md\n",
      "extra_gated_prompt: By clicking \"Agree\", you agree to the\n",
      "FluxDev Non-Commercial License Agreement\n",
      "and acknowledge the\n",
      "Acceptable Use Policy\n",
      ".\n",
      "tags:\n",
      "- text-to-image\n",
      "- image-generation\n",
      "- flux\n",
      "FLUX.1 [dev]\n",
      "is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.\n",
      "For more information, please read our\n",
      "blog post\n",
      ".\n",
      "Key Features\n",
      "Cutting-edge output quality, second only to our state-of-the-art model\n",
      "FLUX.1 [pro]\n",
      ".\n",
      "Competitive prompt following, matching the performance of closed source alternatives .\n",
      "Trained using guidance distillation, making\n",
      "FLUX.1 [dev]\n",
      "more efficient.\n",
      "Open weights to drive new scientific research, and empower artists to develop innovative workflows.\n",
      "Generated outputs can be used for personal, scientific, and commercial purposes as described in the\n",
      "FLUX.1 [dev]\n",
      "Non-Commercial License\n",
      ".\n",
      "Usage\n",
      "We provide a reference implementation of\n",
      "FLUX.1 [dev]\n",
      ", as well as sampling code, in a dedicated\n",
      "github repository\n",
      ".\n",
      "Developers and creatives looking to build on top of\n",
      "FLUX.1 [dev]\n",
      "are encouraged to use this as a starting point.\n",
      "API Endpoints\n",
      "The FLUX.1 models are also available via API from the following sources\n",
      "-\n",
      "bfl.ml\n",
      "(currently\n",
      "FLUX.1 [pro]\n",
      ")\n",
      "-\n",
      "replicate.com\n",
      "-\n",
      "fal.ai\n",
      "-\n",
      "mystic.ai\n",
      "ComfyUI\n",
      "FLUX.1 [dev]\n",
      "is also available in\n",
      "Comfy UI\n",
      "for local inference with a node-based workflow.\n",
      "Diffusers\n",
      "To use\n",
      "FLUX.1 [dev]\n",
      "with the 🧨 diffusers python library, first install or upgrade diffusers\n",
      "shell\n",
      "pip install -U diffusers\n",
      "Then you can use\n",
      "FluxPipeline\n",
      "to run the model\n",
      "```python\n",
      "import torch\n",
      "from diffusers import FluxPipeline\n",
      "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
      "pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\n",
      "prompt = \"A cat holding a sign that says hello world\"\n",
      "image = pipe(\n",
      "    prompt,\n",
      "    height=1024,\n",
      "    width=1024,\n",
      "    guidance_scale=3.5,\n",
      "    num_inference_steps=50,\n",
      "    max_sequence_length=512,\n",
      "    generator=torch.Generator(\"cpu\").manual_seed(0)\n",
      ").images[0]\n",
      "image.save(\"flux-dev.png\")\n",
      "```\n",
      "To learn more check out the\n",
      "diffusers\n",
      "documentation\n",
      "Limitations\n",
      "This model is not intended or able to provide factual information.\n",
      "As a statistical model this checkpoint might amplify existing societal biases.\n",
      "The model may fail to generate output that matches the prompts.\n",
      "Prompt following is heavily influenced by the prompting-style.\n",
      "Out-of-Scope Use\n",
      "The model and its derivatives may not be used\n",
      "In any way that violates any applicable national, federal, state, local or international law or regulation.\n",
      "For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\n",
      "To generate or disseminate verifiably false information and/or content with the purpose of harming others.\n",
      "To generate or disseminate personal identifiable information that can be used to harm an individual.\n",
      "To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\n",
      "To create non-consensual nudity or illegal pornographic content.\n",
      "For fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.\n",
      "Generating or facilitating large-scale disinformation campaigns.\n",
      "License\n",
      "This model falls under the\n",
      "FLUX.1 [dev]\n",
      "Non-Commercial License\n",
      ".\n",
      "> RSP * 12 billion parameters for advanced text-to-image generation.\n",
      "* Cutting-edge output quality, second only to a flagship model.\n",
      "* Competitive prompt following capabilities that rival closed-source alternatives.\n",
      "* Utilizes guidance distillation for improved efficiency.\n",
      "* Open-weight architecture promotes scientific research and creativity.\n",
      "* Licensed for personal, scientific, and commercial use under specific agreements.\n",
      "* Accessible via multiple platforms, including APIs and local inference tools.\n",
      "* Integrated with node-based workflows in ComfyUI.\n",
      "* Available with sample code for developers and creatives to enhance functionality.\n",
      "* Focus on ensuring responsible usage with strict usage policies.\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=hexgrad/Kokoro-82M\n",
      "> RSP {'model_id': 'hexgrad/Kokoro-82M', 'created_at': '26 December 2024 at 00:20:08 UTC', 'downloads': 1479, 'likes': 269, 'trending_score': 168, 'description': '---\\nlicense: apache-2.0\\nlanguage:\\n- en\\nbase_model:\\n- yl4579/StyleTTS2-LJSpeech\\npipeline_tag: text-to-speech\\n---\\n❤️ Kokoro Discord Server: https://discord.gg/QuGxSWBfQy\\n\\n📣 Got Synthetic Data? Want Trained Voicepacks? See https://hf.co/posts/hexgrad/418806998707773\\n\\n<audio controls><source src=\"https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/demo/HEARME.wav\" type=\"audio/wav\"></audio>\\n\\n**Kokoro** is a frontier TTS model for its size of **82 million parameters** (text in/audio out).\\n\\nOn 25 Dec 2024, Kokoro v0.19 weights were permissively released in full fp32 precision under an Apache 2.0 license. As of 2 Jan 2025, 10 unique Voicepacks have been released, and a `.onnx` version of v0.19 is available.\\n\\nIn the weeks leading up to its release, Kokoro v0.19 was the #1🥇 ranked model in [TTS Spaces Arena](https://huggingface.co/hexgrad/Kokoro-82M#evaluation). Kokoro had achieved higher Elo in this single-voice Arena setting over other models, using fewer parameters and less data:\\n1. **Kokoro v0.19: 82M params, Apache, trained on <100 hours of audio**\\n2. XTTS v2: 467M, CPML, >10k hours\\n3. Edge TTS: Microsoft, proprietary\\n4. MetaVoice: 1.2B, Apache, 100k hours\\n5. Parler Mini: 880M, Apache, 45k hours\\n6. Fish Speech: ~500M, CC-BY-NC-SA, 1M hours\\n\\nKokoro\\'s ability to top this Elo ladder suggests that the scaling law (Elo vs compute/data/params) for traditional TTS models might have a steeper slope than previously expected.\\n\\nYou can find a hosted demo at [hf.co/spaces/hexgrad/Kokoro-TTS](https://huggingface.co/spaces/hexgrad/Kokoro-TTS).\\n\\n### Usage\\n\\nThe following can be run in a single cell on [Google Colab](https://colab.research.google.com/).\\n```py\\n# 1️⃣ Install dependencies silently\\n!git lfs install\\n!git clone https://huggingface.co/hexgrad/Kokoro-82M\\n%cd Kokoro-82M\\n!apt-get -qq -y install espeak-ng > /dev/null 2>&1\\n!pip install -q phonemizer torch transformers scipy munch\\n\\n# 2️⃣ Build the model and load the default voicepack\\nfrom models import build_model\\nimport torch\\ndevice = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\nMODEL = build_model(\\'kokoro-v0_19.pth\\', device)\\nVOICE_NAME = [\\n    \\'af\\', # Default voice is a 50-50 mix of Bella & Sarah\\n    \\'af_bella\\', \\'af_sarah\\', \\'am_adam\\', \\'am_michael\\',\\n    \\'bf_emma\\', \\'bf_isabella\\', \\'bm_george\\', \\'bm_lewis\\',\\n    \\'af_nicole\\', \\'af_sky\\',\\n][0]\\nVOICEPACK = torch.load(f\\'voices/{VOICE_NAME}.pt\\', weights_only=True).to(device)\\nprint(f\\'Loaded voice: {VOICE_NAME}\\')\\n\\n# 3️⃣ Call generate, which returns 24khz audio and the phonemes used\\nfrom kokoro import generate\\ntext = \"How could I know? It\\'s an unanswerable question. Like asking an unborn child if they\\'ll lead a good life. They haven\\'t even been born.\"\\naudio, out_ps = generate(MODEL, text, VOICEPACK, lang=VOICE_NAME[0])\\n# Language is determined by the first letter of the VOICE_NAME:\\n# 🇺🇸 \\'a\\' => American English => en-us\\n# 🇬🇧 \\'b\\' => British English => en-gb\\n\\n# 4️⃣ Display the 24khz audio and print the output phonemes\\nfrom IPython.display import display, Audio\\ndisplay(Audio(data=audio, rate=24000, autoplay=True))\\nprint(out_ps)\\n```\\nIf you have trouble with `espeak-ng`, see this [github issue](https://github.com/bootphon/phonemizer/issues/44#issuecomment-1540885186). [Mac users also see this](https://huggingface.co/hexgrad/Kokoro-82M/discussions/12#677435d3d8ace1de46071489), and [Windows users see this](https://huggingface.co/hexgrad/Kokoro-82M/discussions/12#67742594fdeebf74f001ecfc).\\n\\nFor ONNX usage, see [#14](https://huggingface.co/hexgrad/Kokoro-82M/discussions/14).\\n\\n### Model Facts\\n\\nNo affiliation can be assumed between parties on different lines.\\n\\n**Architecture:**\\n- StyleTTS 2: https://arxiv.org/abs/2306.07691\\n- ISTFTNet: https://arxiv.org/abs/2203.02395\\n- Decoder only: no diffusion, no encoder release\\n\\n**Architected by:** Li et al @ https://github.com/yl4579/StyleTTS2\\n\\n**Trained by**: `@rzvzn` on Discord\\n\\n**Supported Languages:** American English, British English\\n\\n**Model SHA256 Hash:** `3b0c392f87508da38fad3a2f9d94c359f1b657ebd2ef79f9d56d69503e470b0a`\\n\\n### Releases\\n- 25 Dec 2024: Model v0.19, `af_bella`, `af_sarah`\\n- 26 Dec 2024: `am_adam`, `am_michael`\\n- 28 Dec 2024: `bf_emma`, `bf_isabella`, `bm_george`, `bm_lewis`\\n- 30 Dec 2024: `af_nicole`\\n- 31 Dec 2024: `af_sky`\\n- 2 Jan 2025: ONNX v0.19 `ebef4245`\\n\\n### Licenses\\n- Apache 2.0 weights in this repository\\n- MIT inference code in [spaces/hexgrad/Kokoro-TTS](https://huggingface.co/spaces/hexgrad/Kokoro-TTS) adapted from [yl4579/StyleTTS2](https://github.com/yl4579/StyleTTS2)\\n- GPLv3 dependency in [espeak-ng](https://github.com/espeak-ng/espeak-ng)\\n\\nThe inference code was originally MIT licensed by the paper author. Note that this card applies only to this model, Kokoro. Original models published by the paper author can be found at [hf.co/yl4579](https://huggingface.co/yl4579).\\n\\n### Evaluation\\n\\n**Metric:** Elo rating\\n\\n**Leaderboard:** [hf.co/spaces/Pendrokar/TTS-Spaces-Arena](https://huggingface.co/spaces/Pendrokar/TTS-Spaces-Arena)\\n\\n![TTS-Spaces-Arena-25-Dec-2024](demo/TTS-Spaces-Arena-25-Dec-2024.png)\\n\\nThe voice ranked in the Arena is a 50-50 mix of Bella and Sarah. For your convenience, this mix is included in this repository as `af.pt`, but you can trivially reproduce it like this:\\n\\n```py\\nimport torch\\nbella = torch.load(\\'voices/af_bella.pt\\', weights_only=True)\\nsarah = torch.load(\\'voices/af_sarah.pt\\', weights_only=True)\\naf = torch.mean(torch.stack([bella, sarah]), dim=0)\\nassert torch.equal(af, torch.load(\\'voices/af.pt\\', weights_only=True))\\n```\\n\\n### Training Details\\n\\n**Compute:** Kokoro was trained on A100 80GB vRAM instances rented from [Vast.ai](https://cloud.vast.ai/?ref_id=79907) (referral link). Vast was chosen over other compute providers due to its competitive on-demand hourly rates. The average hourly cost for the A100 80GB vRAM instances used for training was below $1/hr per GPU, which was around half the quoted rates from other providers at the time.\\n\\n**Data:** Kokoro was trained exclusively on **permissive/non-copyrighted audio data** and IPA phoneme labels. Examples of permissive/non-copyrighted audio include:\\n- Public domain audio\\n- Audio licensed under Apache, MIT, etc\\n- Synthetic audio<sup>[1]</sup> generated by closed<sup>[2]</sup> TTS models from large providers<br/>\\n[1] https://copyright.gov/ai/ai_policy_guidance.pdf<br/>\\n[2] No synthetic audio from open TTS models or \"custom voice clones\"\\n\\n**Epochs:** Less than **20 epochs**\\n\\n**Total Dataset Size:** Less than **100 hours** of audio\\n\\n### Limitations\\n\\nKokoro v0.19 is limited in some specific ways, due to its training set and/or architecture:\\n- [Data] Lacks voice cloning capability, likely due to small <100h training set\\n- [Arch] Relies on external g2p (espeak-ng), which introduces a class of g2p failure modes\\n- [Data] Training dataset is mostly long-form reading and narration, not conversation\\n- [Arch] At 82M params, Kokoro almost certainly falls to a well-trained 1B+ param diffusion transformer, or a many-billion-param MLLM like GPT-4o / Gemini 2.0 Flash\\n- [Data] Multilingual capability is architecturally feasible, but training data is mostly English\\n\\nRefer to the [Philosophy discussion](https://huggingface.co/hexgrad/Kokoro-82M/discussions/5) to better understand these limitations.\\n\\n**Will the other voicepacks be released?** There is currently no release date scheduled for the other voicepacks, but in the meantime you can try them in the hosted demo at [hf.co/spaces/hexgrad/Kokoro-TTS](https://huggingface.co/spaces/hexgrad/Kokoro-TTS).\\n\\n### Acknowledgements\\n- [@yl4579](https://huggingface.co/yl4579) for architecting StyleTTS 2\\n- [@Pendrokar](https://huggingface.co/Pendrokar) for adding Kokoro as a contender in the TTS Spaces Arena\\n\\n### Model Card Contact\\n\\n`@rzvzn` on Discord. Server invite: https://discord.gg/QuGxSWBfQy\\n\\n<img src=\"https://static0.gamerantimages.com/wordpress/wp-content/uploads/2024/08/terminator-zero-41-1.jpg\" width=\"400\" alt=\"kokoro\" />\\n\\nhttps://terminator.fandom.com/wiki/Kokoro'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=hexgrad/Kokoro-82M\n",
      "> RSP [{'title': 'hexgrad/Kokoro-82M · Philosophy - Hugging Face', 'url': 'https://huggingface.co/hexgrad/Kokoro-82M/discussions/5', 'content': \"hexgrad/Kokoro-82M · Philosophy Kokoro is a series of TTS models that seeks the efficient voice frontier, maximizing Elo rating for a given model size (parameter count). Kokoro's training mix heavily favors synthetic data, and all training data must be permissive/non-copyrighted (refer to the Data section of Training Details). Voicepacks are inputs to the model that allow you to generate speech in a particular voice. For one model, there are many Voicepacks. But if you stay entirely permissive/non-copyrighted by using large & permissive (but lower quality) datasets like Common Voice or LibriLight, you might drop your Elo rating since the model learns noise and suboptimal audio patterns. Models Datasets Spaces Pricing Docs\", 'score': 0.79254496, 'raw_content': None}, {'title': 'hexgrad/kokoro: https://hf.co/hexgrad/Kokoro-82M - GitHub', 'url': 'https://github.com/hexgrad/kokoro', 'content': 'GitHub - hexgrad/kokoro: https://hf.co/hexgrad/Kokoro-82M Sign in GitHub Copilot Write better code with AI Code Search Find more, search less View all industries View all solutions View all GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Enterprise platform AI-powered developer platform Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Search or jump to... Search code, repositories, users, issues, pull requests... Search Saved searches To see all available qualifiers, see our documentation. Cancel Create saved search Sign in You signed in with another tab or window. You signed out in another tab or window. 0 stars 0 forks Branches Tags Activity Last commit message 1 Commit View all files Repository files navigation © 2025 GitHub,\\xa0Inc. Footer navigation', 'score': 0.70799106, 'raw_content': None}, {'title': 'Models - Hugging Face', 'url': 'https://hf.wing.moe/models', 'content': 'Models - Hugging Face Models Edit Models filters Reinforcement Learning Models #### PowerInfer/SmallThinker-3B-Preview Text Generation • Updated 1 day ago • 7k • 288 #### hexgrad/Kokoro-82M Text-to-Speech • Updated 1 day ago • 1.48k • 269 #### meta-llama/Llama-3.3-70B-Instruct Text Generation • Updated 17 days ago • 417k • • 1.52k #### Qwen/QVQ-72B-Preview Image-Text-to-Text • Updated 14 days ago • 72.1k • 468 #### tencent/HunyuanVideo Text-to-Video • Updated 21 days ago • 9.54k • 1.38k #### declare-lab/TangoFlux Text-to-Audio • Updated 5 days ago • 974 • 44 #### Datou1111/shou_xin Text-to-Image • Updated 30 days ago • 50.2k • 834 #### xey/sldr_flux_nsfw_v2-studio Text-to-Image • Updated 3 days ago • 137k • • 114 Models Datasets Spaces Pricing Docs', 'score': 0.7036111, 'raw_content': None}, {'title': 'hexgrad/Kokoro-82M · [TODO] FP16 Inference - Hugging Face', 'url': 'https://huggingface.co/hexgrad/Kokoro-82M/discussions/4', 'content': \"hexgrad/Kokoro-82M · [TODO] FP16 Inference Very simple script halve.py cuts the model precision in half, from FP32 down to FP16. The new model is saved as kokoro-v0_19-half.pth and we know it was cut in half because the file size is halved from 320 MB to 160 MB. Run the below cell in Colab to ensure the halved model at fp16/kokoro-v0_19-half.pth still works: MODEL = build_model('fp16/kokoro-v0_19-half.pth', device) # Half precision model upcast to fp32 audio, out_ps = generate(MODEL, text, VOICEPACK) 4️⃣ Display the 24khz audio and print the output phonemes The current inference code implicitly upcasts the half precision model to FP32 before doing inference, so we're not actually gaining any inference speed (or memory footprint reduction, I think) using the FP16 precision model. Models Datasets Spaces Pricing Docs\", 'score': 0.54594, 'raw_content': None}, {'title': 'Models - Hugging Face', 'url': 'https://huggingface.proxy.nlp.skieer.com/models', 'content': 'hexgrad/Kokoro-82M. Text-to-Speech • Updated about 7 hours ago • 164 • 123 meta-llama/Llama-3.3-70B-Instruct. Text Generation • Updated 11 days ago • 373k • • 1.4k PowerInfer/SmallThinker-3B-Preview. Updated 2 days ago • 325', 'score': 0.5271467, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=hexgrad/Kokoro-82M\n",
      "> RSP [{'title': 'Abstract - arXiv.org', 'url': 'https://arxiv.org/pdf/2407.03418', 'content': 'EMU [95] 14.0B 82M Yes End-to-end No interleaved GEMINI - - Yes - Yes interleaved GPT-4V - - Yes - Yes - 2.3 Real-world Use Cases Each use case is drawn from a real-world application with their own specific challenges. Multimedia includes efficient search, retrieval, indexing, and generation of digital content. Multime-).', 'score': 0.19641005, 'raw_content': None}, {'title': '[2306.07691] StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models', 'url': 'https://arxiv.org/abs/2306.07691', 'content': 'arXiv:2306.07691 StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at this https URL. Subjects:   Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD) (or arXiv:2306.07691v2 [eess.AS] for this version) cs', 'score': 0.032519072, 'raw_content': None}, {'title': 'Abstract - arXiv.org', 'url': 'https://arxiv.org/pdf/2304.06634', 'content': 'the goal is to generate profile sentences given speaker utterances. For this, we create a new dataset, the Profile Generation Dataset (PGDataset),', 'score': 0.01102204, 'raw_content': None}, {'title': 'Extending Whisper with Prompt Tuning to Target-Speaker ASR - arXiv.org', 'url': 'https://arxiv.org/html/2312.08079', 'content': 'An overview of the proposed framework is shown in Fig.1, which is composed of four components: 1) a pre-trained foundation model—Whisper, to perform speech recognition, 2) encoder and decoder prompting, which precedes a target speaker embedding and trainable soft prompts to the input features (or text tokens) for target-speaker ASR, 3) intermediate prompting, which replaces the preceding output vectors from intermediate blocks with soft prompts for performance improvement, 4) reparameterization, which reparameterizes all the preceding soft prompts with residual neural networks for training stability. We evaluate the performance of our prompting scheme on three models: Whisper-large v2, Whisper-medium, and Whisper-small.In the training phase, we fix all the parameters of Whisper and optimize the soft prompts, the speaker projection layer, and the reparameterization MLP (if it exists) jointly using the AdamW optimizer [28] with an initial learning rate of 1e-4.', 'score': 0.010571531, 'raw_content': None}, {'title': 'Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech - arXiv.org', 'url': 'https://arxiv.org/pdf/2105.06337', 'content': 'Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech Vadim Popov* 1 Ivan Vovk* 1 2 Vladimir Gogoryan1 2 Tasnima Sadekova 1Mikhail Kudinov Abstract Recently, denoising diffusion probabilistic mod-els and generative score matching have shown', 'score': 0.010257544, 'raw_content': None}, {'title': 'StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion ...', 'url': 'https://arxiv.org/pdf/2306.07691', 'content': 'StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models Yinghao Aaron Li Cong Han Vinay S. The model comprises eight modules, organized into three categories: (1) a speech generation system (acoustic modules) with a text encoder, style encoder, and speech decoder; (2) a TTS prediction system with duration and prosody predictors; and (3) a utility system for training, including a discriminator, text aligner, and pitch extractor. Most notably, using randomly encoded style vectors instead of those sampled through style diffusion considerably affects all metrics, including the subjective CMOS score, thereby underscoring its 3https://github.com/chenqi008/pymcd 4Available at https://zenodo.org/record/4541452 16 Table 6: Comparison of models in ablation study with mel-cepstral distortion (MCD), MCD weighted by Speech Length (MCD-SL), root mean square error of log F0 pitch (F0 RMSE), mean absolute deviation of phoneme duration (DUR MAD), and word error rate (WER) on LJSpeech.', 'score': 0.009906703, 'raw_content': None}, {'title': 'StyleTTS 2: Towards Human-Level Text-to-Speech through Style ... - ar5iv', 'url': 'https://ar5iv.labs.arxiv.org/html/2306.07691', 'content': 'StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. The model comprises eight modules, organized into three categories: (1) a speech generation system (acoustic modules) with a text encoder, style encoder, and speech decoder; (2) a TTS prediction system with duration and prosody predictors; and (3) a utility system for training, including a discriminator, text aligner, and pitch extractor.', 'score': 0.008903209, 'raw_content': None}, {'title': 'StyleTTS: A Style-Based Generative Model for Natural and Diverse Text ...', 'url': 'https://arxiv.org/pdf/2205.15439', 'content': '1 StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis Yinghao Aaron Li, Cong Han, and Nima Mesgarani Abstract—Text-to-Speech (TTS) has recently seen great progress', 'score': 0.0072390935, 'raw_content': None}, {'title': 'Enhancing Inflation Nowcasting with LLM: - arXiv.org', 'url': 'https://arxiv.org/html/2410.20198', 'content': 'We use this model to produce NEWS, an index capturing the monthly sentiment of the news regarding inflation. We model news inflation sentiment by introducing InflaBERT, a BERT-based\\xa0Devlin et\\xa0al. Inaccurate labels or labels that misrepresent the true sentiment of the news articles in our training set will significantly hinder our ability to develop a high-performing model with strong predictive power. This work, through the development of InflaBERT—a BERT-based model fine-tuned for predicting inflation-related news sentiment—has shown that leveraging advanced machine learning techniques can enhance traditional models like the Cleveland Fed’s nowcasting model. In this section, we discuss the cross-validation results employed to ascertain the most effective model for analyzing sentiment concerning inflation in news articles.', 'score': 0.0057889754, 'raw_content': None}, {'title': 'A Survey of Resource-efficient LLM and Multimodal Foundation Models', 'url': 'https://arxiv.org/html/2401.08092v2', 'content': 'The huge resource footprint of large foundation model also hinders its democratization. Till the end of 2023, there are only a few major players capable of training and deploying the state-of-the-art foundation models, who thereby have powerful control over the public and can potentially manipulate them in a way they prefer.', 'score': 0.005176203, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2401.08092\n",
      "> RSP [A Survey of Resource-efficient LLM and Multimodal Foundation Models](http://arxiv.org/abs/2401.08092v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2407.03418\n",
      "> RSP [HEMM: Holistic Evaluation of Multimodal Foundation Models](http://arxiv.org/abs/2407.03418v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2410.20198\n",
      "> RSP [Enhancing Inflation Nowcasting with LLM: Sentiment Analysis on News](http://arxiv.org/abs/2410.20198v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2105.06337\n",
      "> RSP [Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech](http://arxiv.org/abs/2105.06337v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2304.06634\n",
      "> RSP [PGTask: Introducing the Task of Profile Generation from Dialogues](http://arxiv.org/abs/2304.06634v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2205.15439\n",
      "> RSP [StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis](http://arxiv.org/abs/2205.15439v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2306.07691\n",
      "> RSP [StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models](http://arxiv.org/abs/2306.07691v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2312.08079\n",
      "> RSP [Extending Whisper with prompt tuning to target-speaker ASR](http://arxiv.org/abs/2312.08079v2)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'hexgrad/Kokoro-82M · Philosophy - Hugging Face', 'url': 'https://huggingface.co/hexgrad/Kokoro-82M/discussions/5', 'content': \"hexgrad/Kokoro-82M · Philosophy Kokoro is a series of TTS models that seeks the efficient voice frontier, maximizing Elo rating for a given model size (parameter count). Kokoro's training mix heavily favors synthetic data, and all training data must be permissive/non-copyrighted (refer to the Data section of Training Details). Voicepacks are inputs to the model that allow you to generate speech in a particular voice. For one model, there are many Voicepacks. But if you stay entirely permissive/non-copyrighted by using large & permissive (but lower quality) datasets like Common Voice or LibriLight, you might drop your Elo rating since the model learns noise and suboptimal audio patterns. Models Datasets Spaces Pricing Docs\", 'score': 0.79254496, 'raw_content': None}, {'title': 'hexgrad/kokoro: https://hf.co/hexgrad/Kokoro-82M - GitHub', 'url': 'https://github.com/hexgrad/kokoro', 'content': 'GitHub - hexgrad/kokoro: https://hf.co/hexgrad/Kokoro-82M Sign in GitHub Copilot Write better code with AI Code Search Find more, search less View all industries View all solutions View all GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Enterprise platform AI-powered developer platform Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Search or jump to... Search code, repositories, users, issues, pull requests... Search Saved searches To see all available qualifiers, see our documentation. Cancel Create saved search Sign in You signed in with another tab or window. You signed out in another tab or window. 0 stars 0 forks Branches Tags Activity Last commit message 1 Commit View all files Repository files navigation © 2025 GitHub,\\xa0Inc. Footer navigation', 'score': 0.70799106, 'raw_content': None}, {'title': 'Models - Hugging Face', 'url': 'https://hf.wing.moe/models', 'content': 'Models - Hugging Face Models Edit Models filters Reinforcement Learning Models #### PowerInfer/SmallThinker-3B-Preview Text Generation • Updated 1 day ago • 7k • 288 #### hexgrad/Kokoro-82M Text-to-Speech • Updated 1 day ago • 1.48k • 269 #### meta-llama/Llama-3.3-70B-Instruct Text Generation • Updated 17 days ago • 417k • • 1.52k #### Qwen/QVQ-72B-Preview Image-Text-to-Text • Updated 14 days ago • 72.1k • 468 #### tencent/HunyuanVideo Text-to-Video • Updated 21 days ago • 9.54k • 1.38k #### declare-lab/TangoFlux Text-to-Audio • Updated 5 days ago • 974 • 44 #### Datou1111/shou_xin Text-to-Image • Updated 30 days ago • 50.2k • 834 #### xey/sldr_flux_nsfw_v2-studio Text-to-Image • Updated 3 days ago • 137k • • 114 Models Datasets Spaces Pricing Docs', 'score': 0.7036111, 'raw_content': None}, {'title': 'hexgrad/Kokoro-82M · [TODO] FP16 Inference - Hugging Face', 'url': 'https://huggingface.co/hexgrad/Kokoro-82M/discussions/4', 'content': \"hexgrad/Kokoro-82M · [TODO] FP16 Inference Very simple script halve.py cuts the model precision in half, from FP32 down to FP16. The new model is saved as kokoro-v0_19-half.pth and we know it was cut in half because the file size is halved from 320 MB to 160 MB. Run the below cell in Colab to ensure the halved model at fp16/kokoro-v0_19-half.pth still works: MODEL = build_model('fp16/kokoro-v0_19-half.pth', device) # Half precision model upcast to fp32 audio, out_ps = generate(MODEL, text, VOICEPACK) 4️⃣ Display the 24khz audio and print the output phonemes The current inference code implicitly upcasts the half precision model to FP32 before doing inference, so we're not actually gaining any inference speed (or memory footprint reduction, I think) using the FP16 precision model. Models Datasets Spaces Pricing Docs\", 'score': 0.54594, 'raw_content': None}, {'title': 'Models - Hugging Face', 'url': 'https://huggingface.proxy.nlp.skieer.com/models', 'content': 'hexgrad/Kokoro-82M. Text-to-Speech • Updated about 7 hours ago • 164 • 123 meta-llama/Llama-3.3-70B-Instruct. Text Generation • Updated 11 days ago • 373k • • 1.4k PowerInfer/SmallThinker-3B-Preview. Updated 2 days ago • 325', 'score': 0.5271467, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "license: apache-2.0\n",
      "language:\n",
      "- en\n",
      "base_model:\n",
      "- yl4579/StyleTTS2-LJSpeech\n",
      "pipeline_tag: text-to-speech\n",
      "❤️ Kokoro Discord Server: https://discord.gg/QuGxSWBfQy\n",
      "📣 Got Synthetic Data? Want Trained Voicepacks? See https://hf.co/posts/hexgrad/418806998707773\n",
      "Kokoro\n",
      "is a frontier TTS model for its size of\n",
      "82 million parameters\n",
      "(text in/audio out).\n",
      "On 25 Dec 2024, Kokoro v0.19 weights were permissively released in full fp32 precision under an Apache 2.0 license. As of 2 Jan 2025, 10 unique Voicepacks have been released, and a\n",
      ".onnx\n",
      "version of v0.19 is available.\n",
      "In the weeks leading up to its release, Kokoro v0.19 was the #1🥇 ranked model in\n",
      "TTS Spaces Arena\n",
      ". Kokoro had achieved higher Elo in this single-voice Arena setting over other models, using fewer parameters and less data:\n",
      "1.\n",
      "Kokoro v0.19: 82M params, Apache, trained on <100 hours of audio\n",
      "2. XTTS v2: 467M, CPML, >10k hours\n",
      "3. Edge TTS: Microsoft, proprietary\n",
      "4. MetaVoice: 1.2B, Apache, 100k hours\n",
      "5. Parler Mini: 880M, Apache, 45k hours\n",
      "6. Fish Speech: ~500M, CC-BY-NC-SA, 1M hours\n",
      "Kokoro's ability to top this Elo ladder suggests that the scaling law (Elo vs compute/data/params) for traditional TTS models might have a steeper slope than previously expected.\n",
      "You can find a hosted demo at\n",
      "hf.co/spaces/hexgrad/Kokoro-TTS\n",
      ".\n",
      "Usage\n",
      "The following can be run in a single cell on\n",
      "Google Colab\n",
      ".\n",
      "```py\n",
      "1️⃣ Install dependencies silently\n",
      "!git lfs install\n",
      "!git clone https://huggingface.co/hexgrad/Kokoro-82M\n",
      "%cd Kokoro-82M\n",
      "!apt-get -qq -y install espeak-ng > /dev/null 2>&1\n",
      "!pip install -q phonemizer torch transformers scipy munch\n",
      "2️⃣ Build the model and load the default voicepack\n",
      "from models import build_model\n",
      "import torch\n",
      "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
      "MODEL = build_model('kokoro-v0_19.pth', device)\n",
      "VOICE_NAME = [\n",
      "    'af', # Default voice is a 50-50 mix of Bella & Sarah\n",
      "    'af_bella', 'af_sarah', 'am_adam', 'am_michael',\n",
      "    'bf_emma', 'bf_isabella', 'bm_george', 'bm_lewis',\n",
      "    'af_nicole', 'af_sky',\n",
      "][0]\n",
      "VOICEPACK = torch.load(f'voices/{VOICE_NAME}.pt', weights_only=True).to(device)\n",
      "print(f'Loaded voice: {VOICE_NAME}')\n",
      "3️⃣ Call generate, which returns 24khz audio and the phonemes used\n",
      "from kokoro import generate\n",
      "text = \"How could I know? It's an unanswerable question. Like asking an unborn child if they'll lead a good life. They haven't even been born.\"\n",
      "audio, out_ps = generate(MODEL, text, VOICEPACK, lang=VOICE_NAME[0])\n",
      "Language is determined by the first letter of the VOICE_NAME:\n",
      "🇺🇸 'a' => American English => en-us\n",
      "🇬🇧 'b' => British English => en-gb\n",
      "4️⃣ Display the 24khz audio and print the output phonemes\n",
      "from IPython.display import display, Audio\n",
      "display(Audio(data=audio, rate=24000, autoplay=True))\n",
      "print(out_ps)\n",
      "``\n",
      "If you have trouble with\n",
      "espeak-ng`, see this\n",
      "github issue\n",
      ".\n",
      "Mac users also see this\n",
      ", and\n",
      "Windows users see this\n",
      ".\n",
      "For ONNX usage, see\n",
      "#14\n",
      ".\n",
      "Model Facts\n",
      "No affiliation can be assumed between parties on different lines.\n",
      "Architecture:\n",
      "- StyleTTS 2: https://arxiv.org/abs/2306.07691\n",
      "- ISTFTNet: https://arxiv.org/abs/2203.02395\n",
      "- Decoder only: no diffusion, no encoder release\n",
      "Architected by:\n",
      "Li et al @ https://github.com/yl4579/StyleTTS2\n",
      "Trained by\n",
      ":\n",
      "@rzvzn\n",
      "on Discord\n",
      "Supported Languages:\n",
      "American English, British English\n",
      "Model SHA256 Hash:\n",
      "3b0c392f87508da38fad3a2f9d94c359f1b657ebd2ef79f9d56d69503e470b0a\n",
      "Releases\n",
      "25 Dec 2024: Model v0.19,\n",
      "af_bella\n",
      ",\n",
      "af_sarah\n",
      "26 Dec 2024:\n",
      "am_adam\n",
      ",\n",
      "am_michael\n",
      "28 Dec 2024:\n",
      "bf_emma\n",
      ",\n",
      "bf_isabella\n",
      ",\n",
      "bm_george\n",
      ",\n",
      "bm_lewis\n",
      "30 Dec 2024:\n",
      "af_nicole\n",
      "31 Dec 2024:\n",
      "af_sky\n",
      "2 Jan 2025: ONNX v0.19\n",
      "ebef4245\n",
      "Licenses\n",
      "Apache 2.0 weights in this repository\n",
      "MIT inference code in\n",
      "spaces/hexgrad/Kokoro-TTS\n",
      "adapted from\n",
      "yl4579/StyleTTS2\n",
      "GPLv3 dependency in\n",
      "espeak-ng\n",
      "The inference code was originally MIT licensed by the paper author. Note that this card applies only to this model, Kokoro. Original models published by the paper author can be found at\n",
      "hf.co/yl4579\n",
      ".\n",
      "Evaluation\n",
      "Metric:\n",
      "Elo rating\n",
      "Leaderboard:\n",
      "hf.co/spaces/Pendrokar/TTS-Spaces-Arena\n",
      "The voice ranked in the Arena is a 50-50 mix of Bella and Sarah. For your convenience, this mix is included in this repository as\n",
      "af.pt\n",
      ", but you can trivially reproduce it like this:\n",
      "py\n",
      "import torch\n",
      "bella = torch.load('voices/af_bella.pt', weights_only=True)\n",
      "sarah = torch.load('voices/af_sarah.pt', weights_only=True)\n",
      "af = torch.mean(torch.stack([bella, sarah]), dim=0)\n",
      "assert torch.equal(af, torch.load('voices/af.pt', weights_only=True))\n",
      "Training Details\n",
      "Compute:\n",
      "Kokoro was trained on A100 80GB vRAM instances rented from\n",
      "Vast.ai\n",
      "(referral link). Vast was chosen over other compute providers due to its competitive on-demand hourly rates. The average hourly cost for the A100 80GB vRAM instances used for training was below $1/hr per GPU, which was around half the quoted rates from other providers at the time.\n",
      "Data:\n",
      "Kokoro was trained exclusively on\n",
      "permissive/non-copyrighted audio data\n",
      "and IPA phoneme labels. Examples of permissive/non-copyrighted audio include:\n",
      "- Public domain audio\n",
      "- Audio licensed under Apache, MIT, etc\n",
      "- Synthetic audio\n",
      "[1]\n",
      "generated by closed\n",
      "[2]\n",
      "TTS models from large providers\n",
      "[1] https://copyright.gov/ai/ai_policy_guidance.pdf\n",
      "[2] No synthetic audio from open TTS models or \"custom voice clones\"\n",
      "Epochs:\n",
      "Less than\n",
      "20 epochs\n",
      "Total Dataset Size:\n",
      "Less than\n",
      "100 hours\n",
      "of audio\n",
      "Limitations\n",
      "Kokoro v0.19 is limited in some specific ways, due to its training set and/or architecture:\n",
      "- [Data] Lacks voice cloning capability, likely due to small <100h training set\n",
      "- [Arch] Relies on external g2p (espeak-ng), which introduces a class of g2p failure modes\n",
      "- [Data] Training dataset is mostly long-form reading and narration, not conversation\n",
      "- [Arch] At 82M params, Kokoro almost certainly falls to a well-trained 1B+ param diffusion transformer, or a many-billion-param MLLM like GPT-4o / Gemini 2.0 Flash\n",
      "- [Data] Multilingual capability is architecturally feasible, but training data is mostly English\n",
      "Refer to the\n",
      "Philosophy discussion\n",
      "to better understand these limitations.\n",
      "Will the other voicepacks be released?\n",
      "There is currently no release date scheduled for the other voicepacks, but in the meantime you can try them in the hosted demo at\n",
      "hf.co/spaces/hexgrad/Kokoro-TTS\n",
      ".\n",
      "Acknowledgements\n",
      "@yl4579\n",
      "for architecting StyleTTS 2\n",
      "@Pendrokar\n",
      "for adding Kokoro as a contender in the TTS Spaces Arena\n",
      "Model Card Contact\n",
      "@rzvzn\n",
      "on Discord. Server invite: https://discord.gg/QuGxSWBfQy\n",
      "https://terminator.fandom.com/wiki/Kokoro\n",
      "> RSP * High Elo rating for a small parameter size (82 million)\n",
      "* Achieves competitive performance with less training data (<100 hours)\n",
      "* Supports multiple unique voicepacks for diverse applications\n",
      "* Exclusively trained on permissive/non-copyrighted audio data\n",
      "* No reliance on encoders, utilizing a decoder-only architecture\n",
      "* Built using efficient training methods on cost-effective compute resources\n",
      "* Capable of generating 24kHz audio quality\n",
      "* Offers both fp32 and fp16 model precision options\n",
      "* Designed with a focus on synthetic data usage for enhanced performance\n",
      "* Engaging community support through a Discord server for collaboration and feedback\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=meta-llama/Llama-3.3-70B-Instruct\n",
      "> RSP {'model_id': 'meta-llama/Llama-3.3-70B-Instruct', 'created_at': '26 November 2024 at 16:08:47 UTC', 'downloads': 416929, 'likes': 1517, 'trending_score': 128, 'description': '---\\nlibrary_name: transformers\\nlanguage:\\n- en\\n- fr\\n- it\\n- pt\\n- hi\\n- es\\n- th\\n- de\\nbase_model:\\n- meta-llama/Llama-3.1-70B\\ntags:\\n- facebook\\n- meta\\n- pytorch\\n- llama\\n- llama-3\\nextra_gated_prompt: \"### LLAMA 3.3 COMMUNITY LICENSE AGREEMENT\\\\nLlama 3.3 Version Release Date: December 6, 2024\\\\n\\\\\"Agreement\\\\\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\\\\n\\\\\"Documentation\\\\\" means the specifications, manuals and documentation accompanying Llama 3.3 distributed by Meta at [https://www.llama.com/docs/overview](https://llama.com/docs/overview).\\\\n\\\\\"Licensee\\\\\" or \\\\\"you\\\\\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\\\\n\\\\\"Llama 3.3\\\\\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at [https://www.llama.com/llama-downloads](https://www.llama.com/llama-downloads).\\\\n\\\\\"Llama Materials\\\\\" means, collectively, Meta’s proprietary Llama 3.3 and Documentation (and any portion thereof) made available under this Agreement.\\\\n\\\\\"Meta\\\\\" or \\\\\"we\\\\\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\\\\nBy clicking “I Accept” below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\\\\n1. License Rights and Redistribution.\\\\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta’s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\\\\nb. Redistribution and Use.\\\\ni. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Llama” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama” at the beginning of any such AI model name.\\\\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.\\\\_\\\\niii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a “Notice” text file distributed as a part of such copies: “Llama 3.3 is licensed under the Llama 3.3 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\\\\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at [https://www.llama.com/llama3\\\\\\\\_3/use-policy](https://www.llama.com/llama3_3/use-policy)), which is hereby incorporated by reference into this Agreement.  \\\\n2. Additional Commercial Terms. If, on the Llama 3.3 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\\\\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\\\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\\\n5. Intellectual Property.\\\\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the “Mark”) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible at [https://about.meta.com/brand/resources/meta/company-brand/](https://about.meta.com/brand/resources/meta/company-brand/)[)](https://en.facebookbrand.com/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\\\\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\\\\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.3 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\\\\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\\\\n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\\\\n### Llama 3.3 Acceptable Use Policy\\\\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.3. If you access or use Llama 3.3, you agree to this Acceptable Use Policy (“**Policy**”). The most recent copy of this policy can be found at [https://www.llama.com/llama3\\\\\\\\_3/use-policy](https://www.llama.com/llama3_3/use-policy).\\\\nProhibited Uses\\\\nWe want everyone to use Llama 3.3 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.3 to:\\\\n1. Violate the law or others’ rights, including to:\\\\n\\\\n   1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:  \\\\n      1. Violence or terrorism  \\\\n      2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material  \\\\n      3. Human trafficking, exploitation, and sexual violence  \\\\n      4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.  \\\\n      5. Sexual solicitation  \\\\n      6. Any other criminal activity\\\\n\\\\n   2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\\\\n\\\\n   3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\\\\n\\\\n   4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\\\\n\\\\n   5. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals’ identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\\\\n\\\\n   6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\\\\n\\\\n   7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\\\\n\\\\n   8. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\\\\n\\\\n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.3 related to the following:\\\\n\\\\n   1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\\\\n\\\\n   2. Guns and illegal weapons (including weapon development)\\\\n\\\\n   3. Illegal drugs and regulated/controlled substances\\\\n\\\\n   4. Operation of critical infrastructure, transportation technologies, or heavy machinery\\\\n\\\\n   5. Self-harm or harm to others, including suicide, cutting, and eating disorders\\\\n\\\\n   6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\\\\n\\\\n3. Intentionally deceive or mislead others, including use of Llama 3.3 related to the following:\\\\n\\\\n   1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\\\n\\\\n   2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\\\\n\\\\n   3. Generating, promoting, or further distributing spam\\\\n\\\\n   4. Impersonating another individual without consent, authorization, or legal right\\\\n\\\\n   5. Representing that the use of Llama 3.3 or outputs are human-generated\\\\n\\\\n   6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\\\\n\\\\n4. Fail to appropriately disclose to end users any known dangers of your AI system\\\\n5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.3\\\\nWith respect to any multimodal models included in Llama 3.3, the rights granted under Section 1(a) of the Llama 3.3 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\\\\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\\\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)   * Reporting risky content generated by the model: [developers.facebook.com/llama\\\\\\\\_output\\\\\\\\_feedback](http://developers.facebook.com/llama_output_feedback)   * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)   * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.3: LlamaUseReport@meta.com  \"\\nextra_gated_fields:\\n  First Name: text\\n  Last Name: text\\n  Date of birth: date_picker\\n  Country: country\\n  Affiliation: text\\n  Job title:\\n    type: select\\n    options:\\n    - Student\\n    - Research Graduate\\n    - AI researcher\\n    - AI developer/engineer\\n    - Reporter\\n    - Other\\n  geo: ip_location\\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\\nextra_gated_description: >-\\n  The information you provide will be collected, stored, processed and shared in\\n  accordance with the [Meta Privacy\\n  Policy](https://www.facebook.com/privacy/policy/).\\nextra_gated_button_content: Submit\\nlicense: llama3.3\\n---\\n## Model Information\\n\\nThe Meta Llama 3.3 multilingual large language model (LLM) is an instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\\n\\n**Model developer**: Meta\\n\\n**Model Architecture:** Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \\n\\n|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Token count | Knowledge cutoff |\\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\\n| Llama 3.3 (text only)  | A new mix of publicly available online data. | 70B | Multilingual Text | Multilingual Text and code  | 128k | Yes | 15T+ | December 2023 |\\n\\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\\n\\n**Llama 3.3 model**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\\n\\n**Model Release Date:** \\n\\n* **70B Instruct: December 6, 2024** \\n\\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\\n\\n**License** A custom commercial license, the Llama 3.3 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3\\\\_3/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE)\\n\\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \\n\\n## Intended Use\\n\\n**Intended Use Cases** Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases. \\n\\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card\\\\*\\\\*.\\n\\n\\\\*\\\\*Note: Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\\n\\n## How to use\\n\\nThis repository contains two versions of Llama-3.3-70B-Instruct, for use with transformers and with the original `llama` codebase.\\n\\n### Use with transformers\\n\\nStarting with `transformers >= 4.45.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\\n\\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\\n\\nSee the snippet below for usage with Transformers:\\n\\n```python\\nimport transformers\\nimport torch\\n\\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\\n\\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\\n    device_map=\"auto\",\\n)\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\\n]\\n\\noutputs = pipeline(\\n    messages,\\n    max_new_tokens=256,\\n)\\nprint(outputs[0][\"generated_text\"][-1])\\n```\\n\\n### Tool use with transformers\\n\\nLLaMA-3.3 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\\n\\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. \\nHere is a quick example showing a single simple tool:\\n\\n```python\\n# First, define a tool\\ndef get_current_temperature(location: str) -> float:\\n    \"\"\"\\n    Get the current temperature at a location.\\n    \\n    Args:\\n        location: The location to get the temperature for, in the format \"City, Country\"\\n    Returns:\\n        The current temperature at the specified location in the specified units, as a float.\\n    \"\"\"\\n    return 22.  # A real function should probably actually get the temperature!\\n\\n# Next, create a chat and apply the chat template\\nmessages = [\\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\\n  {\"role\": \"user\", \"content\": \"Hey, what\\'s the temperature in Paris right now?\"}\\n]\\n\\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\\n```\\n\\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\\n\\n```python\\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\\n```\\n\\nand then call the tool and append the result, with the `tool` role, like so:\\n\\n```python\\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\\n```\\n\\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\\n\\n\\n### Use with `bitsandbytes`\\n\\nThe model checkpoints can be used in `8-bit` and `4-bit` for further memory optimisations using `bitsandbytes` and `transformers`\\n\\nSee the snippet below for usage:\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\\n\\nquantized_model = AutoModelForCausalLM.from_pretrained(\\n\\tmodel_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\ninput_text = \"What are we having for dinner?\"\\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\\n\\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\\n\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\n```\\n\\nTo load in 4-bit simply pass `load_in_4bit=True`\\n\\n### Use with `llama`\\n\\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\\n\\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\\n\\n```\\nhuggingface-cli download meta-llama/Llama-3.3-70B-Instruct --include \"original/*\" --local-dir Llama-3.3-70B-Instruct\\n```\\n\\n## Hardware and Software\\n\\n**Training Factors** We used custom training libraries, Meta\\'s custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\\n\\n**Training Energy Use** Training utilized a cumulative of **39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \\n\\n## \\n\\n## **Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\\n\\n|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\\n| :---- | :---: | :---: | :---: | :---: |\\n| Llama 3.3 70B | 7.0M | 700 | 2,040 | 0 |\\n\\n## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\\n\\n## Training Data\\n\\n**Overview:** Llama 3.3 was pretrained on \\\\~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \\n\\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\\\.\\n\\n## Benchmarks \\\\- English Text\\n\\nIn this section, we report the results for Llama 3.3 relative to our previous models. \\n\\n### Instruction tuned models\\n\\n## \\n\\n| Category | Benchmark | \\\\# Shots | Metric | Llama 3.1 8B Instruct | Llama 3.1 70B Instruct | Llama-3.3 70B Instruct | Llama 3.1 405B Instruct |\\n| :---- | :---- | ----- | :---- | ----- | ----- | ----- | ----- |\\n|  | MMLU (CoT) | 0 | macro\\\\_avg/acc | 73.0 | 86.0 | 86.0 | 88.6 |\\n|  | MMLU Pro (CoT) | 5 | macro\\\\_avg/acc | 48.3 | 66.4 | 68.9 | 73.3 |\\n| Steerability | IFEval |  |  | 80.4 | 87.5 | 92.1 | 88.6 |\\n| Reasoning | GPQA Diamond (CoT) | 0 | acc | 31.8 | 48.0 | 50.5 | 49.0 |\\n| Code | HumanEval | 0 | pass@1 | 72.6 | 80.5 | 88.4 | 89.0 |\\n|  | MBPP EvalPlus (base) | 0 | pass@1 | 72.8 | 86.0 | 87.6 | 88.6 |\\n| Math | MATH (CoT) | 0 | sympy\\\\_intersection\\\\_score | 51.9 | 68.0 | 77.0 | 73.8 |\\n| Tool Use | BFCL v2 | 0 | overall\\\\_ast\\\\_summary/macro\\\\_avg/valid | 65.4 | 77.5 | 77.3 | 81.1 |\\n| Multilingual | MGSM | 0 | em | 68.9 | 86.9 | 91.1 | 91.6 |\\n\\n## \\n\\n## Responsibility & Safety\\n\\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\\n\\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\\n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\\n* Provide protections for the community to help prevent the misuse of our models.\\n\\n### Responsible deployment\\n\\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\\n\\n#### Llama 3.3 instruct\\n\\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\\n\\n**Fine-tuning data**\\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\\n\\n**Refusals and Tone**\\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\\n\\n#### Llama 3.3 systems\\n\\n**Large language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\\n\\n#### Capability specific considerations \\n\\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \\n\\n**Multilinguality**: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \\n\\n### Evaluations\\n\\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\\n\\n**Red teaming**   \\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.   \\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. . \\n\\n### Critical and other risks \\n\\n### We specifically focused our efforts on mitigating the following critical risk areas:\\n\\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\\nTo assess risks related to proliferation of chemical and biological weapons of the Llama 3 family of models, we performed uplift testing designed to assess whether use of the Llama 3 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\\n\\n### **2\\\\. Child Safety**\\n\\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \\n\\n**3\\\\. Cyber attack enablement**\\nOur cyber attack uplift study investigated whether the Llama 3 family of LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\\n\\n### Community \\n\\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \\n\\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \\n\\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\\n\\n## Ethical Considerations and Limitations\\n\\nThe core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \\n\\nBut Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=meta-llama/Llama-3.3-70B-Instruct\n",
      "> RSP [{'title': '| [Download The New Model Here!]', 'url': 'https://llamaimodel.com/3-70b/', 'content': 'What is Llama 3.3 70B Instruct? Llama 3.3 70B is an advanced, open-source large language model by Meta. It has 70 billion parameters, delivering near state-of-the-art performance. This model supports multiple languages and long context windows. Its efficient design makes high-quality AI more accessible and cost-effective.', 'score': 0.9123193, 'raw_content': None}, {'title': 'Llama 3.3 70B Instruct - API, Providers, Stats | OpenRouter', 'url': 'https://openrouter.ai/meta-llama/llama-3.3-70b-instruct', 'content': 'Llama 3.3 70B Instruct - API, Providers, Stats | OpenRouter Meta: Llama 3.3 70B Instruct meta-llama/llama-3.3-70b-instruct The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). Chat with Llama 3.3 70B Instruct Providers for Llama 3.3 70B Instruct Apps using Llama 3.3 70B Instruct Recent activity on Llama 3.3 70B Instruct Uptime stats for Llama 3.3 70B Instruct Uptime stats for Llama 3.3 70B Instruct across all providers Sample code and API for Llama 3.3 70B Instruct Create API keyOpenRouter provides an OpenAI-compatible completion API to 0 models & providers that you can call directly, or using the OpenAI SDK. model: \"meta-llama/llama-3.3-70b-instruct\", fetch(\"https://openrouter.ai/api/v1/chat/completions\", { \"model\": \"meta-llama/llama-3.3-70b-instruct\",', 'score': 0.8997663, 'raw_content': None}, {'title': 'meta / llama-3.3-70b-instruct - docs.api.nvidia.com', 'url': 'https://docs.api.nvidia.com/nim/reference/meta-llama-3_3-70b-instruct', 'content': 'meta / llama-3.3-70b-instruct meta / llama-3.3-70b-instruct meta / llama-3.1-8b-instruct nvidia / llama-3.1-nemotron-51b-instruct nvidia/llama-3.1-nemotron-70b-instruct nvidia / llama-3.1-nemotron-51b-instruct nvidia/llama-3.1-nemotron-70b-instruct The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction-tuned text-only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.', 'score': 0.8991304, 'raw_content': None}, {'title': \"Meta's new Llama 3.3 70B Instruct model now available on watsonx.ai - IBM\", 'url': 'https://www.ibm.com/new/announcements/meta-s-new-llama-3-3-70b-instruct-model-now-available-on-watsonx-ai', 'content': 'Meta’s new Llama 3.3 70B Instruct model now available on watsonx.ai Meta’s new Llama 3.3 70B Instruct model now available on watsonx.ai The release of the updated Llama 3.3 Instruct 70B model on IBM’s watsonx.ai model catalogue continues to build on the work initiated by IBM and Meta last year, aimed at fostering open innovation in AI. Watsonx.ai already supports the Llama 3.2 models as well. Companies are increasingly using Llama on watsonx.ai as they turn to open AI models and tools to create clear business value. Watsonx.ai offers a modern enterprise studio where AI developers can train, test, fine-tune, and deploy various models, including Llama. Developers can now inference the updated Llama 3.3 70B Instruct model on watsonx.ai.', 'score': 0.8740022, 'raw_content': None}, {'title': 'osllmai-community/Llama-3.3-70B-Instruct - Hugging Face', 'url': 'https://huggingface.co/osllmai-community/Llama-3.3-70B-Instruct', 'content': 'The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner. Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our Community Stories webpage.', 'score': 0.74273956, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=meta-llama/Llama-3.3-70B-Instruct\n",
      "> RSP [{'title': 'Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging: A Comprehensive Evaluation', 'url': 'https://arxiv.org/abs/2406.14971', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. cs arXiv:2406.14971 Help | Advanced Search arXiv author ID Help pages Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging: A Comprehensive Evaluation We conducted extensive experiments on domain adaptation of the Meta-Llama-3-70B-Instruct model on SEC data, exploring its performance on both general and domain-specific benchmarks. Our focus included continual pre-training (CPT) and model merging, aiming to enhance the model's domain-specific capabilities while mitigating catastrophic forgetting. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2406.14971 [cs.CL] (or arXiv:2406.14971v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2406.14971 From: Anneketh Vij [view email] cs.CL cs cs.AI Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle\", 'score': 0.61834323, 'raw_content': None}, {'title': 'arXiv:2409.00096v1 [cs.CL] 27 Aug 2024', 'url': 'https://arxiv.org/pdf/2409.00096', 'content': 'SFT dataset, and the fine-tuned Meta-Llama-3-70b-Instruct model achieved the highest recorded score of 57.0, surpassing even the more advanced Meta-Llama-3.1-70b-Instruct. These results underscore the effec-tiveness of our fine-tuning approach in enhancing the instruction-following capabilities of large language mod-els.', 'score': 0.5417246, 'raw_content': None}, {'title': 'arXiv:2406.19314v1 [cs.CL] 27 Jun 2024', 'url': 'https://arxiv.org/pdf/2406.19314', 'content': 'meta-llama-3-8b-instruct phi-3-small-128k-instruct command-r qwen1.5-72b-chat qwen1.5-110b-chat deepseek-coder-v2-lite-instruct phi-3-medium-128k-instruct phi-3-medium-4k-instruct mistral-small-2402 command-r-plus gpt-3.5-turbo-1106 gpt-3.5-turbo-0125 mixtral-8x22b-instruct-v0.1 claude-3-haiku-20240307 meta-llama-3-70b-instruct claude-3-sonnet', 'score': 0.507251, 'raw_content': None}, {'title': '[2407.21783] The Llama 3 Herd of Models - arXiv.org', 'url': 'https://arxiv.org/abs/2407.21783', 'content': 'Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive', 'score': 0.3097232, 'raw_content': None}, {'title': 'Fine-tuning a Llama 3-70B Model for Radiology Report Processing - arXiv.org', 'url': 'https://arxiv.org/html/2408.11848v2', 'content': 'Fine-tuning a Llama 3-70B Model for Radiology Report Processing Fine-tuning a Llama 3-70B Model for Radiology Report Processing RadiologyLlama-70B is developed using the Llama 3-70B backbone following the fine-tuning scheme of our previous domain-specific models like Radiology-GPT and Radiology-Llama2. Previous work such as Radiology-GPT\\xa0[3] and Radiology-Llama2\\xa0[5] demonstrated the effectiveness of domain-specific LLMs in radiology tasks, outperforming general-purpose models in generating coherent and clinically relevant impressions from radiology findings. [3] Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Lichao Sun, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li, and Tianming Liu. Radiology-gpt: A large language model for radiology, 2024.', 'score': 0.29354164, 'raw_content': None}, {'title': 'A arXiv:2407.14482v2 [cs.CL] 9 Sep 2024', 'url': 'https://arxiv.org/pdf/2407.14482', 'content': 'The concurrent work Llama-3.1-70B-Instruct (Meta-AI,2024) also supports a 128K context window. However, the absence of training data and reproduction recipe makes replicating these models ... fine-tunes the Llama-3-Instruct, which uses NTK-aware interpolation (Peng et al.,2023) and the formula fromLiu et al.(2023b) to scale up θ.', 'score': 0.2911778, 'raw_content': None}, {'title': 'TÜLU 3:PushingFrontiersin OpenLanguageModelPost-Training - arXiv.org', 'url': 'https://arxiv.org/pdf/2411.15124', 'content': 'Demo: https://playground.allenai.org/ Model Checkpoints Stage Llama 3.1 8B Llama 3.1 70B Base Model meta-llama/Llama-3.1-8B meta-llama/Llama-3.1-70B SFT allenai/Llama-3.1-Tulu-3-8B-SFT allenai/Llama-3.1-Tulu-3-70B-SFT DPO allenai/Llama-3.1-Tulu-3-8B-DPO allenai/Llama-3.1-Tulu-3-70B-DPO Final Models (RLVR) allenai/Llama-3.1-Tulu-3-8B RM: allenai/Llama-3.1-Tulu-3-8B-RM allenai/Llama-3.1-Tulu-3-70B Codebases / Tools Type Link Training allenai/open-instruct TÜLU 3 EVAL allenai/olmes Decontamination allenai/open-instruct/tree/main/decontamination Preference Data Inference allenai/birr Instruction Datasets Type Domain Link Full mix General allenai/tulu-3-sft-mixture Task Specific Precise Instruction Following allenai/tulu-3-sft-personas-instruction-following Subsets MATH allenai/tulu-3-sft-personas-math Grade School Math allenai/tulu-3-sft-personas-math-grade Python Code allenai/tulu-3-sft-personas-code Preference Mixes Model Link Llama 3.1 70B allenai/llama-3.1-tulu-3-70b-preference-mixture Llama 3.1 8B allenai/llama-3.1-tulu-3-8b-preference-mixture Specific Preference Datasets Domain Link Precise Instruction Following allenai/tulu-3-pref-personas-instruction-following General allenai/tulu-3-sft-prompts-ultrafeedback General allenai/tulu-3-wildchat-ultrafeedback RL with Verifiable Rewards Training Datasets Domain Link Full Mix allenai/RLVR-GSM-MATH-IF-Mixed-Constraints GSM Only allenai/RLVR-GSM MATH Only allenai/RLVR-MATH IFeval Only allenai/RLVR-IFeval 4 Curate prompts Base Model Tülu3-SFT Tülu3-DPO Tülu3 public datasets persona-driven synthetic instructions data mixing Build evaluation suite Supervised finetuning Direct pref.', 'score': 0.2530161, 'raw_content': None}, {'title': 'Hermes 3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2408.11857', 'content': 'Hermes 3 are highly steerable instruct and chat tuned models created by fine-tuning Llama 3.1 8B, 70B, and 405B. The models attempt to place themselves within the world view indicated in their system prompt and faithfully respond to the request of the user. The models are therefore highly sensitive to the system prompt. The effect of this', 'score': 0.25243515, 'raw_content': None}, {'title': 'A Practice of Post-Training on Llama-3 70B with Optimal Selection of ...', 'url': 'https://arxiv.org/html/2409.06624v1', 'content': 'Continually Pre-Train (CPT) is generally employed to enhance LLM into two types of tasks, to expand the linguistic ability of LLM to a new language Ke and Liu (2023); Cui et\\xa0al. Based upon the recent LlaMA-3 AI@Meta (2024), Llama-3-Chinese conduct CPT on Llama-3 8B with LORA 3, while Llama3-SynE Chen et\\xa0al. Table 3 lists results of the aforementioned benchmarks, in which we compare our CPT model with Base, as well as Llama-3-Chinese on size of 8B. We add another open-sourced baseline, Qwen2-72B-Instruct Team and Group (2024), which has a similar size of model parameters, and is intentionally trained with Chinese corpus and achieve top performance in Chinese benchmark. 2024.D-cpt law: Domain-specific continual pre-training scaling law for large language models.Preprint, arXiv:2406.01375.', 'score': 0.24258713, 'raw_content': None}, {'title': 'The Uniqueness of LLaMA3-70B with Per-Channel - arXiv.org', 'url': 'https://arxiv.org/html/2408.15301v1', 'content': 'To elucidate the unique vulnerability of the LLaMA3-70B series to W8A8 per-channel quantization, we conducted a comparative analysis of its weight distributions against those of other robust models. Figure\\xa01 and Figure\\xa03 present a comparative analysis of accuracy for several models in the LLaMA3-70B series subjected to 8-bit per-channel post-training quantization. In the latter layers, the LLaMA3-70B model demonstrates behavior similar to other models in terms of quantization errors and max_abs of weights. Based on our analysis of Figure\\xa09 , we have empirically identified that in the LLaMA3-70B model, the Q, K, V, Up, and Gate matrices of Block 0, 1, and 3 exhibit exceptionally high quantization errors and maximum absolute weight values.', 'score': 0.18229482, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2408.11857\n",
      "> RSP [Hermes 3 Technical Report](http://arxiv.org/abs/2408.11857v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2408.11848\n",
      "> RSP [MGH Radiology Llama: A Llama 3 70B Model for Radiology](http://arxiv.org/abs/2408.11848v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2406.14971\n",
      "> RSP [Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging: A Comprehensive Evaluation](http://arxiv.org/abs/2406.14971v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2407.21783\n",
      "> RSP [The Llama 3 Herd of Models](http://arxiv.org/abs/2407.21783v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2407.14482\n",
      "> RSP [ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities](http://arxiv.org/abs/2407.14482v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2409.00096\n",
      "> RSP [Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data](http://arxiv.org/abs/2409.00096v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.15124\n",
      "> RSP [Tulu 3: Pushing Frontiers in Open Language Model Post-Training](http://arxiv.org/abs/2411.15124v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2409.06624\n",
      "> RSP [A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio](http://arxiv.org/abs/2409.06624v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2408.15301\n",
      "> RSP [The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization](http://arxiv.org/abs/2408.15301v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2406.19314\n",
      "> RSP [LiveBench: A Challenging, Contamination-Free LLM Benchmark](http://arxiv.org/abs/2406.19314v1)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': '| [Download The New Model Here!]', 'url': 'https://llamaimodel.com/3-70b/', 'content': 'What is Llama 3.3 70B Instruct? Llama 3.3 70B is an advanced, open-source large language model by Meta. It has 70 billion parameters, delivering near state-of-the-art performance. This model supports multiple languages and long context windows. Its efficient design makes high-quality AI more accessible and cost-effective.', 'score': 0.9123193, 'raw_content': None}, {'title': 'Llama 3.3 70B Instruct - API, Providers, Stats | OpenRouter', 'url': 'https://openrouter.ai/meta-llama/llama-3.3-70b-instruct', 'content': 'Llama 3.3 70B Instruct - API, Providers, Stats | OpenRouter Meta: Llama 3.3 70B Instruct meta-llama/llama-3.3-70b-instruct The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). Chat with Llama 3.3 70B Instruct Providers for Llama 3.3 70B Instruct Apps using Llama 3.3 70B Instruct Recent activity on Llama 3.3 70B Instruct Uptime stats for Llama 3.3 70B Instruct Uptime stats for Llama 3.3 70B Instruct across all providers Sample code and API for Llama 3.3 70B Instruct Create API keyOpenRouter provides an OpenAI-compatible completion API to 0 models & providers that you can call directly, or using the OpenAI SDK. model: \"meta-llama/llama-3.3-70b-instruct\", fetch(\"https://openrouter.ai/api/v1/chat/completions\", { \"model\": \"meta-llama/llama-3.3-70b-instruct\",', 'score': 0.8997663, 'raw_content': None}, {'title': 'meta / llama-3.3-70b-instruct - docs.api.nvidia.com', 'url': 'https://docs.api.nvidia.com/nim/reference/meta-llama-3_3-70b-instruct', 'content': 'meta / llama-3.3-70b-instruct meta / llama-3.3-70b-instruct meta / llama-3.1-8b-instruct nvidia / llama-3.1-nemotron-51b-instruct nvidia/llama-3.1-nemotron-70b-instruct nvidia / llama-3.1-nemotron-51b-instruct nvidia/llama-3.1-nemotron-70b-instruct The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction-tuned text-only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.', 'score': 0.8991304, 'raw_content': None}, {'title': \"Meta's new Llama 3.3 70B Instruct model now available on watsonx.ai - IBM\", 'url': 'https://www.ibm.com/new/announcements/meta-s-new-llama-3-3-70b-instruct-model-now-available-on-watsonx-ai', 'content': 'Meta’s new Llama 3.3 70B Instruct model now available on watsonx.ai Meta’s new Llama 3.3 70B Instruct model now available on watsonx.ai The release of the updated Llama 3.3 Instruct 70B model on IBM’s watsonx.ai model catalogue continues to build on the work initiated by IBM and Meta last year, aimed at fostering open innovation in AI. Watsonx.ai already supports the Llama 3.2 models as well. Companies are increasingly using Llama on watsonx.ai as they turn to open AI models and tools to create clear business value. Watsonx.ai offers a modern enterprise studio where AI developers can train, test, fine-tune, and deploy various models, including Llama. Developers can now inference the updated Llama 3.3 70B Instruct model on watsonx.ai.', 'score': 0.8740022, 'raw_content': None}, {'title': 'osllmai-community/Llama-3.3-70B-Instruct - Hugging Face', 'url': 'https://huggingface.co/osllmai-community/Llama-3.3-70B-Instruct', 'content': 'The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner. Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our Community Stories webpage.', 'score': 0.74273956, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "library_name: transformers\n",
      "language:\n",
      "- en\n",
      "- fr\n",
      "- it\n",
      "- pt\n",
      "- hi\n",
      "- es\n",
      "- th\n",
      "- de\n",
      "base_model:\n",
      "- meta-llama/Llama-3.1-70B\n",
      "tags:\n",
      "- facebook\n",
      "- meta\n",
      "- pytorch\n",
      "- llama\n",
      "- llama-3\n",
      "extra_gated_prompt: \"### LLAMA 3.3 COMMUNITY LICENSE AGREEMENT\\nLlama 3.3 Version Release Date: December 6, 2024\\n\\\"Agreement\\\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\\n\\\"Documentation\\\" means the specifications, manuals and documentation accompanying Llama 3.3 distributed by Meta at\n",
      "https://www.llama.com/docs/overview\n",
      ".\\n\\\"Licensee\\\" or \\\"you\\\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\\n\\\"Llama 3.3\\\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at\n",
      "https://www.llama.com/llama-downloads\n",
      ".\\n\\\"Llama Materials\\\" means, collectively, Meta’s proprietary Llama 3.3 and Documentation (and any portion thereof) made available under this Agreement.\\n\\\"Meta\\\" or \\\"we\\\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\\nBy clicking “I Accept” below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\\n1. License Rights and Redistribution.\\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta’s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\\nb. Redistribution and Use.\\ni. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Llama” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama” at the beginning of any such AI model name.\\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you._\\niii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a “Notice” text file distributed as a part of such copies: “Llama 3.3 is licensed under the Llama 3.3 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at\n",
      "https://www.llama.com/llama3\\_3/use-policy\n",
      "), which is hereby incorporated by reference into this Agreement.  \\n2. Additional Commercial Terms. If, on the Llama 3.3 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the “Mark”) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible at\n",
      "https://about.meta.com/brand/resources/meta/company-brand/\n",
      ")\n",
      ". All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.3 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\\n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\\n### Llama 3.3 Acceptable Use Policy\\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.3. If you access or use Llama 3.3, you agree to this Acceptable Use Policy (“\n",
      "Policy\n",
      "”). The most recent copy of this policy can be found at\n",
      "https://www.llama.com/llama3\\_3/use-policy\n",
      ".\\nProhibited Uses\\nWe want everyone to use Llama 3.3 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.3 to:\\n1. Violate the law or others’ rights, including to:\\n\\n   1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:  \\n      1. Violence or terrorism  \\n      2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material  \\n      3. Human trafficking, exploitation, and sexual violence  \\n      4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.  \\n      5. Sexual solicitation  \\n      6. Any other criminal activity\\n\\n   2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\n   3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\\n\\n   4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\\n\\n   5. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals’ identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\\n\\n   6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\\n\\n   7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\\n\\n   8. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\\n\\n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.3 related to the following:\\n\\n   1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\\n\\n   2. Guns and illegal weapons (including weapon development)\\n\\n   3. Illegal drugs and regulated/controlled substances\\n\\n   4. Operation of critical infrastructure, transportation technologies, or heavy machinery\\n\\n   5. Self-harm or harm to others, including suicide, cutting, and eating disorders\\n\\n   6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\\n\\n3. Intentionally deceive or mislead others, including use of Llama 3.3 related to the following:\\n\\n   1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\n   2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\\n\\n   3. Generating, promoting, or further distributing spam\\n\\n   4. Impersonating another individual without consent, authorization, or legal right\\n\\n   5. Representing that the use of Llama 3.3 or outputs are human-generated\\n\\n   6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\\n\\n4. Fail to appropriately disclose to end users any known dangers of your AI system\\n5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.3\\nWith respect to any multimodal models included in Llama 3.3, the rights granted under Section 1(a) of the Llama 3.3 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\\n* Reporting issues with the model:\n",
      "https://github.com/meta-llama/llama-models/issues\n",
      "* Reporting risky content generated by the model:\n",
      "developers.facebook.com/llama\\_output\\_feedback\n",
      "* Reporting bugs and security concerns:\n",
      "facebook.com/whitehat/info\n",
      "* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.3: LlamaUseReport@meta.com  \"\n",
      "extra_gated_fields:\n",
      "  First Name: text\n",
      "  Last Name: text\n",
      "  Date of birth: date_picker\n",
      "  Country: country\n",
      "  Affiliation: text\n",
      "  Job title:\n",
      "    type: select\n",
      "    options:\n",
      "    - Student\n",
      "    - Research Graduate\n",
      "    - AI researcher\n",
      "    - AI developer/engineer\n",
      "    - Reporter\n",
      "    - Other\n",
      "  geo: ip_location\n",
      "  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\n",
      "extra_gated_description: >-\n",
      "  The information you provide will be collected, stored, processed and shared in\n",
      "  accordance with the\n",
      "Meta Privacy\n",
      "  Policy\n",
      ".\n",
      "extra_gated_button_content: Submit\n",
      "license: llama3.3\n",
      "Model Information\n",
      "The Meta Llama 3.3 multilingual large language model (LLM) is an instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n",
      "Model developer\n",
      ": Meta\n",
      "Model Architecture:\n",
      "Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n",
      "|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Token count | Knowledge cutoff |\n",
      "| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n",
      "| Llama 3.3 (text only)  | A new mix of publicly available online data. | 70B | Multilingual Text | Multilingual Text and code  | 128k | Yes | 15T+ | December 2023 |\n",
      "Supported languages:\n",
      "English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
      "Llama 3.3 model\n",
      ". Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n",
      "Model Release Date:\n",
      "70B Instruct: December 6, 2024\n",
      "Status:\n",
      "This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n",
      "License\n",
      "A custom commercial license, the Llama 3.3 Community License Agreement, is available at:\n",
      "https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE\n",
      "Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model\n",
      "README\n",
      ". For more technical information about generation parameters and recipes for how to use Llama 3.3 in applications, please go\n",
      "here\n",
      ".\n",
      "Intended Use\n",
      "Intended Use Cases\n",
      "Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.\n",
      "Out-of-scope\n",
      "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n",
      "**Note: Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\n",
      "How to use\n",
      "This repository contains two versions of Llama-3.3-70B-Instruct, for use with transformers and with the original\n",
      "llama\n",
      "codebase.\n",
      "Use with transformers\n",
      "Starting with\n",
      "transformers >= 4.45.0\n",
      "onward, you can run conversational inference using the Transformers\n",
      "pipeline\n",
      "abstraction or by leveraging the Auto classes with the\n",
      "generate()\n",
      "function.\n",
      "Make sure to update your transformers installation via\n",
      "pip install --upgrade transformers\n",
      ".\n",
      "See the snippet below for usage with Transformers:\n",
      "```python\n",
      "import transformers\n",
      "import torch\n",
      "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
      "pipeline = transformers.pipeline(\n",
      "    \"text-generation\",\n",
      "    model=model_id,\n",
      "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
      "    device_map=\"auto\",\n",
      ")\n",
      "messages = [\n",
      "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
      "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
      "]\n",
      "outputs = pipeline(\n",
      "    messages,\n",
      "    max_new_tokens=256,\n",
      ")\n",
      "print(outputs[0][\"generated_text\"][-1])\n",
      "```\n",
      "Tool use with transformers\n",
      "LLaMA-3.3 supports multiple tool use formats. You can see a full guide to prompt formatting\n",
      "here\n",
      ".\n",
      "Tool use is also supported through\n",
      "chat templates\n",
      "in Transformers. \n",
      "Here is a quick example showing a single simple tool:\n",
      "```python\n",
      "First, define a tool\n",
      "def get_current_temperature(location: str) -> float:\n",
      "    \"\"\"\n",
      "    Get the current temperature at a location.\n",
      "Args:\n",
      "    location: The location to get the temperature for, in the format \"City, Country\"\n",
      "Returns:\n",
      "    The current temperature at the specified location in the specified units, as a float.\n",
      "\"\"\"\n",
      "return 22.  # A real function should probably actually get the temperature!\n",
      "Next, create a chat and apply the chat template\n",
      "messages = [\n",
      "  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n",
      "  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n",
      "]\n",
      "inputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n",
      "```\n",
      "You can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n",
      "python\n",
      "tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\n",
      "messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n",
      "and then call the tool and append the result, with the\n",
      "tool\n",
      "role, like so:\n",
      "python\n",
      "messages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n",
      "After that, you can\n",
      "generate()\n",
      "again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\n",
      "see the\n",
      "LLaMA prompt format docs\n",
      "and the Transformers\n",
      "tool use documentation\n",
      ".\n",
      "Use with\n",
      "bitsandbytes\n",
      "The model checkpoints can be used in\n",
      "8-bit\n",
      "and\n",
      "4-bit\n",
      "for further memory optimisations using\n",
      "bitsandbytes\n",
      "and\n",
      "transformers\n",
      "See the snippet below for usage:\n",
      "```python\n",
      "import torch\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
      "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
      "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
      "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "input_text = \"What are we having for dinner?\"\n",
      "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
      "output = quantized_model.generate(**input_ids, max_new_tokens=10)\n",
      "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
      "```\n",
      "To load in 4-bit simply pass\n",
      "load_in_4bit=True\n",
      "Use with\n",
      "llama\n",
      "Please, follow the instructions in the\n",
      "repository\n",
      ".\n",
      "To download Original checkpoints, see the example command below leveraging\n",
      "huggingface-cli\n",
      ":\n",
      "huggingface-cli download meta-llama/Llama-3.3-70B-Instruct --include \"original/*\" --local-dir Llama-3.3-70B-Instruct\n",
      "Hardware and Software\n",
      "Training Factors\n",
      "We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n",
      "Training Energy Use\n",
      "Training utilized a cumulative of\n",
      "39.3\n",
      "M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n",
      "Training Greenhouse Gas Emissions\n",
      "Estimated total location-based greenhouse gas emissions were\n",
      "11,390\n",
      "tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n",
      "|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n",
      "| :---- | :---: | :---: | :---: | :---: |\n",
      "| Llama 3.3 70B | 7.0M | 700 | 2,040 | 0 |\n",
      "The methodology used to determine training energy use and greenhouse gas emissions can be found\n",
      "here\n",
      ".  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n",
      "Training Data\n",
      "Overview:\n",
      "Llama 3.3 was pretrained on \\~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\n",
      "Data Freshness:\n",
      "The pretraining data has a cutoff of December 2023.\n",
      "Benchmarks - English Text\n",
      "In this section, we report the results for Llama 3.3 relative to our previous models.\n",
      "Instruction tuned models\n",
      "| Category | Benchmark | # Shots | Metric | Llama 3.1 8B Instruct | Llama 3.1 70B Instruct | Llama-3.3 70B Instruct | Llama 3.1 405B Instruct |\n",
      "| :---- | :---- | ----- | :---- | ----- | ----- | ----- | ----- |\n",
      "|  | MMLU (CoT) | 0 | macro_avg/acc | 73.0 | 86.0 | 86.0 | 88.6 |\n",
      "|  | MMLU Pro (CoT) | 5 | macro_avg/acc | 48.3 | 66.4 | 68.9 | 73.3 |\n",
      "| Steerability | IFEval |  |  | 80.4 | 87.5 | 92.1 | 88.6 |\n",
      "| Reasoning | GPQA Diamond (CoT) | 0 | acc | 31.8 | 48.0 | 50.5 | 49.0 |\n",
      "| Code | HumanEval | 0 | pass@1 | 72.6 | 80.5 | 88.4 | 89.0 |\n",
      "|  | MBPP EvalPlus (base) | 0 | pass@1 | 72.8 | 86.0 | 87.6 | 88.6 |\n",
      "| Math | MATH (CoT) | 0 | sympy_intersection_score | 51.9 | 68.0 | 77.0 | 73.8 |\n",
      "| Tool Use | BFCL v2 | 0 | overall_ast_summary/macro_avg/valid | 65.4 | 77.5 | 77.3 | 81.1 |\n",
      "| Multilingual | MGSM | 0 | em | 68.9 | 86.9 | 91.1 | 91.6 |\n",
      "Responsibility & Safety\n",
      "As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n",
      "Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n",
      "Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n",
      "Provide protections for the community to help prevent the misuse of our models.\n",
      "Responsible deployment\n",
      "Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our\n",
      "Community Stories webpage\n",
      ". Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the\n",
      "Responsible Use Guide\n",
      "to learn more.\n",
      "Llama 3.3 instruct\n",
      "Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\n",
      "Fine-tuning data\n",
      "We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n",
      "Refusals and Tone\n",
      "Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n",
      "Llama 3.3 systems\n",
      "Large language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.\n",
      "Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\n",
      "As part of our responsible release approach, we provide the community with\n",
      "safeguards\n",
      "that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our\n",
      "reference implementations\n",
      "demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n",
      "Capability specific considerations\n",
      "Tool-use\n",
      ": Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards.\n",
      "Multilinguality\n",
      ": Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide.\n",
      "Evaluations\n",
      "We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.\n",
      "Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n",
      "Red teaming\n",
      "For both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.\n",
      "We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. .\n",
      "Critical and other risks\n",
      "We specifically focused our efforts on mitigating the following critical risk areas:\n",
      "1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness\n",
      "To assess risks related to proliferation of chemical and biological weapons of the Llama 3 family of models, we performed uplift testing designed to assess whether use of the Llama 3 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n",
      "2. Child Safety\n",
      "Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n",
      "3. Cyber attack enablement\n",
      "Our cyber attack uplift study investigated whether the Llama 3 family of LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n",
      "Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n",
      "Community\n",
      "Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our\n",
      "Github repository\n",
      ".\n",
      "We also set up the\n",
      "Llama Impact Grants\n",
      "program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found\n",
      "here\n",
      ".\n",
      "Finally, we put in place a set of resources including an\n",
      "output reporting mechanism\n",
      "and\n",
      "bug bounty program\n",
      "to continuously improve the Llama technology with the help of the community.\n",
      "Ethical Considerations and Limitations\n",
      "The core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n",
      "But Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our\n",
      "Responsible Use Guide\n",
      ",\n",
      "Trust and Safety\n",
      "solutions, and other\n",
      "resources\n",
      "to learn more about responsible development.\n",
      "> RSP * 70 billion parameters for high-capacity performance\n",
      "* Multilingual support for eight major languages\n",
      "* Optimized for long context windows up to 128k tokens\n",
      "* Instruction-tuned for enhanced capabilities in dialogue scenarios\n",
      "* Outperforms many existing models on common industry benchmarks\n",
      "* Capable of both text and code output modalities\n",
      "* Incorporates Grouped-Query Attention (GQA) for improved inference scalability\n",
      "* Designed for use in diverse applications, from chatbots to coding assistants\n",
      "* Community-driven license allows for fine-tuning and derivative works\n",
      "* Emphasis on safety and responsible use with built-in safeguards\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=StephanST/WALDO30\n",
      "> RSP {'model_id': 'StephanST/WALDO30', 'created_at': '02 October 2024 at 14:20:40 UTC', 'downloads': 0, 'likes': 163, 'trending_score': 102, 'description': '---\\nlicense: mit\\nlanguage:\\n- en\\nbase_model:\\n- Ultralytics/YOLOv8\\npipeline_tag: object-detection\\n---\\n\\nW.A.L.D.O.\\nWhereabouts Ascertainment for Low-lying Detectable Objects \\n---------------------------------------------------------------------\\n\\n\\n[![WALDO 3.0 preview vid](https://i.imgur.com/hGghrLn.jpeg)](https://www.youtube.com/watch?v=1y5y9yklj2U)\\n\\nWelcome to the WALDO v3.0 public release\\n---------------------------------------------------------------------\\n\\n\\nWHAT IS WALDO?\\n\\nWALDO is a detection AI model, based on a large YOLO-v8 backbone and my own\\nsynthetic data pipeline. **The model is capable of detecting these classes \\nof items in overhead imagery ranging in altitude from about 30 feet to \\nsatellite imagery!**\\n\\n\\nOutput classes:\\n\\n0 -> \\'LightVehicle\\'  --> all kinds of civilan cars, including pickup trucks, vans etc... 🚗🏎️🚓🚐🚑 </br>\\n1 -> \\'Person\\' --> people! all kinds of people including ones that are on bikes or swimming in the sea 🧍\\u200d♀️🕺💃🧜🏽\\u200d♀️🏂🧞</br> \\n2 -> \\'Building\\' --> all kinds of buildings 🕌🏛️🏭🏡</br>\\n3 -> \\'UPole\\' --> utility poles, power poles, anything thin and sticking up that you should avoid with a drone 🎏</br>\\n4 -> \\'Boat\\' --> boats, ships, canoes, kayaks, surf boards... all the floaty stuff 🚢🏄</br>\\n5 -> \\'Bike\\' --> bikes, mopeds, motorbikes, all stuff with 2 wheels 🚲</br>\\n6 -> \\'Container\\' --> shipping containers, including on the back of an articulated truck... 📦🏗️</br>\\n7 -> \\'Truck\\' --> large commercial vehicles including articulated trucks or big box-on-chassis delivery trucks 🚚</br>\\n8 -> \\'Gastank\\'--> cylindrical tanks such as butane tanks and gas expansion tanks, or grain silos... pretty much anything that looks cylindrical for storing liquids 🫙</br>\\n10 -> \\'Digger\\' --> all kinds of construction vehicles, including tractors and construction gear 🚜</br>\\n11 -> \\'Solarpanels\\' --> solar panels ▪️🌞▪️</br>\\n12 -> \\'Bus\\' --> a bus 🚌</br>\\n\\n--> In general the lower the class number the better-trained you can expect it to be.\\nFor users of previous versions of WALDO: note that I removed the military class and smoke detection. This is meant to be a FOSS tool for civilian use and I don\\'t want to pursue making it work for military applications.\\n\\n\\n---------------------------------------------------------------------\\n\\nWHERE IS WALDO?\\n\\nRight here on HF!\\n\\nNote there are a couple more models that have slightly better performance over on Gumroad here: https://6228189440665.gumroad.com/l/WALDOv3\\nThose are for sale as a kind of sponsorship for the project: if you find value in the free ones here you can buy those for a nice little performance boost... but it\\'s entirey up to you! \\n\\n\\n[![P2 model performance boost](https://i.imgur.com/VKa5NN5.png)]\\n\\n\\nIn both cases the actual files are MIT license and you can freely share them, so if someone gives you the ones from Gumroad you are free yo use them including commercially. It\\'s really just a way to offset some of the work and compute that went into making this project and keeping it FOSS.\\n\\n\\n---------------------------------------------------------------------                                                                                                                                                               \\n\\nWHAT IS IT GOOD FOR?\\n\\nPeople are currently using versions of WALDO for:\\n1. disaster recovery\\n2. monitoring wildlife sanctuaries (intruder detection)\\n3. occupancy calculation (parking lots etc..)\\n4. monitoring infrastructure \\n5. construction site monitoring\\n6. traffic flow management\\n7. crowd counting\\n8. some fun AI art applications!\\n9. drone safety (avoiding people / cars on the ground)\\n10. lots of other fun stuff...\\n\\nThe main reason for me to make WALDO free has in fact been discovering all these cool applications. Let me know what you build!\\n\\n---------------------------------------------------------------------                                                                                                                                                               \\n\\nFOR AI NERDS !\\n\\nIt\\'s a set of YOLOv8 model, trained on my own datasets of synthetic and \"augmented\" / semi-synthetic data.\\nI\\'m not going to release the dataset for the time being.\\n\\nThe weights are completely open, allowing you to deploy in any number of ways this time! \\n\\n\\n---------------------------------------------------------------------                                                                                \\n\\nHOW CAN I START WITH WALDO?  \\n\\nCheck out the boilerplate code in the repo to run the models and output pretty detections using the wonderful Supervision annotation library from Roboflow :) \\n\\n---------------------------------------------------------------------\\n\\nGOING DEEPER\\n\\nOf course if you know your way around deploying AI models there is a lot more you do\\nwith this release, inclusing:\\n\\n1. fine-tuning the models on your own data (if you know what you are doing, this is probably your starting point)\\n2. building a nicely optimized sliding-window inference setup that works nicely on your edge hardware\\n3. quantizing the models for super-duper edge performance on cheap devices\\n4. using the models to annotate your own data and train something of your own!\\n\\n\\nEnjoy!\\n\\n---------------------------------------------------------------------\\n\\n\\nPREVIOUS VERSIONS\\n\\nI am retiring the old versions, this is the only one that will stay online.\\n\\n---------------------------------------------------------------------\\n\\n\\nCAN YOU HELP ME WITH X? \\n\\nSure, email me at stephan.sturges@gmail.com\\n\\n\\n---------------------------------------------------------------------\\n\\n\\nDETECTION OF X ISN\\'T WORKING AS EXPECTED:\\n\\nI\\'d love to see example images, videos, sample data, etc at:\\nstephan.sturges@gmail.com\\n\\n\\n---------------------------------------------------------------------\\n\\nSUPPORT WALDO!\\n\\nVisit [![the WALDO gumroad page](https://t.co/kRvhYkVxW2)] to support the project!\\n\\n---------------------------------------------------------------------\\n\\n\\nLICENSE\\n----------------------------------------------------------------------------\\n\\nUnless otherwise specified all code in this release is published with the \\nlicence conditions below.\\n----------------------------------------------------------------------------\\n\\n\\nMIT License\\n\\nCopyright (c) 2024 Stephan Sturges / Aircortex.com \\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=StephanST/WALDO30\n",
      "> RSP [{'title': 'WALDO30 | AI Model Details', 'url': 'https://www.aimodels.fyi/models/huggingFace/waldo30-stephanst', 'content': 'The WALDO30 model is a powerful object detection AI developed by StephanST. It is capable of detecting a wide range of objects in overhead imagery, from low-altitude drone footage to satellite imagery. Compared to similar models like YOLOX and YOLO-World, WALDO30 offers a more specialized and focused set of object detection capabilities tailored for civilian use cases. The WALDO30 model takes in overhead imagery as input and outputs bounding boxes and class labels for the detected objects. The WALDO30 model excels at detecting a diverse range of objects in overhead imagery, from small vehicles and people to large buildings and infrastructure. One interesting thing to try with the WALDO30 model is to experiment with the varying levels of performance across the different object classes.', 'score': 0.9014448, 'raw_content': None}, {'title': 'StephanST/WALDO30 - Hugging Face', 'url': 'https://huggingface.co/StephanST/WALDO30', 'content': 'Models WALDO is a detection AI model, based on a large YOLO-v8 backbone and my own synthetic data pipeline. Note there are a couple more models that have slightly better performance over on Gumroad here: https://6228189440665.gumroad.com/l/WALDOv3 Those are for sale as a kind of sponsorship for the project: if you find value in the free ones here you can buy those for a nice little performance boost... Check out the boilerplate code in the repo to run the models and output pretty detections using the wonderful Supervision annotation library from Roboflow :) Of course if you know your way around deploying AI models there is a lot more you do with this release, inclusing: Downloads are not tracked for this model. Model tree for StephanST/WALDO30 this model Models Datasets Spaces Pricing Docs', 'score': 0.7216064, 'raw_content': None}, {'title': 'StephanST/WALDO30 · Gumroad Link - Hugging Face', 'url': 'https://huggingface.co/StephanST/WALDO30/discussions/1', 'content': 'StephanST/WALDO30 · Gumroad Link Models Model card Files Files and versions Community 2 Gumroad Link Hi there, Is there a link for those? Also, are the free versions available for commercial use under your MIT license? If there any links to the data that these models were trained on? Thanks, fixed the link. All the versions both here and on gumroad are MIT and free to use commercially. Hi Stephan These models were trained with grayscale augmentation already (all of them). StephanST changed discussion status to closed Oct 28, 2024 Upload images, audio, and videos by dragging in the text input, pasting, or clicking here. Comment· Sign up or log in to comment Models Datasets Spaces Pricing Docs', 'score': 0.5677694, 'raw_content': None}, {'title': 'StephanST/WALDO30 at main - Hugging Face', 'url': 'https://huggingface.co/StephanST/WALDO30/tree/main', 'content': '1.52 kBinitial commit 3 months ago 7.4 kBUpdate README.md 3 months ago \"ultralytics.nn.modules.head.Detect\", \"torch.nn.modules.batchnorm.BatchNorm2d\", \"torch.nn.modules.container.Sequential\", \"torch.nn.modules.activation.SiLU\", \"torch.nn.modules.upsampling.Upsample\", \"torch.nn.modules.pooling.MaxPool2d\", \"torch.nn.modules.container.ModuleList\", \"torch.nn.modules.conv.Conv2d\", \"torch.nn.modules.loss.BCEWithLogitsLoss\" 52.1 MB LFSUpload 3 files 3 months ago 440 kBUpload 6 files 3 months ago 251 kBUpload 6 files 3 months ago \"torch.nn.modules.upsampling.Upsample\", \"torch.nn.modules.container.ModuleList\", \"ultralytics.nn.modules.head.Detect\", \"torch.nn.modules.pooling.MaxPool2d\", \"torch.nn.modules.conv.Conv2d\", \"torch.nn.modules.loss.BCEWithLogitsLoss\", \"torch.nn.modules.container.Sequential\", \"torch.nn.modules.activation.SiLU\", \"torch.nn.modules.batchnorm.BatchNorm2d\" 6.25 MB LFSUpload 3 files 3 months ago 389 kBUpload 6 files 3 months ago 246 kBUpload 6 files 3 months ago \"torch.nn.modules.upsampling.Upsample\", \"torch.nn.modules.container.ModuleList\", \"ultralytics.nn.modules.head.Detect\", \"torch.nn.modules.pooling.MaxPool2d\", \"torch.nn.modules.conv.Conv2d\", \"torch.nn.modules.loss.BCEWithLogitsLoss\", \"torch.nn.modules.container.Sequential\", \"torch.nn.modules.activation.SiLU\", 6.29 MB LFSUpload 3 files 3 months ago 258 kBUpload 6 files 3 months ago *   WALDO30_yolov8n_640x640_confusion_matrix_normalized_F1_curve.png 431 kBUpload 6 files 3 months ago 3.68 kBUpload 2 files 3 months ago 4.83 kBUpload 2 files 3 months ago', 'score': 0.430507, 'raw_content': None}, {'title': 'Waldo30 - use with Halio - General - Hailo Community', 'url': 'https://community.hailo.ai/t/waldo30-use-with-halio/4832', 'content': 'Waldo30 - use with Halio - General - Hailo Community Hailo.ai Topics hailo8 Waldo30 - use with Halio You have selected 0 posts. select all cancel selecting Would someone be able to tell me if and how i can use waldo30 with the basic piplines on a RPI Ai board. Hi @ewan, welcome to the Hailo Community! Related topics | Topic | Replies | Views | Activity | Open source course for AI kit and Pi | 17 | 1.6k | Nov 2024 | Yolov5s_personface for Hailo 8L | 0 | 166 | Nov 2024 | Getting started with running pretrained models on Hailo 8 Beginner in Embedded AI Development - Vehicle Detection with Raspberry Pi 5 + Hailo Kit', 'score': 0.26699454, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=StephanST/WALDO30\n",
      "> RSP [{'title': 'JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust ...', 'url': 'https://arxiv.org/abs/2412.07523', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. arXiv:2412.07523 Help | Advanced Search arXiv author ID Help pages We present near- and mid-infrared (IR) broadband imaging observations of the edge-on protoplanetary disk around HH 30 with the James Webb Space Telescope/Near Infrared Camera (NIRCam) and the Mid-Infrared Instrument (MIRI). In the near- and mid-IR, the images capture not only bi-reflection nebulae separated by a dark lane but also diverse dynamical processes occurring in the HH 30 disk, such as spiral- and tail-like structures, a conical outflow, and a collimated jet. Cite as:    arXiv:2412.07523 [astro-ph.EP] (or arXiv:2412.07523v1 [astro-ph.EP] for this version) From: Ryo Tazaki [view email] Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Help\", 'score': 0.08679465, 'raw_content': None}, {'title': 'Strong-lensing cosmography using third-generation gravitational-wave ...', 'url': 'https://arxiv.org/html/2405.17805v2', 'content': 'GW observations of compact binary mergers will allow us to measure the luminosity distance d L subscript 𝑑 𝐿 d_{L} italic_d start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT accurately without using any distance ladders, as these objects are absolutely calibrated standard sirens [4, 5].An electromagnetic (EM) counterpart of the merger will enable us to measure their redshifts, thus allowing', 'score': 0.048590824, 'raw_content': None}, {'title': 'Applications of machine learning in gravitational wave research with ...', 'url': 'https://arxiv.org/html/2412.15046v1', 'content': 'Alhassan et\\xa0al [2023]   Alhassan W, Bulik T, Suchenek M (2023) Einstein Telescope: binary black holes gravitational wave signals detection from three detectors combined data using deep learning. Chan et\\xa0al [2020]   Chan ML, Heng IS, Messenger C (2020) Detection and classification of supernova gravitational wave signals: A deep learning approach. Cuoco et\\xa0al [2017]  Cuoco E, Powell J, Torres-Forné A, et\\xa0al (2017) Strategy for signal classification to improve data quality for Advanced Detectors gravitational-wave searches. Jarov et\\xa0al [2023]  Jarov S, Thiele S, Soni S, et\\xa0al (2023) A new method to distinguish gravitational-wave signals from detector noise transients with Gravity Spy. arXiv e-prints arXiv:2307.15867 [gr-qc]', 'score': 0.03421971, 'raw_content': None}, {'title': 'Kernel Methods for Interferometric Imaging - arXiv.org', 'url': 'https://arxiv.org/html/2412.01908v1', 'content': 'This fact allows us to go a step further in detaching the imaging algorithm from any particular training set and instead use kernel regression to effectively reconstruct the Fourier maps between the observed data points (Rasmussen & Williams, 2006).Kernel methods have the combined advantage of fitting the data with an infinite and complete set of functions and, at the same time, depend only on', 'score': 0.026925152, 'raw_content': None}, {'title': 'Dark Matter (H)eats Young Planets - arXiv.org', 'url': 'https://arxiv.org/html/2309.02495v3', 'content': 'By investigating the formation process of Jovian planets we argue that an additional heat source associated with DM annihilation could halt the formation process at various stages, depending on the DM parameters, such as mass, cross section, and ambient number density. For small DM scattering cross sections, core capture dominates, preventing any gas accumulation for Jovian planets at some distance from the GC. For larger DM cross sections, envelope capture dominates, leading to the formation of Jovian planets with a decreasing mass the closer to the GC they are formed. Moreover, as we explained in the previous section, for a particular DM (sub-component) mass and cross section the formation of Jovian planets is affected as a function of the distance from the GC, enabling a differential detection of the heating signal.', 'score': 0.02590068, 'raw_content': None}, {'title': 'MSA-3D: Metallicity Gradients in Galaxies at z ∼ 1 with JWST/NIRSpec ...', 'url': 'https://arxiv.org/html/2409.01616v3', 'content': 'In this model, metals can be redistributed by various physical processes, including stellar feedback and rotation, leading to flat gradients which can be observed in isolated high-redshift galaxies (Yuan et\\xa0al., 2011; Jones et\\xa0al., 2013). Observations of high-redshift galaxies have found that a larger fraction of these galaxies exhibit flat or inverted metallicity gradients compared to local galaxies (Yuan et\\xa0al., 2011; Jones et\\xa0al., 2013; Wuyts et\\xa0al., 2016; Leethochawalit et\\xa0al., 2016; Wang et\\xa0al., 2017, 2019, 2022b; Simons et\\xa0al., 2021; Cheng et\\xa0al., 2024). Therefore, we can observe the changes in the metallicity gradient from small to high-mass galaxies as representing different stages in their growth (Hemler et\\xa0al., 2021).', 'score': 0.023909599, 'raw_content': None}, {'title': 'A Broad-line, Low-luminosity Active Galactic Nucleus at z 73 Anchoring ...', 'url': 'https://arxiv.org/pdf/2411.11534', 'content': 'James Webb Space Telescope (JWST) spectroscopy has confirmed dozens of (type-1) active galactic nuclei (AGN) by detection of a broad (FWHM>1000kms−1) emission-line component to the Hα (or Hβ) line1-5,12-14, characteristic of gas motion in the gravitational field of a supermassive black hole (SMBH). A particularly intriguing subclass of these broad-line AGN (∼20%) appear as compact', 'score': 0.022579623, 'raw_content': None}, {'title': 'A Multi-Wavelength Technique for Estimating Galaxy Cluster Mass ...', 'url': 'https://arxiv.org/html/2412.05370v1', 'content': 'More direct estimates include using constituent galaxies’ spectroscopy (e.g., Pizzardo et\\xa0al., 2023), using a classifying machine learning (ML) model to identify clusters with relaxed or merging dynamical state (Arendt et\\xa0al., 2024), or connecting mass accretion rates directly to halo and galaxy properties (Mendoza et\\xa0al., 2023). Our model, utilizing a form of neural network known as a “Normalizing Flow” (Papamakarios et\\xa0al., 2019), provides an accurate estimate of the probability distribution of potential MARs for a given galaxy cluster. The ML model is therefore underexposed to this region of parameter space and less effective at estimating these MARs. Nevertheless, even here, the model still outperforms the fitting function, which struggles to accurately estimate high MARs. It is worth noting that tracking progenitors along merger trees can sometimes lead to discontinuities in the mass history, occasionally leading to erroneously extreme MAR (Srisawat et\\xa0al., 2013, see discussion in section 6).', 'score': 0.022424964, 'raw_content': None}, {'title': 'Computer Science authors/titles recent submissions (1312 skipped)', 'url': 'http://export.arxiv.org/list/cs/pastweek?skip=1312&show=1636', 'content': 'Title: IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With an End-to-End Analog Charge-Based .15-8POPS/W Macro Featuring Distribution-Aware Data Reshaping', 'score': 0.022322435, 'raw_content': None}, {'title': 'Computer Science authors/titles \"new\"', 'url': 'http://export.arxiv.org/list/cs/new?skip=0&show=721', 'content': 'Big data visualization - the visual-spatial display of quantitative information culled from huge data sets - is now firmly embedded within the everyday experiences of people across the globe, yet scholarship on it remains surprisingly small.', 'score': 0.021816617, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2309.02495\n",
      "> RSP [Dark Matter (H)eats Young Planets](http://arxiv.org/abs/2309.02495v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.05370\n",
      "> RSP [A Multi-Wavelength Technique for Estimating Galaxy Cluster Mass Accretion Rates](http://arxiv.org/abs/2412.05370v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.01908\n",
      "> RSP [Kernel Methods for Interferometric Imaging](http://arxiv.org/abs/2412.01908v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2405.17805\n",
      "> RSP [Strong-lensing cosmography using third-generation gravitational-wave detectors](http://arxiv.org/abs/2405.17805v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.11534\n",
      "> RSP [A Broad-line, Low-luminosity Active Galactic Nucleus at ${z=7.3}$ Anchoring a Large Galaxy Overdensity](http://arxiv.org/abs/2411.11534v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.15046\n",
      "> RSP [Applications of machine learning in gravitational wave research with current interferometric detectors](http://arxiv.org/abs/2412.15046v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2409.01616\n",
      "> RSP [MSA-3D: Metallicity Gradients in Galaxies at $z\\sim1$ with JWST/NIRSpec Slit-stepping Spectroscopy](http://arxiv.org/abs/2409.01616v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.07523\n",
      "> RSP [JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust Scattering in the HH 30 disk](http://arxiv.org/abs/2412.07523v1)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'WALDO30 | AI Model Details', 'url': 'https://www.aimodels.fyi/models/huggingFace/waldo30-stephanst', 'content': 'The WALDO30 model is a powerful object detection AI developed by StephanST. It is capable of detecting a wide range of objects in overhead imagery, from low-altitude drone footage to satellite imagery. Compared to similar models like YOLOX and YOLO-World, WALDO30 offers a more specialized and focused set of object detection capabilities tailored for civilian use cases. The WALDO30 model takes in overhead imagery as input and outputs bounding boxes and class labels for the detected objects. The WALDO30 model excels at detecting a diverse range of objects in overhead imagery, from small vehicles and people to large buildings and infrastructure. One interesting thing to try with the WALDO30 model is to experiment with the varying levels of performance across the different object classes.', 'score': 0.9014448, 'raw_content': None}, {'title': 'StephanST/WALDO30 - Hugging Face', 'url': 'https://huggingface.co/StephanST/WALDO30', 'content': 'Models WALDO is a detection AI model, based on a large YOLO-v8 backbone and my own synthetic data pipeline. Note there are a couple more models that have slightly better performance over on Gumroad here: https://6228189440665.gumroad.com/l/WALDOv3 Those are for sale as a kind of sponsorship for the project: if you find value in the free ones here you can buy those for a nice little performance boost... Check out the boilerplate code in the repo to run the models and output pretty detections using the wonderful Supervision annotation library from Roboflow :) Of course if you know your way around deploying AI models there is a lot more you do with this release, inclusing: Downloads are not tracked for this model. Model tree for StephanST/WALDO30 this model Models Datasets Spaces Pricing Docs', 'score': 0.7216064, 'raw_content': None}, {'title': 'StephanST/WALDO30 · Gumroad Link - Hugging Face', 'url': 'https://huggingface.co/StephanST/WALDO30/discussions/1', 'content': 'StephanST/WALDO30 · Gumroad Link Models Model card Files Files and versions Community 2 Gumroad Link Hi there, Is there a link for those? Also, are the free versions available for commercial use under your MIT license? If there any links to the data that these models were trained on? Thanks, fixed the link. All the versions both here and on gumroad are MIT and free to use commercially. Hi Stephan These models were trained with grayscale augmentation already (all of them). StephanST changed discussion status to closed Oct 28, 2024 Upload images, audio, and videos by dragging in the text input, pasting, or clicking here. Comment· Sign up or log in to comment Models Datasets Spaces Pricing Docs', 'score': 0.5677694, 'raw_content': None}, {'title': 'StephanST/WALDO30 at main - Hugging Face', 'url': 'https://huggingface.co/StephanST/WALDO30/tree/main', 'content': '1.52 kBinitial commit 3 months ago 7.4 kBUpdate README.md 3 months ago \"ultralytics.nn.modules.head.Detect\", \"torch.nn.modules.batchnorm.BatchNorm2d\", \"torch.nn.modules.container.Sequential\", \"torch.nn.modules.activation.SiLU\", \"torch.nn.modules.upsampling.Upsample\", \"torch.nn.modules.pooling.MaxPool2d\", \"torch.nn.modules.container.ModuleList\", \"torch.nn.modules.conv.Conv2d\", \"torch.nn.modules.loss.BCEWithLogitsLoss\" 52.1 MB LFSUpload 3 files 3 months ago 440 kBUpload 6 files 3 months ago 251 kBUpload 6 files 3 months ago \"torch.nn.modules.upsampling.Upsample\", \"torch.nn.modules.container.ModuleList\", \"ultralytics.nn.modules.head.Detect\", \"torch.nn.modules.pooling.MaxPool2d\", \"torch.nn.modules.conv.Conv2d\", \"torch.nn.modules.loss.BCEWithLogitsLoss\", \"torch.nn.modules.container.Sequential\", \"torch.nn.modules.activation.SiLU\", \"torch.nn.modules.batchnorm.BatchNorm2d\" 6.25 MB LFSUpload 3 files 3 months ago 389 kBUpload 6 files 3 months ago 246 kBUpload 6 files 3 months ago \"torch.nn.modules.upsampling.Upsample\", \"torch.nn.modules.container.ModuleList\", \"ultralytics.nn.modules.head.Detect\", \"torch.nn.modules.pooling.MaxPool2d\", \"torch.nn.modules.conv.Conv2d\", \"torch.nn.modules.loss.BCEWithLogitsLoss\", \"torch.nn.modules.container.Sequential\", \"torch.nn.modules.activation.SiLU\", 6.29 MB LFSUpload 3 files 3 months ago 258 kBUpload 6 files 3 months ago *   WALDO30_yolov8n_640x640_confusion_matrix_normalized_F1_curve.png 431 kBUpload 6 files 3 months ago 3.68 kBUpload 2 files 3 months ago 4.83 kBUpload 2 files 3 months ago', 'score': 0.430507, 'raw_content': None}, {'title': 'Waldo30 - use with Halio - General - Hailo Community', 'url': 'https://community.hailo.ai/t/waldo30-use-with-halio/4832', 'content': 'Waldo30 - use with Halio - General - Hailo Community Hailo.ai Topics hailo8 Waldo30 - use with Halio You have selected 0 posts. select all cancel selecting Would someone be able to tell me if and how i can use waldo30 with the basic piplines on a RPI Ai board. Hi @ewan, welcome to the Hailo Community! Related topics | Topic | Replies | Views | Activity | Open source course for AI kit and Pi | 17 | 1.6k | Nov 2024 | Yolov5s_personface for Hailo 8L | 0 | 166 | Nov 2024 | Getting started with running pretrained models on Hailo 8 Beginner in Embedded AI Development - Vehicle Detection with Raspberry Pi 5 + Hailo Kit', 'score': 0.26699454, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "license: mit\n",
      "language:\n",
      "- en\n",
      "base_model:\n",
      "- Ultralytics/YOLOv8\n",
      "pipeline_tag: object-detection\n",
      "W.A.L.D.O.\n",
      "Whereabouts Ascertainment for Low-lying Detectable Objects\n",
      "Welcome to the WALDO v3.0 public release\n",
      "WHAT IS WALDO?\n",
      "WALDO is a detection AI model, based on a large YOLO-v8 backbone and my own\n",
      "synthetic data pipeline.\n",
      "The model is capable of detecting these classes \n",
      "of items in overhead imagery ranging in altitude from about 30 feet to \n",
      "satellite imagery!\n",
      "Output classes:\n",
      "0 -> 'LightVehicle'  --> all kinds of civilan cars, including pickup trucks, vans etc... 🚗🏎️🚓🚐🚑\n",
      "1 -> 'Person' --> people! all kinds of people including ones that are on bikes or swimming in the sea 🧍‍♀️🕺💃🧜🏽‍♀️🏂🧞\n",
      "2 -> 'Building' --> all kinds of buildings 🕌🏛️🏭🏡\n",
      "3 -> 'UPole' --> utility poles, power poles, anything thin and sticking up that you should avoid with a drone 🎏\n",
      "4 -> 'Boat' --> boats, ships, canoes, kayaks, surf boards... all the floaty stuff 🚢🏄\n",
      "5 -> 'Bike' --> bikes, mopeds, motorbikes, all stuff with 2 wheels 🚲\n",
      "6 -> 'Container' --> shipping containers, including on the back of an articulated truck... 📦🏗️\n",
      "7 -> 'Truck' --> large commercial vehicles including articulated trucks or big box-on-chassis delivery trucks 🚚\n",
      "8 -> 'Gastank'--> cylindrical tanks such as butane tanks and gas expansion tanks, or grain silos... pretty much anything that looks cylindrical for storing liquids 🫙\n",
      "10 -> 'Digger' --> all kinds of construction vehicles, including tractors and construction gear 🚜\n",
      "11 -> 'Solarpanels' --> solar panels ▪️🌞▪️\n",
      "12 -> 'Bus' --> a bus 🚌\n",
      "--> In general the lower the class number the better-trained you can expect it to be.\n",
      "For users of previous versions of WALDO: note that I removed the military class and smoke detection. This is meant to be a FOSS tool for civilian use and I don't want to pursue making it work for military applications.\n",
      "WHERE IS WALDO?\n",
      "Right here on HF!\n",
      "Note there are a couple more models that have slightly better performance over on Gumroad here: https://6228189440665.gumroad.com/l/WALDOv3\n",
      "Those are for sale as a kind of sponsorship for the project: if you find value in the free ones here you can buy those for a nice little performance boost... but it's entirey up to you!\n",
      "[\n",
      "]\n",
      "In both cases the actual files are MIT license and you can freely share them, so if someone gives you the ones from Gumroad you are free yo use them including commercially. It's really just a way to offset some of the work and compute that went into making this project and keeping it FOSS.\n",
      "WHAT IS IT GOOD FOR?\n",
      "People are currently using versions of WALDO for:\n",
      "1. disaster recovery\n",
      "2. monitoring wildlife sanctuaries (intruder detection)\n",
      "3. occupancy calculation (parking lots etc..)\n",
      "4. monitoring infrastructure \n",
      "5. construction site monitoring\n",
      "6. traffic flow management\n",
      "7. crowd counting\n",
      "8. some fun AI art applications!\n",
      "9. drone safety (avoiding people / cars on the ground)\n",
      "10. lots of other fun stuff...\n",
      "The main reason for me to make WALDO free has in fact been discovering all these cool applications. Let me know what you build!\n",
      "FOR AI NERDS !\n",
      "It's a set of YOLOv8 model, trained on my own datasets of synthetic and \"augmented\" / semi-synthetic data.\n",
      "I'm not going to release the dataset for the time being.\n",
      "The weights are completely open, allowing you to deploy in any number of ways this time!\n",
      "HOW CAN I START WITH WALDO?\n",
      "Check out the boilerplate code in the repo to run the models and output pretty detections using the wonderful Supervision annotation library from Roboflow :)\n",
      "GOING DEEPER\n",
      "Of course if you know your way around deploying AI models there is a lot more you do\n",
      "with this release, inclusing:\n",
      "fine-tuning the models on your own data (if you know what you are doing, this is probably your starting point)\n",
      "building a nicely optimized sliding-window inference setup that works nicely on your edge hardware\n",
      "quantizing the models for super-duper edge performance on cheap devices\n",
      "using the models to annotate your own data and train something of your own!\n",
      "Enjoy!\n",
      "PREVIOUS VERSIONS\n",
      "I am retiring the old versions, this is the only one that will stay online.\n",
      "CAN YOU HELP ME WITH X?\n",
      "Sure, email me at stephan.sturges@gmail.com\n",
      "DETECTION OF X ISN'T WORKING AS EXPECTED:\n",
      "I'd love to see example images, videos, sample data, etc at:\n",
      "stephan.sturges@gmail.com\n",
      "SUPPORT WALDO!\n",
      "Visit [\n",
      "] to support the project!\n",
      "LICENSE\n",
      "Unless otherwise specified all code in this release is published with the \n",
      "licence conditions below.\n",
      "MIT License\n",
      "Copyright (c) 2024 Stephan Sturges / Aircortex.com\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "> RSP * Specialized for civilian use cases in overhead imagery\n",
      "* Based on a large YOLO-v8 backbone architecture\n",
      "* Capable of detecting a wide variety of object classes\n",
      "* Focused on low-altitude to satellite imagery detection\n",
      "* Supports both synthetic and augmented training data\n",
      "* Open weights for flexible deployment in various applications\n",
      "* Provides outputs of bounding boxes and class labels\n",
      "* Ideal for diverse applications like disaster recovery and wildlife monitoring\n",
      "* Free and open-source under MIT license\n",
      "* User-friendly with boilerplate code and adaptation options for fine-tuning\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=nomic-ai/modernbert-embed-base\n",
      "> RSP {'model_id': 'nomic-ai/modernbert-embed-base', 'created_at': '29 December 2024 at 23:51:30 UTC', 'downloads': 4837, 'likes': 135, 'trending_score': 89, 'description': '---\\npipeline_tag: sentence-similarity\\ntags:\\n- sentence-transformers\\n- feature-extraction\\n- sentence-similarity\\n- mteb\\n- transformers.js\\nmodel-index:\\n- name: binarize_False\\n  results:\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB AmazonCounterfactualClassification (en)\\n      config: en\\n      split: test\\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\\n    metrics:\\n    - type: accuracy\\n      value: 78.13432835820896\\n    - type: ap\\n      value: 42.190424731303246\\n    - type: f1\\n      value: 72.34446401534811\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB AmazonPolarityClassification\\n      config: default\\n      split: test\\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\\n    metrics:\\n    - type: accuracy\\n      value: 93.093825\\n    - type: ap\\n      value: 90.03727505544286\\n    - type: f1\\n      value: 93.0874055138833\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB AmazonReviewsClassification (en)\\n      config: en\\n      split: test\\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\\n    metrics:\\n    - type: accuracy\\n      value: 48.428000000000004\\n    - type: f1\\n      value: 47.74311520203536\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB ArguAna\\n      config: default\\n      split: test\\n      revision: c22ab2a51041ffd869aaddef7af8d8215647e41a\\n    metrics:\\n    - type: map_at_1\\n      value: 23.898\\n    - type: map_at_10\\n      value: 39.775\\n    - type: map_at_100\\n      value: 40.827000000000005\\n    - type: map_at_1000\\n      value: 40.837\\n    - type: map_at_20\\n      value: 40.604\\n    - type: map_at_3\\n      value: 34.519\\n    - type: map_at_5\\n      value: 37.307\\n    - type: mrr_at_1\\n      value: 24.395\\n    - type: mrr_at_10\\n      value: 39.963\\n    - type: mrr_at_100\\n      value: 41.014\\n    - type: mrr_at_1000\\n      value: 41.024\\n    - type: mrr_at_20\\n      value: 40.791\\n    - type: mrr_at_3\\n      value: 34.732\\n    - type: mrr_at_5\\n      value: 37.480999999999995\\n    - type: ndcg_at_1\\n      value: 23.898\\n    - type: ndcg_at_10\\n      value: 48.962\\n    - type: ndcg_at_100\\n      value: 53.386\\n    - type: ndcg_at_1000\\n      value: 53.634\\n    - type: ndcg_at_20\\n      value: 51.898999999999994\\n    - type: ndcg_at_3\\n      value: 38.034\\n    - type: ndcg_at_5\\n      value: 43.036\\n    - type: precision_at_1\\n      value: 23.898\\n    - type: precision_at_10\\n      value: 7.852\\n    - type: precision_at_100\\n      value: 0.9769999999999999\\n    - type: precision_at_1000\\n      value: 0.1\\n    - type: precision_at_20\\n      value: 4.4990000000000006\\n    - type: precision_at_3\\n      value: 16.073999999999998\\n    - type: precision_at_5\\n      value: 12.063\\n    - type: recall_at_1\\n      value: 23.898\\n    - type: recall_at_10\\n      value: 78.521\\n    - type: recall_at_100\\n      value: 97.724\\n    - type: recall_at_1000\\n      value: 99.644\\n    - type: recall_at_20\\n      value: 89.972\\n    - type: recall_at_3\\n      value: 48.222\\n    - type: recall_at_5\\n      value: 60.313\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB ArxivClusteringP2P\\n      config: default\\n      split: test\\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\\n    metrics:\\n    - type: v_measure\\n      value: 47.69067314293749\\n    - type: v_measures\\n      value: [0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413]\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB ArxivClusteringS2S\\n      config: default\\n      split: test\\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\\n    metrics:\\n    - type: v_measure\\n      value: 38.0916537995626\\n    - type: v_measures\\n      value: [0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377, 0.37814352051854533, 0.39235658929084877, 0.3871170834588581, 0.4042678213739614, 0.3918486409557737, 0.38473003463452093, 0.35622070034791886, 0.3911472272128115, 0.3986923912337426, 0.39040109467533013, 0.4370949482641744, 0.4414023630938724, 0.4351473848532441, 0.4401176389499172, 0.4423731097742471, 0.438309696145818, 0.43410597641884624, 0.43900908630646696, 0.44081346534023286, 0.4386000014888906, 0.4047539306032343, 0.21697191913450847, 0.29241358200068185, 0.3390740154458194, 0.2793967439904601, 0.20383792346854981, 0.23904022437429004, 0.14733601126565044, 0.22946888289524586, 1.0, 0.19422067034794377]\\n  - task:\\n      type: Reranking\\n    dataset:\\n      type: None\\n      name: MTEB AskUbuntuDupQuestions\\n      config: default\\n      split: test\\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\\n    metrics:\\n    - type: map\\n      value: 62.33195643912506\\n    - type: mrr\\n      value: 76.43978366970057\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB BIOSSES\\n      config: default\\n      split: test\\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 81.20285894915236\\n    - type: cos_sim_spearman\\n      value: 78.16322678527897\\n    - type: euclidean_pearson\\n      value: 80.6118408638417\\n    - type: euclidean_spearman\\n      value: 78.19033583671204\\n    - type: manhattan_pearson\\n      value: 80.41282660275819\\n    - type: manhattan_spearman\\n      value: 77.98611431591628\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB Banking77Classification\\n      config: default\\n      split: test\\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\\n    metrics:\\n    - type: accuracy\\n      value: 85.25324675324676\\n    - type: f1\\n      value: 85.19854235582687\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB BiorxivClusteringP2P\\n      config: default\\n      split: test\\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\\n    metrics:\\n    - type: v_measure\\n      value: 39.65216461057432\\n    - type: v_measures\\n      value: [0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885, 0.409550367831406, 0.3943451642663655, 0.38843873187080014, 0.40032616646112934, 0.3956833025503425, 0.3842865397042604, 0.3950585966936957, 0.41669832667987455, 0.39790986378306964, 0.3829194012164885]\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB BiorxivClusteringS2S\\n      config: default\\n      split: test\\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\\n    metrics:\\n    - type: v_measure\\n      value: 33.28787287895752\\n    - type: v_measures\\n      value: [0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306, 0.3235019092705102, 0.34053753555843735, 0.32485572754337366, 0.3149662563474906, 0.3326837187664875, 0.3229632335470733, 0.33078383561261365, 0.35111148393509534, 0.33383133843449825, 0.35355224888017306]\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackAndroidRetrieval\\n      config: default\\n      split: test\\n      revision: f46a197baaae43b4f621051089b82a364682dfeb\\n    metrics:\\n    - type: map_at_1\\n      value: 32.677\\n    - type: map_at_10\\n      value: 43.739\\n    - type: map_at_100\\n      value: 45.152\\n    - type: map_at_1000\\n      value: 45.279\\n    - type: map_at_20\\n      value: 44.553\\n    - type: map_at_3\\n      value: 40.321\\n    - type: map_at_5\\n      value: 42.201\\n    - type: mrr_at_1\\n      value: 40.2\\n    - type: mrr_at_10\\n      value: 49.755\\n    - type: mrr_at_100\\n      value: 50.468\\n    - type: mrr_at_1000\\n      value: 50.513\\n    - type: mrr_at_20\\n      value: 50.192\\n    - type: mrr_at_3\\n      value: 47.163\\n    - type: mrr_at_5\\n      value: 48.686\\n    - type: ndcg_at_1\\n      value: 40.2\\n    - type: ndcg_at_10\\n      value: 49.963\\n    - type: ndcg_at_100\\n      value: 54.978\\n    - type: ndcg_at_1000\\n      value: 56.979\\n    - type: ndcg_at_20\\n      value: 51.983000000000004\\n    - type: ndcg_at_3\\n      value: 45.086999999999996\\n    - type: ndcg_at_5\\n      value: 47.309\\n    - type: precision_at_1\\n      value: 40.2\\n    - type: precision_at_10\\n      value: 9.328\\n    - type: precision_at_100\\n      value: 1.443\\n    - type: precision_at_1000\\n      value: 0.19\\n    - type: precision_at_20\\n      value: 5.558\\n    - type: precision_at_3\\n      value: 21.364\\n    - type: precision_at_5\\n      value: 15.222\\n    - type: recall_at_1\\n      value: 32.677\\n    - type: recall_at_10\\n      value: 61.71\\n    - type: recall_at_100\\n      value: 82.431\\n    - type: recall_at_1000\\n      value: 94.896\\n    - type: recall_at_20\\n      value: 68.73700000000001\\n    - type: recall_at_3\\n      value: 47.431\\n    - type: recall_at_5\\n      value: 53.739000000000004\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackEnglishRetrieval\\n      config: default\\n      split: test\\n      revision: ad9991cb51e31e31e430383c75ffb2885547b5f0\\n    metrics:\\n    - type: map_at_1\\n      value: 32.71\\n    - type: map_at_10\\n      value: 43.297000000000004\\n    - type: map_at_100\\n      value: 44.607\\n    - type: map_at_1000\\n      value: 44.729\\n    - type: map_at_20\\n      value: 44.013999999999996\\n    - type: map_at_3\\n      value: 40.213\\n    - type: map_at_5\\n      value: 42.004000000000005\\n    - type: mrr_at_1\\n      value: 40.892\\n    - type: mrr_at_10\\n      value: 49.394\\n    - type: mrr_at_100\\n      value: 50.005\\n    - type: mrr_at_1000\\n      value: 50.043000000000006\\n    - type: mrr_at_20\\n      value: 49.764\\n    - type: mrr_at_3\\n      value: 47.134\\n    - type: mrr_at_5\\n      value: 48.522\\n    - type: ndcg_at_1\\n      value: 40.892\\n    - type: ndcg_at_10\\n      value: 49.047000000000004\\n    - type: ndcg_at_100\\n      value: 53.266999999999996\\n    - type: ndcg_at_1000\\n      value: 55.096999999999994\\n    - type: ndcg_at_20\\n      value: 50.707\\n    - type: ndcg_at_3\\n      value: 44.896\\n    - type: ndcg_at_5\\n      value: 46.983000000000004\\n    - type: precision_at_1\\n      value: 40.892\\n    - type: precision_at_10\\n      value: 9.293\\n    - type: precision_at_100\\n      value: 1.473\\n    - type: precision_at_1000\\n      value: 0.192\\n    - type: precision_at_20\\n      value: 5.446\\n    - type: precision_at_3\\n      value: 21.592\\n    - type: precision_at_5\\n      value: 15.540999999999999\\n    - type: recall_at_1\\n      value: 32.71\\n    - type: recall_at_10\\n      value: 58.592999999999996\\n    - type: recall_at_100\\n      value: 76.242\\n    - type: recall_at_1000\\n      value: 87.717\\n    - type: recall_at_20\\n      value: 64.646\\n    - type: recall_at_3\\n      value: 46.253\\n    - type: recall_at_5\\n      value: 51.946999999999996\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackGamingRetrieval\\n      config: default\\n      split: test\\n      revision: 4885aa143210c98657558c04aaf3dc47cfb54340\\n    metrics:\\n    - type: map_at_1\\n      value: 41.644999999999996\\n    - type: map_at_10\\n      value: 53.825\\n    - type: map_at_100\\n      value: 54.82\\n    - type: map_at_1000\\n      value: 54.87499999999999\\n    - type: map_at_20\\n      value: 54.43\\n    - type: map_at_3\\n      value: 50.705\\n    - type: map_at_5\\n      value: 52.501\\n    - type: mrr_at_1\\n      value: 47.524\\n    - type: mrr_at_10\\n      value: 57.260999999999996\\n    - type: mrr_at_100\\n      value: 57.902\\n    - type: mrr_at_1000\\n      value: 57.931999999999995\\n    - type: mrr_at_20\\n      value: 57.689\\n    - type: mrr_at_3\\n      value: 55.089\\n    - type: mrr_at_5\\n      value: 56.38999999999999\\n    - type: ndcg_at_1\\n      value: 47.524\\n    - type: ndcg_at_10\\n      value: 59.41499999999999\\n    - type: ndcg_at_100\\n      value: 63.258\\n    - type: ndcg_at_1000\\n      value: 64.376\\n    - type: ndcg_at_20\\n      value: 61.149\\n    - type: ndcg_at_3\\n      value: 54.381\\n    - type: ndcg_at_5\\n      value: 56.89999999999999\\n    - type: precision_at_1\\n      value: 47.524\\n    - type: precision_at_10\\n      value: 9.386\\n    - type: precision_at_100\\n      value: 1.221\\n    - type: precision_at_1000\\n      value: 0.136\\n    - type: precision_at_20\\n      value: 5.223\\n    - type: precision_at_3\\n      value: 24.096\\n    - type: precision_at_5\\n      value: 16.364\\n    - type: recall_at_1\\n      value: 41.644999999999996\\n    - type: recall_at_10\\n      value: 72.386\\n    - type: recall_at_100\\n      value: 88.794\\n    - type: recall_at_1000\\n      value: 96.75399999999999\\n    - type: recall_at_20\\n      value: 78.74\\n    - type: recall_at_3\\n      value: 59.028000000000006\\n    - type: recall_at_5\\n      value: 65.197\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackGisRetrieval\\n      config: default\\n      split: test\\n      revision: 5003b3064772da1887988e05400cf3806fe491f2\\n    metrics:\\n    - type: map_at_1\\n      value: 28.648\\n    - type: map_at_10\\n      value: 36.388999999999996\\n    - type: map_at_100\\n      value: 37.372\\n    - type: map_at_1000\\n      value: 37.457\\n    - type: map_at_20\\n      value: 36.912\\n    - type: map_at_3\\n      value: 34.076\\n    - type: map_at_5\\n      value: 35.415\\n    - type: mrr_at_1\\n      value: 30.508000000000003\\n    - type: mrr_at_10\\n      value: 38.132\\n    - type: mrr_at_100\\n      value: 39.04\\n    - type: mrr_at_1000\\n      value: 39.106\\n    - type: mrr_at_20\\n      value: 38.643\\n    - type: mrr_at_3\\n      value: 35.876000000000005\\n    - type: mrr_at_5\\n      value: 37.208999999999996\\n    - type: ndcg_at_1\\n      value: 30.508000000000003\\n    - type: ndcg_at_10\\n      value: 40.762\\n    - type: ndcg_at_100\\n      value: 45.732\\n    - type: ndcg_at_1000\\n      value: 47.799\\n    - type: ndcg_at_20\\n      value: 42.591\\n    - type: ndcg_at_3\\n      value: 36.266999999999996\\n    - type: ndcg_at_5\\n      value: 38.58\\n    - type: precision_at_1\\n      value: 30.508000000000003\\n    - type: precision_at_10\\n      value: 6.010999999999999\\n    - type: precision_at_100\\n      value: 0.897\\n    - type: precision_at_1000\\n      value: 0.11100000000000002\\n    - type: precision_at_20\\n      value: 3.412\\n    - type: precision_at_3\\n      value: 14.991\\n    - type: precision_at_5\\n      value: 10.328\\n    - type: recall_at_1\\n      value: 28.648\\n    - type: recall_at_10\\n      value: 52.342999999999996\\n    - type: recall_at_100\\n      value: 75.268\\n    - type: recall_at_1000\\n      value: 90.641\\n    - type: recall_at_20\\n      value: 59.303\\n    - type: recall_at_3\\n      value: 40.447\\n    - type: recall_at_5\\n      value: 46.117000000000004\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackMathematicaRetrieval\\n      config: default\\n      split: test\\n      revision: 90fceea13679c63fe563ded68f3b6f06e50061de\\n    metrics:\\n    - type: map_at_1\\n      value: 18.476\\n    - type: map_at_10\\n      value: 27.148\\n    - type: map_at_100\\n      value: 28.317999999999998\\n    - type: map_at_1000\\n      value: 28.427999999999997\\n    - type: map_at_20\\n      value: 27.764\\n    - type: map_at_3\\n      value: 24.801000000000002\\n    - type: map_at_5\\n      value: 26.133\\n    - type: mrr_at_1\\n      value: 22.886\\n    - type: mrr_at_10\\n      value: 31.741000000000003\\n    - type: mrr_at_100\\n      value: 32.708\\n    - type: mrr_at_1000\\n      value: 32.769\\n    - type: mrr_at_20\\n      value: 32.296\\n    - type: mrr_at_3\\n      value: 29.498\\n    - type: mrr_at_5\\n      value: 30.773\\n    - type: ndcg_at_1\\n      value: 22.886\\n    - type: ndcg_at_10\\n      value: 32.265\\n    - type: ndcg_at_100\\n      value: 37.829\\n    - type: ndcg_at_1000\\n      value: 40.558\\n    - type: ndcg_at_20\\n      value: 34.372\\n    - type: ndcg_at_3\\n      value: 28.105000000000004\\n    - type: ndcg_at_5\\n      value: 30.04\\n    - type: precision_at_1\\n      value: 22.886\\n    - type: precision_at_10\\n      value: 5.808\\n    - type: precision_at_100\\n      value: 0.985\\n    - type: precision_at_1000\\n      value: 0.13699999999999998\\n    - type: precision_at_20\\n      value: 3.495\\n    - type: precision_at_3\\n      value: 13.639999999999999\\n    - type: precision_at_5\\n      value: 9.577\\n    - type: recall_at_1\\n      value: 18.476\\n    - type: recall_at_10\\n      value: 43.442\\n    - type: recall_at_100\\n      value: 67.376\\n    - type: recall_at_1000\\n      value: 86.874\\n    - type: recall_at_20\\n      value: 51.038\\n    - type: recall_at_3\\n      value: 31.785999999999998\\n    - type: recall_at_5\\n      value: 36.858999999999995\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackPhysicsRetrieval\\n      config: default\\n      split: test\\n      revision: 79531abbd1fb92d06c6d6315a0cbbbf5bb247ea4\\n    metrics:\\n    - type: map_at_1\\n      value: 29.098000000000003\\n    - type: map_at_10\\n      value: 38.97\\n    - type: map_at_100\\n      value: 40.293\\n    - type: map_at_1000\\n      value: 40.397\\n    - type: map_at_20\\n      value: 39.778999999999996\\n    - type: map_at_3\\n      value: 35.723\\n    - type: map_at_5\\n      value: 37.519999999999996\\n    - type: mrr_at_1\\n      value: 35.515\\n    - type: mrr_at_10\\n      value: 44.55\\n    - type: mrr_at_100\\n      value: 45.37\\n    - type: mrr_at_1000\\n      value: 45.412\\n    - type: mrr_at_20\\n      value: 45.054\\n    - type: mrr_at_3\\n      value: 41.835\\n    - type: mrr_at_5\\n      value: 43.356\\n    - type: ndcg_at_1\\n      value: 35.515\\n    - type: ndcg_at_10\\n      value: 44.91\\n    - type: ndcg_at_100\\n      value: 50.27700000000001\\n    - type: ndcg_at_1000\\n      value: 52.215\\n    - type: ndcg_at_20\\n      value: 47.235\\n    - type: ndcg_at_3\\n      value: 39.505\\n    - type: ndcg_at_5\\n      value: 42.016\\n    - type: precision_at_1\\n      value: 35.515\\n    - type: precision_at_10\\n      value: 8.152\\n    - type: precision_at_100\\n      value: 1.262\\n    - type: precision_at_1000\\n      value: 0.16\\n    - type: precision_at_20\\n      value: 4.851\\n    - type: precision_at_3\\n      value: 18.447\\n    - type: precision_at_5\\n      value: 13.321\\n    - type: recall_at_1\\n      value: 29.098000000000003\\n    - type: recall_at_10\\n      value: 57.115\\n    - type: recall_at_100\\n      value: 79.467\\n    - type: recall_at_1000\\n      value: 92.162\\n    - type: recall_at_20\\n      value: 65.161\\n    - type: recall_at_3\\n      value: 42.254000000000005\\n    - type: recall_at_5\\n      value: 48.415\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackProgrammersRetrieval\\n      config: default\\n      split: test\\n      revision: 6184bc1440d2dbc7612be22b50686b8826d22b32\\n    metrics:\\n    - type: map_at_1\\n      value: 27.372000000000003\\n    - type: map_at_10\\n      value: 37.781\\n    - type: map_at_100\\n      value: 39.128\\n    - type: map_at_1000\\n      value: 39.238\\n    - type: map_at_20\\n      value: 38.592\\n    - type: map_at_3\\n      value: 34.782999999999994\\n    - type: map_at_5\\n      value: 36.466\\n    - type: mrr_at_1\\n      value: 33.904\\n    - type: mrr_at_10\\n      value: 43.15\\n    - type: mrr_at_100\\n      value: 44.049\\n    - type: mrr_at_1000\\n      value: 44.107\\n    - type: mrr_at_20\\n      value: 43.721\\n    - type: mrr_at_3\\n      value: 40.677\\n    - type: mrr_at_5\\n      value: 42.19\\n    - type: ndcg_at_1\\n      value: 33.904\\n    - type: ndcg_at_10\\n      value: 43.527\\n    - type: ndcg_at_100\\n      value: 49.004999999999995\\n    - type: ndcg_at_1000\\n      value: 51.276999999999994\\n    - type: ndcg_at_20\\n      value: 45.988\\n    - type: ndcg_at_3\\n      value: 38.824999999999996\\n    - type: ndcg_at_5\\n      value: 41.04\\n    - type: precision_at_1\\n      value: 33.904\\n    - type: precision_at_10\\n      value: 7.854\\n    - type: precision_at_100\\n      value: 1.2309999999999999\\n    - type: precision_at_1000\\n      value: 0.16\\n    - type: precision_at_20\\n      value: 4.692\\n    - type: precision_at_3\\n      value: 18.531\\n    - type: precision_at_5\\n      value: 13.150999999999998\\n    - type: recall_at_1\\n      value: 27.372000000000003\\n    - type: recall_at_10\\n      value: 55.245999999999995\\n    - type: recall_at_100\\n      value: 78.278\\n    - type: recall_at_1000\\n      value: 93.718\\n    - type: recall_at_20\\n      value: 64.095\\n    - type: recall_at_3\\n      value: 41.665\\n    - type: recall_at_5\\n      value: 47.632000000000005\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackRetrieval\\n      config: default\\n      split: test\\n      revision: f46a197baaae43b4f621051089b82a364682dfeb\\n    metrics:\\n    - type: map_at_1\\n      value: 27.734166666666667\\n    - type: map_at_10\\n      value: 36.858\\n    - type: map_at_100\\n      value: 38.043833333333325\\n    - type: map_at_1000\\n      value: 38.15541666666667\\n    - type: map_at_20\\n      value: 37.521249999999995\\n    - type: map_at_3\\n      value: 34.07658333333333\\n    - type: map_at_5\\n      value: 35.62683333333333\\n    - type: mrr_at_1\\n      value: 32.676249999999996\\n    - type: mrr_at_10\\n      value: 40.999\\n    - type: mrr_at_100\\n      value: 41.835\\n    - type: mrr_at_1000\\n      value: 41.8895\\n    - type: mrr_at_20\\n      value: 41.4865\\n    - type: mrr_at_3\\n      value: 38.645\\n    - type: mrr_at_5\\n      value: 39.99725000000001\\n    - type: ndcg_at_1\\n      value: 32.676249999999996\\n    - type: ndcg_at_10\\n      value: 42.08016666666666\\n    - type: ndcg_at_100\\n      value: 47.082750000000004\\n    - type: ndcg_at_1000\\n      value: 49.276583333333335\\n    - type: ndcg_at_20\\n      value: 44.04808333333334\\n    - type: ndcg_at_3\\n      value: 37.43375\\n    - type: ndcg_at_5\\n      value: 39.623000000000005\\n    - type: precision_at_1\\n      value: 32.676249999999996\\n    - type: precision_at_10\\n      value: 7.271\\n    - type: precision_at_100\\n      value: 1.1458333333333333\\n    - type: precision_at_1000\\n      value: 0.152\\n    - type: precision_at_20\\n      value: 4.282916666666667\\n    - type: precision_at_3\\n      value: 17.061416666666666\\n    - type: precision_at_5\\n      value: 12.05466666666667\\n    - type: recall_at_1\\n      value: 27.734166666666667\\n    - type: recall_at_10\\n      value: 53.33574999999999\\n    - type: recall_at_100\\n      value: 75.16275\\n    - type: recall_at_1000\\n      value: 90.34891666666665\\n    - type: recall_at_20\\n      value: 60.4935\\n    - type: recall_at_3\\n      value: 40.377916666666664\\n    - type: recall_at_5\\n      value: 46.0195\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackStatsRetrieval\\n      config: default\\n      split: test\\n      revision: 65ac3a16b8e91f9cee4c9828cc7c335575432a2a\\n    metrics:\\n    - type: map_at_1\\n      value: 25.653\\n    - type: map_at_10\\n      value: 32.151\\n    - type: map_at_100\\n      value: 33.152\\n    - type: map_at_1000\\n      value: 33.243\\n    - type: map_at_20\\n      value: 32.717\\n    - type: map_at_3\\n      value: 30.287\\n    - type: map_at_5\\n      value: 31.25\\n    - type: mrr_at_1\\n      value: 28.988000000000003\\n    - type: mrr_at_10\\n      value: 35.131\\n    - type: mrr_at_100\\n      value: 36.002\\n    - type: mrr_at_1000\\n      value: 36.069\\n    - type: mrr_at_20\\n      value: 35.61\\n    - type: mrr_at_3\\n      value: 33.308\\n    - type: mrr_at_5\\n      value: 34.259\\n    - type: ndcg_at_1\\n      value: 28.988000000000003\\n    - type: ndcg_at_10\\n      value: 35.988\\n    - type: ndcg_at_100\\n      value: 40.764\\n    - type: ndcg_at_1000\\n      value: 43.112\\n    - type: ndcg_at_20\\n      value: 37.852999999999994\\n    - type: ndcg_at_3\\n      value: 32.562000000000005\\n    - type: ndcg_at_5\\n      value: 33.983000000000004\\n    - type: precision_at_1\\n      value: 28.988000000000003\\n    - type: precision_at_10\\n      value: 5.475\\n    - type: precision_at_100\\n      value: 0.8500000000000001\\n    - type: precision_at_1000\\n      value: 0.11199999999999999\\n    - type: precision_at_20\\n      value: 3.229\\n    - type: precision_at_3\\n      value: 13.905999999999999\\n    - type: precision_at_5\\n      value: 9.386999999999999\\n    - type: recall_at_1\\n      value: 25.653\\n    - type: recall_at_10\\n      value: 44.962\\n    - type: recall_at_100\\n      value: 66.405\\n    - type: recall_at_1000\\n      value: 83.88799999999999\\n    - type: recall_at_20\\n      value: 51.79899999999999\\n    - type: recall_at_3\\n      value: 35.144999999999996\\n    - type: recall_at_5\\n      value: 38.814\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackTexRetrieval\\n      config: default\\n      split: test\\n      revision: 46989137a86843e03a6195de44b09deda022eec7\\n    metrics:\\n    - type: map_at_1\\n      value: 17.825\\n    - type: map_at_10\\n      value: 25.592\\n    - type: map_at_100\\n      value: 26.613999999999997\\n    - type: map_at_1000\\n      value: 26.734\\n    - type: map_at_20\\n      value: 26.115\\n    - type: map_at_3\\n      value: 23.119\\n    - type: map_at_5\\n      value: 24.54\\n    - type: mrr_at_1\\n      value: 21.335\\n    - type: mrr_at_10\\n      value: 29.165000000000003\\n    - type: mrr_at_100\\n      value: 30.049\\n    - type: mrr_at_1000\\n      value: 30.121\\n    - type: mrr_at_20\\n      value: 29.639\\n    - type: mrr_at_3\\n      value: 26.863999999999997\\n    - type: mrr_at_5\\n      value: 28.185\\n    - type: ndcg_at_1\\n      value: 21.335\\n    - type: ndcg_at_10\\n      value: 30.357\\n    - type: ndcg_at_100\\n      value: 35.410000000000004\\n    - type: ndcg_at_1000\\n      value: 38.24\\n    - type: ndcg_at_20\\n      value: 32.08\\n    - type: ndcg_at_3\\n      value: 25.95\\n    - type: ndcg_at_5\\n      value: 28.081\\n    - type: precision_at_1\\n      value: 21.335\\n    - type: precision_at_10\\n      value: 5.506\\n    - type: precision_at_100\\n      value: 0.928\\n    - type: precision_at_1000\\n      value: 0.135\\n    - type: precision_at_20\\n      value: 3.2550000000000003\\n    - type: precision_at_3\\n      value: 12.239\\n    - type: precision_at_5\\n      value: 8.885\\n    - type: recall_at_1\\n      value: 17.825\\n    - type: recall_at_10\\n      value: 41.105999999999995\\n    - type: recall_at_100\\n      value: 64.17\\n    - type: recall_at_1000\\n      value: 84.19200000000001\\n    - type: recall_at_20\\n      value: 47.497\\n    - type: recall_at_3\\n      value: 28.862\\n    - type: recall_at_5\\n      value: 34.348\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackUnixRetrieval\\n      config: default\\n      split: test\\n      revision: 6c6430d3a6d36f8d2a829195bc5dc94d7e063e53\\n    metrics:\\n    - type: map_at_1\\n      value: 29.435\\n    - type: map_at_10\\n      value: 38.261\\n    - type: map_at_100\\n      value: 39.242\\n    - type: map_at_1000\\n      value: 39.347\\n    - type: map_at_20\\n      value: 38.742\\n    - type: map_at_3\\n      value: 35.457\\n    - type: map_at_5\\n      value: 37.043\\n    - type: mrr_at_1\\n      value: 34.235\\n    - type: mrr_at_10\\n      value: 42.24\\n    - type: mrr_at_100\\n      value: 42.988\\n    - type: mrr_at_1000\\n      value: 43.043\\n    - type: mrr_at_20\\n      value: 42.613\\n    - type: mrr_at_3\\n      value: 39.832\\n    - type: mrr_at_5\\n      value: 41.227000000000004\\n    - type: ndcg_at_1\\n      value: 34.235\\n    - type: ndcg_at_10\\n      value: 43.384\\n    - type: ndcg_at_100\\n      value: 48.14\\n    - type: ndcg_at_1000\\n      value: 50.414\\n    - type: ndcg_at_20\\n      value: 44.913\\n    - type: ndcg_at_3\\n      value: 38.454\\n    - type: ndcg_at_5\\n      value: 40.776\\n    - type: precision_at_1\\n      value: 34.235\\n    - type: precision_at_10\\n      value: 7.164\\n    - type: precision_at_100\\n      value: 1.065\\n    - type: precision_at_1000\\n      value: 0.13699999999999998\\n    - type: precision_at_20\\n      value: 4.021\\n    - type: precision_at_3\\n      value: 17.226\\n    - type: precision_at_5\\n      value: 12.071\\n    - type: recall_at_1\\n      value: 29.435\\n    - type: recall_at_10\\n      value: 54.93900000000001\\n    - type: recall_at_100\\n      value: 76.176\\n    - type: recall_at_1000\\n      value: 91.989\\n    - type: recall_at_20\\n      value: 60.451\\n    - type: recall_at_3\\n      value: 41.332\\n    - type: recall_at_5\\n      value: 47.316\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackWebmastersRetrieval\\n      config: default\\n      split: test\\n      revision: 160c094312a0e1facb97e55eeddb698c0abe3571\\n    metrics:\\n    - type: map_at_1\\n      value: 25.605\\n    - type: map_at_10\\n      value: 34.162\\n    - type: map_at_100\\n      value: 35.827999999999996\\n    - type: map_at_1000\\n      value: 36.04\\n    - type: map_at_20\\n      value: 35.016000000000005\\n    - type: map_at_3\\n      value: 30.984\\n    - type: map_at_5\\n      value: 32.717\\n    - type: mrr_at_1\\n      value: 30.435000000000002\\n    - type: mrr_at_10\\n      value: 38.681\\n    - type: mrr_at_100\\n      value: 39.656000000000006\\n    - type: mrr_at_1000\\n      value: 39.71\\n    - type: mrr_at_20\\n      value: 39.208999999999996\\n    - type: mrr_at_3\\n      value: 35.903\\n    - type: mrr_at_5\\n      value: 37.454\\n    - type: ndcg_at_1\\n      value: 30.435000000000002\\n    - type: ndcg_at_10\\n      value: 39.916000000000004\\n    - type: ndcg_at_100\\n      value: 45.958\\n    - type: ndcg_at_1000\\n      value: 48.449999999999996\\n    - type: ndcg_at_20\\n      value: 42.085\\n    - type: ndcg_at_3\\n      value: 34.696\\n    - type: ndcg_at_5\\n      value: 37.147000000000006\\n    - type: precision_at_1\\n      value: 30.435000000000002\\n    - type: precision_at_10\\n      value: 7.767\\n    - type: precision_at_100\\n      value: 1.547\\n    - type: precision_at_1000\\n      value: 0.23800000000000002\\n    - type: precision_at_20\\n      value: 4.941\\n    - type: precision_at_3\\n      value: 16.073999999999998\\n    - type: precision_at_5\\n      value: 11.937000000000001\\n    - type: recall_at_1\\n      value: 25.605\\n    - type: recall_at_10\\n      value: 50.654999999999994\\n    - type: recall_at_100\\n      value: 77.609\\n    - type: recall_at_1000\\n      value: 93.518\\n    - type: recall_at_20\\n      value: 58.845000000000006\\n    - type: recall_at_3\\n      value: 36.272\\n    - type: recall_at_5\\n      value: 42.596000000000004\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: BeIR/cqadupstack\\n      name: MTEB CQADupstackWordpressRetrieval\\n      config: default\\n      split: test\\n      revision: 4ffe81d471b1924886b33c7567bfb200e9eec5c4\\n    metrics:\\n    - type: map_at_1\\n      value: 23.666\\n    - type: map_at_10\\n      value: 30.980999999999998\\n    - type: map_at_100\\n      value: 32.0\\n    - type: map_at_1000\\n      value: 32.098\\n    - type: map_at_20\\n      value: 31.621\\n    - type: map_at_3\\n      value: 28.449999999999996\\n    - type: map_at_5\\n      value: 29.731999999999996\\n    - type: mrr_at_1\\n      value: 25.692999999999998\\n    - type: mrr_at_10\\n      value: 32.788000000000004\\n    - type: mrr_at_100\\n      value: 33.783\\n    - type: mrr_at_1000\\n      value: 33.849000000000004\\n    - type: mrr_at_20\\n      value: 33.408\\n    - type: mrr_at_3\\n      value: 30.561\\n    - type: mrr_at_5\\n      value: 31.716\\n    - type: ndcg_at_1\\n      value: 25.692999999999998\\n    - type: ndcg_at_10\\n      value: 35.428\\n    - type: ndcg_at_100\\n      value: 40.375\\n    - type: ndcg_at_1000\\n      value: 42.802\\n    - type: ndcg_at_20\\n      value: 37.621\\n    - type: ndcg_at_3\\n      value: 30.476999999999997\\n    - type: ndcg_at_5\\n      value: 32.621\\n    - type: precision_at_1\\n      value: 25.692999999999998\\n    - type: precision_at_10\\n      value: 5.508\\n    - type: precision_at_100\\n      value: 0.848\\n    - type: precision_at_1000\\n      value: 0.116\\n    - type: precision_at_20\\n      value: 3.272\\n    - type: precision_at_3\\n      value: 12.631\\n    - type: precision_at_5\\n      value: 8.872\\n    - type: recall_at_1\\n      value: 23.666\\n    - type: recall_at_10\\n      value: 47.532000000000004\\n    - type: recall_at_100\\n      value: 69.73700000000001\\n    - type: recall_at_1000\\n      value: 87.83800000000001\\n    - type: recall_at_20\\n      value: 55.61000000000001\\n    - type: recall_at_3\\n      value: 34.06\\n    - type: recall_at_5\\n      value: 39.254\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB ClimateFEVER\\n      config: default\\n      split: test\\n      revision: 47f2ac6acb640fc46020b02a5b59fdda04d39380\\n    metrics:\\n    - type: map_at_1\\n      value: 16.337\\n    - type: map_at_10\\n      value: 26.488\\n    - type: map_at_100\\n      value: 28.415000000000003\\n    - type: map_at_1000\\n      value: 28.584\\n    - type: map_at_20\\n      value: 27.557\\n    - type: map_at_3\\n      value: 22.665\\n    - type: map_at_5\\n      value: 24.542\\n    - type: mrr_at_1\\n      value: 36.417\\n    - type: mrr_at_10\\n      value: 48.001\\n    - type: mrr_at_100\\n      value: 48.784\\n    - type: mrr_at_1000\\n      value: 48.809000000000005\\n    - type: mrr_at_20\\n      value: 48.507\\n    - type: mrr_at_3\\n      value: 45.103\\n    - type: mrr_at_5\\n      value: 46.843\\n    - type: ndcg_at_1\\n      value: 36.417\\n    - type: ndcg_at_10\\n      value: 35.67\\n    - type: ndcg_at_100\\n      value: 42.716\\n    - type: ndcg_at_1000\\n      value: 45.639\\n    - type: ndcg_at_20\\n      value: 38.471\\n    - type: ndcg_at_3\\n      value: 30.444\\n    - type: ndcg_at_5\\n      value: 32.004\\n    - type: precision_at_1\\n      value: 36.417\\n    - type: precision_at_10\\n      value: 10.73\\n    - type: precision_at_100\\n      value: 1.833\\n    - type: precision_at_1000\\n      value: 0.23800000000000002\\n    - type: precision_at_20\\n      value: 6.596\\n    - type: precision_at_3\\n      value: 22.302\\n    - type: precision_at_5\\n      value: 16.521\\n    - type: recall_at_1\\n      value: 16.337\\n    - type: recall_at_10\\n      value: 40.671\\n    - type: recall_at_100\\n      value: 64.55300000000001\\n    - type: recall_at_1000\\n      value: 80.934\\n    - type: recall_at_20\\n      value: 48.381\\n    - type: recall_at_3\\n      value: 27.279999999999998\\n    - type: recall_at_5\\n      value: 32.621\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB DBPedia\\n      config: default\\n      split: test\\n      revision: c0f706b76e590d620bd6618b3ca8efdd34e2d659\\n    metrics:\\n    - type: map_at_1\\n      value: 9.056000000000001\\n    - type: map_at_10\\n      value: 19.419\\n    - type: map_at_100\\n      value: 27.069\\n    - type: map_at_1000\\n      value: 28.666000000000004\\n    - type: map_at_20\\n      value: 22.434\\n    - type: map_at_3\\n      value: 13.895\\n    - type: map_at_5\\n      value: 16.121\\n    - type: mrr_at_1\\n      value: 69.0\\n    - type: mrr_at_10\\n      value: 75.804\\n    - type: mrr_at_100\\n      value: 76.117\\n    - type: mrr_at_1000\\n      value: 76.125\\n    - type: mrr_at_20\\n      value: 76.009\\n    - type: mrr_at_3\\n      value: 74.375\\n    - type: mrr_at_5\\n      value: 75.4\\n    - type: ndcg_at_1\\n      value: 57.49999999999999\\n    - type: ndcg_at_10\\n      value: 41.495\\n    - type: ndcg_at_100\\n      value: 45.208\\n    - type: ndcg_at_1000\\n      value: 52.221\\n    - type: ndcg_at_20\\n      value: 40.617999999999995\\n    - type: ndcg_at_3\\n      value: 46.592\\n    - type: ndcg_at_5\\n      value: 43.559\\n    - type: precision_at_1\\n      value: 69.0\\n    - type: precision_at_10\\n      value: 32.574999999999996\\n    - type: precision_at_100\\n      value: 10.205\\n    - type: precision_at_1000\\n      value: 2.036\\n    - type: precision_at_20\\n      value: 24.687\\n    - type: precision_at_3\\n      value: 49.75\\n    - type: precision_at_5\\n      value: 42.0\\n    - type: recall_at_1\\n      value: 9.056000000000001\\n    - type: recall_at_10\\n      value: 24.866\\n    - type: recall_at_100\\n      value: 50.097\\n    - type: recall_at_1000\\n      value: 72.038\\n    - type: recall_at_20\\n      value: 31.858999999999998\\n    - type: recall_at_3\\n      value: 15.096000000000002\\n    - type: recall_at_5\\n      value: 18.548000000000002\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB EmotionClassification\\n      config: default\\n      split: test\\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\\n    metrics:\\n    - type: accuracy\\n      value: 48.259999999999984\\n    - type: f1\\n      value: 43.1498589523159\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB FEVER\\n      config: default\\n      split: test\\n      revision: bea83ef9e8fb933d90a2f1d5515737465d613e12\\n    metrics:\\n    - type: map_at_1\\n      value: 74.798\\n    - type: map_at_10\\n      value: 83.454\\n    - type: map_at_100\\n      value: 83.623\\n    - type: map_at_1000\\n      value: 83.635\\n    - type: map_at_20\\n      value: 83.55\\n    - type: map_at_3\\n      value: 82.392\\n    - type: map_at_5\\n      value: 83.167\\n    - type: mrr_at_1\\n      value: 80.708\\n    - type: mrr_at_10\\n      value: 88.377\\n    - type: mrr_at_100\\n      value: 88.411\\n    - type: mrr_at_1000\\n      value: 88.411\\n    - type: mrr_at_20\\n      value: 88.402\\n    - type: mrr_at_3\\n      value: 87.646\\n    - type: mrr_at_5\\n      value: 88.232\\n    - type: ndcg_at_1\\n      value: 80.708\\n    - type: ndcg_at_10\\n      value: 87.35199999999999\\n    - type: ndcg_at_100\\n      value: 87.91600000000001\\n    - type: ndcg_at_1000\\n      value: 88.12299999999999\\n    - type: ndcg_at_20\\n      value: 87.593\\n    - type: ndcg_at_3\\n      value: 85.738\\n    - type: ndcg_at_5\\n      value: 86.845\\n    - type: precision_at_1\\n      value: 80.708\\n    - type: precision_at_10\\n      value: 10.432\\n    - type: precision_at_100\\n      value: 1.091\\n    - type: precision_at_1000\\n      value: 0.11299999999999999\\n    - type: precision_at_20\\n      value: 5.296\\n    - type: precision_at_3\\n      value: 32.778\\n    - type: precision_at_5\\n      value: 20.399\\n    - type: recall_at_1\\n      value: 74.798\\n    - type: recall_at_10\\n      value: 94.459\\n    - type: recall_at_100\\n      value: 96.614\\n    - type: recall_at_1000\\n      value: 97.868\\n    - type: recall_at_20\\n      value: 95.254\\n    - type: recall_at_3\\n      value: 90.144\\n    - type: recall_at_5\\n      value: 92.965\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB FiQA2018\\n      config: default\\n      split: test\\n      revision: 27a168819829fe9bcd655c2df245fb19452e8e06\\n    metrics:\\n    - type: map_at_1\\n      value: 20.008\\n    - type: map_at_10\\n      value: 32.731\\n    - type: map_at_100\\n      value: 34.467999999999996\\n    - type: map_at_1000\\n      value: 34.643\\n    - type: map_at_20\\n      value: 33.717000000000006\\n    - type: map_at_3\\n      value: 28.427999999999997\\n    - type: map_at_5\\n      value: 30.788\\n    - type: mrr_at_1\\n      value: 40.586\\n    - type: mrr_at_10\\n      value: 49.056\\n    - type: mrr_at_100\\n      value: 49.887\\n    - type: mrr_at_1000\\n      value: 49.929\\n    - type: mrr_at_20\\n      value: 49.552\\n    - type: mrr_at_3\\n      value: 46.785\\n    - type: mrr_at_5\\n      value: 48.004000000000005\\n    - type: ndcg_at_1\\n      value: 40.586\\n    - type: ndcg_at_10\\n      value: 40.589999999999996\\n    - type: ndcg_at_100\\n      value: 47.03\\n    - type: ndcg_at_1000\\n      value: 49.994\\n    - type: ndcg_at_20\\n      value: 43.229\\n    - type: ndcg_at_3\\n      value: 37.061\\n    - type: ndcg_at_5\\n      value: 37.992\\n    - type: precision_at_1\\n      value: 40.586\\n    - type: precision_at_10\\n      value: 11.219\\n    - type: precision_at_100\\n      value: 1.781\\n    - type: precision_at_1000\\n      value: 0.232\\n    - type: precision_at_20\\n      value: 6.705\\n    - type: precision_at_3\\n      value: 24.743000000000002\\n    - type: precision_at_5\\n      value: 18.086\\n    - type: recall_at_1\\n      value: 20.008\\n    - type: recall_at_10\\n      value: 47.412\\n    - type: recall_at_100\\n      value: 71.274\\n    - type: recall_at_1000\\n      value: 88.898\\n    - type: recall_at_20\\n      value: 55.706999999999994\\n    - type: recall_at_3\\n      value: 33.346\\n    - type: recall_at_5\\n      value: 39.112\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB HotpotQA\\n      config: default\\n      split: test\\n      revision: ab518f4d6fcca38d87c25209f94beba119d02014\\n    metrics:\\n    - type: map_at_1\\n      value: 41.789\\n    - type: map_at_10\\n      value: 57.898\\n    - type: map_at_100\\n      value: 58.632\\n    - type: map_at_1000\\n      value: 58.693\\n    - type: map_at_20\\n      value: 58.314\\n    - type: map_at_3\\n      value: 55.236\\n    - type: map_at_5\\n      value: 56.852999999999994\\n    - type: mrr_at_1\\n      value: 83.57900000000001\\n    - type: mrr_at_10\\n      value: 87.631\\n    - type: mrr_at_100\\n      value: 87.764\\n    - type: mrr_at_1000\\n      value: 87.77000000000001\\n    - type: mrr_at_20\\n      value: 87.70700000000001\\n    - type: mrr_at_3\\n      value: 87.02499999999999\\n    - type: mrr_at_5\\n      value: 87.34100000000001\\n    - type: ndcg_at_1\\n      value: 83.57900000000001\\n    - type: ndcg_at_10\\n      value: 67.11399999999999\\n    - type: ndcg_at_100\\n      value: 69.686\\n    - type: ndcg_at_1000\\n      value: 70.926\\n    - type: ndcg_at_20\\n      value: 68.119\\n    - type: ndcg_at_3\\n      value: 63.402\\n    - type: ndcg_at_5\\n      value: 65.354\\n    - type: precision_at_1\\n      value: 83.57900000000001\\n    - type: precision_at_10\\n      value: 13.333\\n    - type: precision_at_100\\n      value: 1.537\\n    - type: precision_at_1000\\n      value: 0.16999999999999998\\n    - type: precision_at_20\\n      value: 6.988999999999999\\n    - type: precision_at_3\\n      value: 38.929\\n    - type: precision_at_5\\n      value: 24.897\\n    - type: recall_at_1\\n      value: 41.789\\n    - type: recall_at_10\\n      value: 66.664\\n    - type: recall_at_100\\n      value: 76.833\\n    - type: recall_at_1000\\n      value: 85.14500000000001\\n    - type: recall_at_20\\n      value: 69.892\\n    - type: recall_at_3\\n      value: 58.392999999999994\\n    - type: recall_at_5\\n      value: 62.242\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB ImdbClassification\\n      config: default\\n      split: test\\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\\n    metrics:\\n    - type: accuracy\\n      value: 86.6108\\n    - type: ap\\n      value: 81.63890253106925\\n    - type: f1\\n      value: 86.54585789538082\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB MSMARCO\\n      config: default\\n      split: dev\\n      revision: c5a29a104738b98a9e76336939199e264163d4a0\\n    metrics:\\n    - type: map_at_1\\n      value: 22.407\\n    - type: map_at_10\\n      value: 34.603\\n    - type: map_at_100\\n      value: 35.808\\n    - type: map_at_1000\\n      value: 35.855\\n    - type: map_at_20\\n      value: 35.368\\n    - type: map_at_3\\n      value: 30.764000000000003\\n    - type: map_at_5\\n      value: 32.964\\n    - type: mrr_at_1\\n      value: 23.009\\n    - type: mrr_at_10\\n      value: 35.136\\n    - type: mrr_at_100\\n      value: 36.284\\n    - type: mrr_at_1000\\n      value: 36.325\\n    - type: mrr_at_20\\n      value: 35.869\\n    - type: mrr_at_3\\n      value: 31.351000000000003\\n    - type: mrr_at_5\\n      value: 33.54\\n    - type: ndcg_at_1\\n      value: 23.009\\n    - type: ndcg_at_10\\n      value: 41.471999999999994\\n    - type: ndcg_at_100\\n      value: 47.211999999999996\\n    - type: ndcg_at_1000\\n      value: 48.361\\n    - type: ndcg_at_20\\n      value: 44.169000000000004\\n    - type: ndcg_at_3\\n      value: 33.646\\n    - type: ndcg_at_5\\n      value: 37.580000000000005\\n    - type: precision_at_1\\n      value: 23.009\\n    - type: precision_at_10\\n      value: 6.54\\n    - type: precision_at_100\\n      value: 0.941\\n    - type: precision_at_1000\\n      value: 0.104\\n    - type: precision_at_20\\n      value: 3.832\\n    - type: precision_at_3\\n      value: 14.283999999999999\\n    - type: precision_at_5\\n      value: 10.564\\n    - type: recall_at_1\\n      value: 22.407\\n    - type: recall_at_10\\n      value: 62.678999999999995\\n    - type: recall_at_100\\n      value: 89.09700000000001\\n    - type: recall_at_1000\\n      value: 97.822\\n    - type: recall_at_20\\n      value: 73.116\\n    - type: recall_at_3\\n      value: 41.4\\n    - type: recall_at_5\\n      value: 50.855\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB MTOPDomainClassification (en)\\n      config: en\\n      split: test\\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\\n    metrics:\\n    - type: accuracy\\n      value: 92.94573643410853\\n    - type: f1\\n      value: 92.73148878666994\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB MTOPIntentClassification (en)\\n      config: en\\n      split: test\\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\\n    metrics:\\n    - type: accuracy\\n      value: 77.86137710898313\\n    - type: f1\\n      value: 60.360562463738724\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB MassiveIntentClassification (en)\\n      config: en\\n      split: test\\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\\n    metrics:\\n    - type: accuracy\\n      value: 73.83322125084062\\n    - type: f1\\n      value: 71.61864304680206\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB MassiveScenarioClassification (en)\\n      config: en\\n      split: test\\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\\n    metrics:\\n    - type: accuracy\\n      value: 77.50504371217215\\n    - type: f1\\n      value: 77.52039268347185\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB MedrxivClusteringP2P\\n      config: default\\n      split: test\\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\\n    metrics:\\n    - type: v_measure\\n      value: 34.346952648910225\\n    - type: v_measures\\n      value: [0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024, 0.3246964225451952, 0.33269208719245646, 0.3355911472371345, 0.32978655133380147, 0.3275090874657499, 0.3752583186941529, 0.3494711327267592, 0.36636134409497156, 0.3538734420417993, 0.3394557315590024]\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB MedrxivClusteringS2S\\n      config: default\\n      split: test\\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\\n    metrics:\\n    - type: v_measure\\n      value: 32.19992734583148\\n    - type: v_measures\\n      value: [0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027, 0.31100967211136193, 0.31302897733611235, 0.3126922134381441, 0.30243629014133017, 0.31564501718268645, 0.34772968477866795, 0.32522623268021805, 0.3410158265159116, 0.33581770403870503, 0.31539111636001027]\\n  - task:\\n      type: Reranking\\n    dataset:\\n      type: None\\n      name: MTEB MindSmallReranking\\n      config: default\\n      split: test\\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\\n    metrics:\\n    - type: map\\n      value: 30.62309561205373\\n    - type: mrr\\n      value: 31.707879717902554\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB NFCorpus\\n      config: default\\n      split: test\\n      revision: ec0fa4fe99da2ff19ca1214b7966684033a58814\\n    metrics:\\n    - type: map_at_1\\n      value: 5.668\\n    - type: map_at_10\\n      value: 12.225999999999999\\n    - type: map_at_100\\n      value: 15.122\\n    - type: map_at_1000\\n      value: 16.422\\n    - type: map_at_20\\n      value: 13.361999999999998\\n    - type: map_at_3\\n      value: 9.083\\n    - type: map_at_5\\n      value: 10.5\\n    - type: mrr_at_1\\n      value: 46.44\\n    - type: mrr_at_10\\n      value: 53.553\\n    - type: mrr_at_100\\n      value: 54.15\\n    - type: mrr_at_1000\\n      value: 54.193000000000005\\n    - type: mrr_at_20\\n      value: 53.837\\n    - type: mrr_at_3\\n      value: 51.702999999999996\\n    - type: mrr_at_5\\n      value: 52.647\\n    - type: ndcg_at_1\\n      value: 44.272\\n    - type: ndcg_at_10\\n      value: 33.395\\n    - type: ndcg_at_100\\n      value: 29.976999999999997\\n    - type: ndcg_at_1000\\n      value: 38.388\\n    - type: ndcg_at_20\\n      value: 30.606\\n    - type: ndcg_at_3\\n      value: 39.212\\n    - type: ndcg_at_5\\n      value: 36.611\\n    - type: precision_at_1\\n      value: 46.129999999999995\\n    - type: precision_at_10\\n      value: 24.334\\n    - type: precision_at_100\\n      value: 7.553999999999999\\n    - type: precision_at_1000\\n      value: 1.994\\n    - type: precision_at_20\\n      value: 17.678\\n    - type: precision_at_3\\n      value: 36.326\\n    - type: precision_at_5\\n      value: 31.330999999999996\\n    - type: recall_at_1\\n      value: 5.668\\n    - type: recall_at_10\\n      value: 15.837000000000002\\n    - type: recall_at_100\\n      value: 29.845\\n    - type: recall_at_1000\\n      value: 60.563\\n    - type: recall_at_20\\n      value: 18.587999999999997\\n    - type: recall_at_3\\n      value: 10.096\\n    - type: recall_at_5\\n      value: 12.261\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB NQ\\n      config: default\\n      split: test\\n      revision: b774495ed302d8c44a3a7ea25c90dbce03968f31\\n    metrics:\\n    - type: map_at_1\\n      value: 39.335\\n    - type: map_at_10\\n      value: 54.932\\n    - type: map_at_100\\n      value: 55.742000000000004\\n    - type: map_at_1000\\n      value: 55.766000000000005\\n    - type: map_at_20\\n      value: 55.504\\n    - type: map_at_3\\n      value: 50.904\\n    - type: map_at_5\\n      value: 53.388999999999996\\n    - type: mrr_at_1\\n      value: 44.003\\n    - type: mrr_at_10\\n      value: 57.419\\n    - type: mrr_at_100\\n      value: 57.963\\n    - type: mrr_at_1000\\n      value: 57.981\\n    - type: mrr_at_20\\n      value: 57.80499999999999\\n    - type: mrr_at_3\\n      value: 54.30199999999999\\n    - type: mrr_at_5\\n      value: 56.257000000000005\\n    - type: ndcg_at_1\\n      value: 43.974999999999994\\n    - type: ndcg_at_10\\n      value: 62.153999999999996\\n    - type: ndcg_at_100\\n      value: 65.326\\n    - type: ndcg_at_1000\\n      value: 65.862\\n    - type: ndcg_at_20\\n      value: 63.922999999999995\\n    - type: ndcg_at_3\\n      value: 54.834\\n    - type: ndcg_at_5\\n      value: 58.857000000000006\\n    - type: precision_at_1\\n      value: 43.974999999999994\\n    - type: precision_at_10\\n      value: 9.722\\n    - type: precision_at_100\\n      value: 1.153\\n    - type: precision_at_1000\\n      value: 0.12\\n    - type: precision_at_20\\n      value: 5.3\\n    - type: precision_at_3\\n      value: 24.392\\n    - type: precision_at_5\\n      value: 16.993\\n    - type: recall_at_1\\n      value: 39.335\\n    - type: recall_at_10\\n      value: 81.501\\n    - type: recall_at_100\\n      value: 94.851\\n    - type: recall_at_1000\\n      value: 98.817\\n    - type: recall_at_20\\n      value: 87.968\\n    - type: recall_at_3\\n      value: 62.795\\n    - type: recall_at_5\\n      value: 71.985\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB QuoraRetrieval\\n      config: default\\n      split: test\\n      revision: e4e08e0b7dbe3c8700f0daef558ff32256715259\\n    metrics:\\n    - type: map_at_1\\n      value: 71.222\\n    - type: map_at_10\\n      value: 85.193\\n    - type: map_at_100\\n      value: 85.802\\n    - type: map_at_1000\\n      value: 85.81800000000001\\n    - type: map_at_20\\n      value: 85.587\\n    - type: map_at_3\\n      value: 82.253\\n    - type: map_at_5\\n      value: 84.142\\n    - type: mrr_at_1\\n      value: 82.04\\n    - type: mrr_at_10\\n      value: 88.101\\n    - type: mrr_at_100\\n      value: 88.196\\n    - type: mrr_at_1000\\n      value: 88.196\\n    - type: mrr_at_20\\n      value: 88.175\\n    - type: mrr_at_3\\n      value: 87.145\\n    - type: mrr_at_5\\n      value: 87.825\\n    - type: ndcg_at_1\\n      value: 82.04\\n    - type: ndcg_at_10\\n      value: 88.849\\n    - type: ndcg_at_100\\n      value: 89.992\\n    - type: ndcg_at_1000\\n      value: 90.089\\n    - type: ndcg_at_20\\n      value: 89.468\\n    - type: ndcg_at_3\\n      value: 86.06899999999999\\n    - type: ndcg_at_5\\n      value: 87.669\\n    - type: precision_at_1\\n      value: 82.04\\n    - type: precision_at_10\\n      value: 13.447000000000001\\n    - type: precision_at_100\\n      value: 1.528\\n    - type: precision_at_1000\\n      value: 0.157\\n    - type: precision_at_20\\n      value: 7.116\\n    - type: precision_at_3\\n      value: 37.617\\n    - type: precision_at_5\\n      value: 24.776\\n    - type: recall_at_1\\n      value: 71.222\\n    - type: recall_at_10\\n      value: 95.73899999999999\\n    - type: recall_at_100\\n      value: 99.572\\n    - type: recall_at_1000\\n      value: 99.988\\n    - type: recall_at_20\\n      value: 97.725\\n    - type: recall_at_3\\n      value: 87.742\\n    - type: recall_at_5\\n      value: 92.23400000000001\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB RedditClustering\\n      config: default\\n      split: test\\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\\n    metrics:\\n    - type: v_measure\\n      value: 56.502005725283524\\n    - type: v_measures\\n      value: [0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237, 0.5845673186673394, 0.648423996059595, 0.5081078446363154, 0.577059582267051, 0.5449838765447135, 0.5255305026550916, 0.6001776953894321, 0.5075448301528861, 0.5238448212279936, 0.5329001795025329, 0.5112306232092642, 0.6002807353254037, 0.5525285295615835, 0.56281813563348, 0.6722346506108504, 0.5293879728430999, 0.5972632642217942, 0.6345018102197326, 0.515945887049231, 0.5291998092690363, 0.5250323799432043, 0.538426398169316, 0.6954213901632498, 0.580008522375662, 0.5280806756230237]\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB RedditClusteringP2P\\n      config: default\\n      split: test\\n      revision: 385e3cb46b4cfa89021f56c4380204149d0efe33\\n    metrics:\\n    - type: v_measure\\n      value: 63.14989421688691\\n    - type: v_measures\\n      value: [0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534, 0.673210410652684, 0.6825035243902045, 0.6275126414823813, 0.40001836573261074, 0.711458797825346, 0.6212317163461291, 0.4113635660304527, 0.7394060043565659, 0.6969073197749642, 0.7513770750973534]\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB SCIDOCS\\n      config: default\\n      split: test\\n      revision: f8c2fcf00f625baaa80f62ec5bd9e1fff3b8ae88\\n    metrics:\\n    - type: map_at_1\\n      value: 4.4830000000000005\\n    - type: map_at_10\\n      value: 11.04\\n    - type: map_at_100\\n      value: 12.764000000000001\\n    - type: map_at_1000\\n      value: 13.04\\n    - type: map_at_20\\n      value: 11.953\\n    - type: map_at_3\\n      value: 8.125\\n    - type: map_at_5\\n      value: 9.565999999999999\\n    - type: mrr_at_1\\n      value: 22.1\\n    - type: mrr_at_10\\n      value: 32.494\\n    - type: mrr_at_100\\n      value: 33.525\\n    - type: mrr_at_1000\\n      value: 33.596\\n    - type: mrr_at_20\\n      value: 33.089\\n    - type: mrr_at_3\\n      value: 29.416999999999998\\n    - type: mrr_at_5\\n      value: 31.267\\n    - type: ndcg_at_1\\n      value: 22.1\\n    - type: ndcg_at_10\\n      value: 18.587\\n    - type: ndcg_at_100\\n      value: 25.482\\n    - type: ndcg_at_1000\\n      value: 30.581999999999997\\n    - type: ndcg_at_20\\n      value: 21.077\\n    - type: ndcg_at_3\\n      value: 18.165\\n    - type: ndcg_at_5\\n      value: 15.676000000000002\\n    - type: precision_at_1\\n      value: 22.1\\n    - type: precision_at_10\\n      value: 9.48\\n    - type: precision_at_100\\n      value: 1.942\\n    - type: precision_at_1000\\n      value: 0.316\\n    - type: precision_at_20\\n      value: 6.175\\n    - type: precision_at_3\\n      value: 17.033\\n    - type: precision_at_5\\n      value: 13.719999999999999\\n    - type: recall_at_1\\n      value: 4.4830000000000005\\n    - type: recall_at_10\\n      value: 19.208\\n    - type: recall_at_100\\n      value: 39.417\\n    - type: recall_at_1000\\n      value: 64.235\\n    - type: recall_at_20\\n      value: 25.057000000000002\\n    - type: recall_at_3\\n      value: 10.348\\n    - type: recall_at_5\\n      value: 13.893\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB SICK-R\\n      config: default\\n      split: test\\n      revision: 20a6d6f312dd54037fe07a32d58e5e168867909d\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 83.50181312649208\\n    - type: cos_sim_spearman\\n      value: 79.92900705478993\\n    - type: euclidean_pearson\\n      value: 81.13482128094503\\n    - type: euclidean_spearman\\n      value: 79.92732266864367\\n    - type: manhattan_pearson\\n      value: 81.06702121654993\\n    - type: manhattan_spearman\\n      value: 79.86983106619135\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB STS12\\n      config: default\\n      split: test\\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 83.85431681906961\\n    - type: cos_sim_spearman\\n      value: 77.61671419416626\\n    - type: euclidean_pearson\\n      value: 81.30538320520961\\n    - type: euclidean_spearman\\n      value: 77.62096481461272\\n    - type: manhattan_pearson\\n      value: 81.2306021173407\\n    - type: manhattan_spearman\\n      value: 77.58386300715222\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB STS13\\n      config: default\\n      split: test\\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 84.98057702322754\\n    - type: cos_sim_spearman\\n      value: 86.13305071688859\\n    - type: euclidean_pearson\\n      value: 85.70903555966376\\n    - type: euclidean_spearman\\n      value: 86.13150222328171\\n    - type: manhattan_pearson\\n      value: 85.69380834788831\\n    - type: manhattan_spearman\\n      value: 86.10784739081191\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB STS14\\n      config: default\\n      split: test\\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 83.43368314724589\\n    - type: cos_sim_spearman\\n      value: 81.26767916144169\\n    - type: euclidean_pearson\\n      value: 83.23234690932492\\n    - type: euclidean_spearman\\n      value: 81.2671726214706\\n    - type: manhattan_pearson\\n      value: 83.2381239261109\\n    - type: manhattan_spearman\\n      value: 81.27674961470714\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB STS15\\n      config: default\\n      split: test\\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 86.8637546411748\\n    - type: cos_sim_spearman\\n      value: 88.25330888676139\\n    - type: euclidean_pearson\\n      value: 87.81194589390417\\n    - type: euclidean_spearman\\n      value: 88.25258669625579\\n    - type: manhattan_pearson\\n      value: 87.8131866998459\\n    - type: manhattan_spearman\\n      value: 88.26523268929576\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB STS16\\n      config: default\\n      split: test\\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 83.83129743147286\\n    - type: cos_sim_spearman\\n      value: 85.73732687732624\\n    - type: euclidean_pearson\\n      value: 85.18051277328075\\n    - type: euclidean_spearman\\n      value: 85.73565846174445\\n    - type: manhattan_pearson\\n      value: 85.179029651079\\n    - type: manhattan_spearman\\n      value: 85.75709685404729\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB STS17 (en-en)\\n      config: en-en\\n      split: test\\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 87.04715794253148\\n    - type: cos_sim_spearman\\n      value: 87.61577496386343\\n    - type: euclidean_pearson\\n      value: 88.34713614361046\\n    - type: euclidean_spearman\\n      value: 87.56541901567275\\n    - type: manhattan_pearson\\n      value: 88.26010824585985\\n    - type: manhattan_spearman\\n      value: 87.35211736948182\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB STS22 (en)\\n      config: en\\n      split: test\\n      revision: eea2b4fe26a775864c896887d910b76a8098ad3f\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 62.36160793264433\\n    - type: cos_sim_spearman\\n      value: 66.07767480051893\\n    - type: euclidean_pearson\\n      value: 66.4716471304865\\n    - type: euclidean_spearman\\n      value: 66.03999286501872\\n    - type: manhattan_pearson\\n      value: 66.46197824372902\\n    - type: manhattan_spearman\\n      value: 65.82936468127227\\n  - task:\\n      type: STS\\n    dataset:\\n      type: None\\n      name: MTEB STSBenchmark\\n      config: default\\n      split: test\\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 85.27768996785856\\n    - type: cos_sim_spearman\\n      value: 86.96704639052885\\n    - type: euclidean_pearson\\n      value: 86.48753189555983\\n    - type: euclidean_spearman\\n      value: 86.96981285751171\\n    - type: manhattan_pearson\\n      value: 86.49262465015401\\n    - type: manhattan_spearman\\n      value: 86.95378609580054\\n  - task:\\n      type: Reranking\\n    dataset:\\n      type: None\\n      name: MTEB SciDocsRR\\n      config: default\\n      split: test\\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\\n    metrics:\\n    - type: map\\n      value: 81.52012853393428\\n    - type: mrr\\n      value: 94.70817671798063\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB SciFact\\n      config: default\\n      split: test\\n      revision: 0228b52cf27578f30900b9e5271d331663a030d7\\n    metrics:\\n    - type: map_at_1\\n      value: 55.344\\n    - type: map_at_10\\n      value: 64.82900000000001\\n    - type: map_at_100\\n      value: 65.42\\n    - type: map_at_1000\\n      value: 65.443\\n    - type: map_at_20\\n      value: 65.2\\n    - type: map_at_3\\n      value: 61.8\\n    - type: map_at_5\\n      value: 63.510999999999996\\n    - type: mrr_at_1\\n      value: 58.333\\n    - type: mrr_at_10\\n      value: 66.24600000000001\\n    - type: mrr_at_100\\n      value: 66.742\\n    - type: mrr_at_1000\\n      value: 66.762\\n    - type: mrr_at_20\\n      value: 66.549\\n    - type: mrr_at_3\\n      value: 64.056\\n    - type: mrr_at_5\\n      value: 65.372\\n    - type: ndcg_at_1\\n      value: 58.333\\n    - type: ndcg_at_10\\n      value: 69.626\\n    - type: ndcg_at_100\\n      value: 72.236\\n    - type: ndcg_at_1000\\n      value: 72.872\\n    - type: ndcg_at_20\\n      value: 70.864\\n    - type: ndcg_at_3\\n      value: 64.50399999999999\\n    - type: ndcg_at_5\\n      value: 67.07600000000001\\n    - type: precision_at_1\\n      value: 58.333\\n    - type: precision_at_10\\n      value: 9.4\\n    - type: precision_at_100\\n      value: 1.073\\n    - type: precision_at_1000\\n      value: 0.11299999999999999\\n    - type: precision_at_20\\n      value: 4.983\\n    - type: precision_at_3\\n      value: 25.222\\n    - type: precision_at_5\\n      value: 16.8\\n    - type: recall_at_1\\n      value: 55.344\\n    - type: recall_at_10\\n      value: 82.789\\n    - type: recall_at_100\\n      value: 94.6\\n    - type: recall_at_1000\\n      value: 99.667\\n    - type: recall_at_20\\n      value: 87.533\\n    - type: recall_at_3\\n      value: 69.18299999999999\\n    - type: recall_at_5\\n      value: 75.622\\n  - task:\\n      type: PairClassification\\n    dataset:\\n      type: None\\n      name: MTEB SprintDuplicateQuestions\\n      config: default\\n      split: test\\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\\n    metrics:\\n    - type: cos_sim_accuracy\\n      value: 99.69405940594059\\n    - type: cos_sim_ap\\n      value: 92.03642221694545\\n    - type: cos_sim_f1\\n      value: 84.06395048994327\\n    - type: cos_sim_precision\\n      value: 86.79446219382322\\n    - type: cos_sim_recall\\n      value: 81.5\\n    - type: dot_accuracy\\n      value: 99.6930693069307\\n    - type: dot_ap\\n      value: 91.9971441434875\\n    - type: dot_f1\\n      value: 83.8006230529595\\n    - type: dot_precision\\n      value: 87.14902807775377\\n    - type: dot_recall\\n      value: 80.7\\n    - type: euclidean_accuracy\\n      value: 99.69504950495049\\n    - type: euclidean_ap\\n      value: 92.03626548389335\\n    - type: euclidean_f1\\n      value: 84.10732714138285\\n    - type: euclidean_precision\\n      value: 86.88699360341151\\n    - type: euclidean_recall\\n      value: 81.5\\n    - type: manhattan_accuracy\\n      value: 99.69504950495049\\n    - type: manhattan_ap\\n      value: 92.02049659660081\\n    - type: manhattan_f1\\n      value: 84.34959349593495\\n    - type: manhattan_precision\\n      value: 85.74380165289256\\n    - type: manhattan_recall\\n      value: 83.0\\n    - type: max_accuracy\\n      value: 99.69504950495049\\n    - type: max_ap\\n      value: 92.03642221694545\\n    - type: max_f1\\n      value: 84.34959349593495\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB StackExchangeClustering\\n      config: default\\n      split: test\\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\\n    metrics:\\n    - type: v_measure\\n      value: 67.04916654680977\\n    - type: v_measures\\n      value: [0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096, 0.707614120277991, 0.694974842783697, 0.5756359888519659, 0.6964499615297283, 0.6547764033608466, 0.6448470247319567, 0.6263766967145058, 0.7139286894225703, 0.6737195749489034, 0.6824504575459811, 0.7667603743275774, 0.7595788549615426, 0.7086156082505461, 0.6624140136843005, 0.6136884209896801, 0.6717953455355791, 0.6494834308652331, 0.6507885275711466, 0.6382769468968572, 0.6556052416453325, 0.6700496626301571, 0.6424264693175464, 0.6400679099051025, 0.7118398877792876, 0.6501271821744096]\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB StackExchangeClusteringP2P\\n      config: default\\n      split: test\\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\\n    metrics:\\n    - type: v_measure\\n      value: 33.36641413495258\\n    - type: v_measures\\n      value: [0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235, 0.3245963448931168, 0.31882294716748927, 0.31975204745764507, 0.30752650651575314, 0.3191185767616115, 0.35880812225202774, 0.3427515820677152, 0.344097881083346, 0.35390675395072985, 0.3472606513458235]\\n  - task:\\n      type: Reranking\\n    dataset:\\n      type: None\\n      name: MTEB StackOverflowDupQuestions\\n      config: default\\n      split: test\\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\\n    metrics:\\n    - type: map\\n      value: 51.19282080158746\\n    - type: mrr\\n      value: 51.871100713012474\\n  - task:\\n      type: Summarization\\n    dataset:\\n      type: None\\n      name: MTEB SummEval\\n      config: default\\n      split: test\\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\\n    metrics:\\n    - type: cos_sim_pearson\\n      value: 31.437664703708485\\n    - type: cos_sim_spearman\\n      value: 31.391119208581575\\n    - type: dot_pearson\\n      value: 31.19925970504054\\n    - type: dot_spearman\\n      value: 31.38087224016694\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB TRECCOVID\\n      config: default\\n      split: test\\n      revision: bb9466bac8153a0349341eb1b22e06409e78ef4e\\n    metrics:\\n    - type: map_at_1\\n      value: 0.249\\n    - type: map_at_10\\n      value: 2.163\\n    - type: map_at_100\\n      value: 13.242999999999999\\n    - type: map_at_1000\\n      value: 30.866\\n    - type: map_at_20\\n      value: 3.9539999999999997\\n    - type: map_at_3\\n      value: 0.718\\n    - type: map_at_5\\n      value: 1.169\\n    - type: mrr_at_1\\n      value: 96.0\\n    - type: mrr_at_10\\n      value: 98.0\\n    - type: mrr_at_100\\n      value: 98.0\\n    - type: mrr_at_1000\\n      value: 98.0\\n    - type: mrr_at_20\\n      value: 98.0\\n    - type: mrr_at_3\\n      value: 98.0\\n    - type: mrr_at_5\\n      value: 98.0\\n    - type: ndcg_at_1\\n      value: 92.0\\n    - type: ndcg_at_10\\n      value: 84.147\\n    - type: ndcg_at_100\\n      value: 65.143\\n    - type: ndcg_at_1000\\n      value: 56.038\\n    - type: ndcg_at_20\\n      value: 80.869\\n    - type: ndcg_at_3\\n      value: 89.11200000000001\\n    - type: ndcg_at_5\\n      value: 87.199\\n    - type: precision_at_1\\n      value: 96.0\\n    - type: precision_at_10\\n      value: 87.8\\n    - type: precision_at_100\\n      value: 66.72\\n    - type: precision_at_1000\\n      value: 24.684\\n    - type: precision_at_20\\n      value: 84.3\\n    - type: precision_at_3\\n      value: 94.0\\n    - type: precision_at_5\\n      value: 91.2\\n    - type: recall_at_1\\n      value: 0.249\\n    - type: recall_at_10\\n      value: 2.284\\n    - type: recall_at_100\\n      value: 16.025\\n    - type: recall_at_1000\\n      value: 52.068999999999996\\n    - type: recall_at_20\\n      value: 4.3180000000000005\\n    - type: recall_at_3\\n      value: 0.738\\n    - type: recall_at_5\\n      value: 1.212\\n  - task:\\n      type: Retrieval\\n    dataset:\\n      type: None\\n      name: MTEB Touche2020\\n      config: default\\n      split: test\\n      revision: a34f9a33db75fa0cbb21bb5cfc3dae8dc8bec93f\\n    metrics:\\n    - type: map_at_1\\n      value: 3.4520000000000004\\n    - type: map_at_10\\n      value: 13.045000000000002\\n    - type: map_at_100\\n      value: 19.442\\n    - type: map_at_1000\\n      value: 21.09\\n    - type: map_at_20\\n      value: 15.667\\n    - type: map_at_3\\n      value: 7.409000000000001\\n    - type: map_at_5\\n      value: 9.73\\n    - type: mrr_at_1\\n      value: 46.939\\n    - type: mrr_at_10\\n      value: 60.295\\n    - type: mrr_at_100\\n      value: 60.904\\n    - type: mrr_at_1000\\n      value: 60.919000000000004\\n    - type: mrr_at_20\\n      value: 60.77\\n    - type: mrr_at_3\\n      value: 58.50300000000001\\n    - type: mrr_at_5\\n      value: 59.014\\n    - type: ndcg_at_1\\n      value: 44.897999999999996\\n    - type: ndcg_at_10\\n      value: 31.911\\n    - type: ndcg_at_100\\n      value: 41.945\\n    - type: ndcg_at_1000\\n      value: 53.181999999999995\\n    - type: ndcg_at_20\\n      value: 31.505\\n    - type: ndcg_at_3\\n      value: 39.745000000000005\\n    - type: ndcg_at_5\\n      value: 35.528999999999996\\n    - type: precision_at_1\\n      value: 46.939\\n    - type: precision_at_10\\n      value: 26.531\\n    - type: precision_at_100\\n      value: 8.163\\n    - type: precision_at_1000\\n      value: 1.559\\n    - type: precision_at_20\\n      value: 19.387999999999998\\n    - type: precision_at_3\\n      value: 40.136\\n    - type: precision_at_5\\n      value: 33.878\\n    - type: recall_at_1\\n      value: 3.4520000000000004\\n    - type: recall_at_10\\n      value: 18.899\\n    - type: recall_at_100\\n      value: 50.207\\n    - type: recall_at_1000\\n      value: 83.871\\n    - type: recall_at_20\\n      value: 26.756999999999998\\n    - type: recall_at_3\\n      value: 8.729000000000001\\n    - type: recall_at_5\\n      value: 12.084999999999999\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB ToxicConversationsClassification\\n      config: default\\n      split: test\\n      revision: edfaf9da55d3dd50d43143d90c1ac476895ae6de\\n    metrics:\\n    - type: accuracy\\n      value: 67.4560546875\\n    - type: ap\\n      value: 12.720403845355294\\n    - type: f1\\n      value: 51.76062666567839\\n  - task:\\n      type: Classification\\n    dataset:\\n      type: None\\n      name: MTEB TweetSentimentExtractionClassification\\n      config: default\\n      split: test\\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\\n    metrics:\\n    - type: accuracy\\n      value: 62.36276174306734\\n    - type: f1\\n      value: 62.69956906934332\\n  - task:\\n      type: Clustering\\n    dataset:\\n      type: None\\n      name: MTEB TwentyNewsgroupsClustering\\n      config: default\\n      split: test\\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\\n    metrics:\\n    - type: v_measure\\n      value: 49.473492910233965\\n    - type: v_measures\\n      value: [0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281, 0.48829262296803855, 0.49853262011854643, 0.48457750518082765, 0.5020774116970983, 0.5001897357021557, 0.4702417082210781, 0.4763216048226018, 0.49932879417585735, 0.5129628835129124, 0.514824404624281]\\n  - task:\\n      type: PairClassification\\n    dataset:\\n      type: None\\n      name: MTEB TwitterSemEval2015\\n      config: default\\n      split: test\\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\\n    metrics:\\n    - type: cos_sim_accuracy\\n      value: 85.75430649102938\\n    - type: cos_sim_ap\\n      value: 73.62842656477649\\n    - type: cos_sim_f1\\n      value: 67.76023680315738\\n    - type: cos_sim_precision\\n      value: 63.61741547012506\\n    - type: cos_sim_recall\\n      value: 72.4802110817942\\n    - type: dot_accuracy\\n      value: 85.7423854085951\\n    - type: dot_ap\\n      value: 73.59147637253723\\n    - type: dot_f1\\n      value: 67.69498693867396\\n    - type: dot_precision\\n      value: 64.03859731701577\\n    - type: dot_recall\\n      value: 71.79419525065963\\n    - type: euclidean_accuracy\\n      value: 85.7423854085951\\n    - type: euclidean_ap\\n      value: 73.6288990409654\\n    - type: euclidean_f1\\n      value: 67.80415430267064\\n    - type: euclidean_precision\\n      value: 63.79711493718009\\n    - type: euclidean_recall\\n      value: 72.34828496042216\\n    - type: manhattan_accuracy\\n      value: 85.69470107885796\\n    - type: manhattan_ap\\n      value: 73.49219614602531\\n    - type: manhattan_f1\\n      value: 67.60809797550613\\n    - type: manhattan_precision\\n      value: 64.22127255460589\\n    - type: manhattan_recall\\n      value: 71.37203166226914\\n    - type: max_accuracy\\n      value: 85.75430649102938\\n    - type: max_ap\\n      value: 73.6288990409654\\n    - type: max_f1\\n      value: 67.80415430267064\\n  - task:\\n      type: PairClassification\\n    dataset:\\n      type: None\\n      name: MTEB TwitterURLCorpus\\n      config: default\\n      split: test\\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\\n    metrics:\\n    - type: cos_sim_accuracy\\n      value: 89.08293553770326\\n    - type: cos_sim_ap\\n      value: 86.21246419992926\\n    - type: cos_sim_f1\\n      value: 78.49922526377924\\n    - type: cos_sim_precision\\n      value: 75.35769939084857\\n    - type: cos_sim_recall\\n      value: 81.9140745303357\\n    - type: dot_accuracy\\n      value: 89.08681647067955\\n    - type: dot_ap\\n      value: 86.19733517196862\\n    - type: dot_f1\\n      value: 78.51132446157838\\n    - type: dot_precision\\n      value: 75.70233755093287\\n    - type: dot_recall\\n      value: 81.53680320295658\\n    - type: euclidean_accuracy\\n      value: 89.07517367175069\\n    - type: euclidean_ap\\n      value: 86.21198725320203\\n    - type: euclidean_f1\\n      value: 78.49867139061116\\n    - type: euclidean_precision\\n      value: 75.38276155372839\\n    - type: euclidean_recall\\n      value: 81.88327687095781\\n    - type: manhattan_accuracy\\n      value: 89.0538285403811\\n    - type: manhattan_ap\\n      value: 86.17785515765131\\n    - type: manhattan_f1\\n      value: 78.48184098593084\\n    - type: manhattan_precision\\n      value: 74.34396308285694\\n    - type: manhattan_recall\\n      value: 83.10748383122882\\n    - type: max_accuracy\\n      value: 89.08681647067955\\n    - type: max_ap\\n      value: 86.21246419992926\\n    - type: max_f1\\n      value: 78.51132446157838\\nlicense: apache-2.0\\nlanguage:\\n- en\\nbase_model:\\n- answerdotai/ModernBERT-base\\n- nomic-ai/modernbert-embed-unsupervised\\n---\\n\\n# ModernBERT Embed\\n\\n[![image/png](modernbertembed.png)](https://huggingface.co/nomic-ai/modernbert-embed-base)\\n\\nModernBERT Embed is an embedding model trained from [ModernBERT-base](https://huggingface.co/answerdotai/ModernBERT-base), bringing the new advances of ModernBERT to embeddings!\\n\\nTrained on the [Nomic Embed](https://arxiv.org/abs/2402.01613) weakly-supervised and supervised datasets, `modernbert-embed` also supports Matryoshka Representation Learning dimensions of 256, reducing memory by 3x with minimal performance loss.\\n\\n## Performance\\n\\n| Model                 | Dimensions | Average (56) | Classification (12) | Clustering (11) | Pair Classification (3) | Reranking (4) | Retrieval (15) | STS (10)  | Summarization (1) |\\n|-----------------------|------------|--------------|---------------------|-----------------|-------------------------|---------------|----------------|-----------|------------------|\\n| nomic-embed-text-v1   | 768        | 62.4         | 74.1                | 43.9            | **85.2**                | 55.7          | 52.8           | 82.1      | 30.1             |\\n| nomic-embed-text-v1.5 | 768        | 62.28        | 73.55               | 43.93           | 84.61                   | 55.78         | **53.01**      | **81.94** | 30.4             |\\n| modernbert-embed-base      | 768        | **62.62**    | **74.31**           | **44.98**       | 83.96                   | **56.42**     | 52.89          | 81.78     | **31.39**        |\\n| nomic-embed-text-v1.5 | 256        | 61.04        | 72.1                | 43.16           | 84.09                   | 55.18         | 50.81          | 81.34     |   30.05               |\\n| modernbert-embed-base      | 256        | 61.17        | 72.40               | 43.82           | 83.45                   | 55.69         | 50.62          | 81.12     | 31.27            |\\n\\n\\n\\n## Usage\\n\\nYou can use these models directly with the transformers library. Until the next transformers release, doing so requires installing `transformers` from `main`:\\n\\n```bash\\npip install git+https://github.com/huggingface/transformers.git\\n```\\n\\nReminder, this model is trained similarly to Nomic Embed and **REQUIRES** prefixes to be added to the input. For more information, see the instructions in [Nomic Embed](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5#task-instruction-prefixes).\\n\\nMost use cases, adding `search_query: ` to the query and `search_document: ` to the documents will be sufficient.\\n\\n### Sentence Transformers\\n\\n```python\\nfrom sentence_transformers import SentenceTransformer\\n\\nmodel = SentenceTransformer(\"nomic-ai/modernbert-embed-base\")\\n\\nquery_embeddings = model.encode([\\n    \"search_query: What is TSNE?\",\\n    \"search_query: Who is Laurens van der Maaten?\",\\n])\\ndoc_embeddings = model.encode([\\n    \"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\\n])\\nprint(query_embeddings.shape, doc_embeddings.shape)\\n# (2, 768) (1, 768)\\n\\nsimilarities = model.similarity(query_embeddings, doc_embeddings)\\nprint(similarities)\\n# tensor([[0.7214],\\n#         [0.3260]])\\n```\\n\\n<details><summary>Click to see Sentence Transformers usage with Matryoshka Truncation</summary>\\n\\nIn Sentence Transformers, you can truncate embeddings to a smaller dimension by using the `truncate_dim` parameter when loading the `SentenceTransformer` model.\\n\\n```python\\nfrom sentence_transformers import SentenceTransformer\\n\\nmodel = SentenceTransformer(\"nomic-ai/modernbert-embed-base\", truncate_dim=256)\\n\\nquery_embeddings = model.encode([\\n    \"search_query: What is TSNE?\",\\n    \"search_query: Who is Laurens van der Maaten?\",\\n])\\ndoc_embeddings = model.encode([\\n    \"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\\n])\\nprint(query_embeddings.shape, doc_embeddings.shape)\\n# (2, 256) (1, 256)\\n\\nsimilarities = model.similarity(query_embeddings, doc_embeddings)\\nprint(similarities)\\n# tensor([[0.7759],\\n#         [0.3419]])\\n```\\n\\nNote the small differences compared to the full 768-dimensional similarities.\\n\\n</details>\\n\\n### Transformers\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\nfrom transformers import AutoTokenizer, AutoModel\\n\\n\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]\\n    input_mask_expanded = (\\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    )\\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\\n        input_mask_expanded.sum(1), min=1e-9\\n    )\\n\\n\\nqueries = [\"search_query: What is TSNE?\", \"search_query: Who is Laurens van der Maaten?\"]\\ndocuments = [\"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\"]\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/modernbert-embed-base\")\\nmodel = AutoModel.from_pretrained(\"nomic-ai/modernbert-embed-base\")\\n\\nencoded_queries = tokenizer(queries, padding=True, truncation=True, return_tensors=\"pt\")\\nencoded_documents = tokenizer(documents, padding=True, truncation=True, return_tensors=\"pt\")\\n\\nwith torch.no_grad():\\n    queries_outputs = model(**encoded_queries)\\n    documents_outputs = model(**encoded_documents)\\n\\nquery_embeddings = mean_pooling(queries_outputs, encoded_queries[\"attention_mask\"])\\nquery_embeddings = F.normalize(query_embeddings, p=2, dim=1)\\ndoc_embeddings = mean_pooling(documents_outputs, encoded_documents[\"attention_mask\"])\\ndoc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)\\nprint(query_embeddings.shape, doc_embeddings.shape)\\n# torch.Size([2, 768]) torch.Size([1, 768])\\n\\nsimilarities = query_embeddings @ doc_embeddings.T\\nprint(similarities)\\n# tensor([[0.7214],\\n#         [0.3260]])\\n```\\n\\n<details><summary>Click to see Transformers usage with Matryoshka Truncation</summary>\\n\\nIn `transformers`, you can truncate embeddings to a smaller dimension by slicing the mean pooled embeddings, prior to normalization.\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\nfrom transformers import AutoTokenizer, AutoModel\\n\\n\\ndef mean_pooling(model_output, attention_mask):\\n    token_embeddings = model_output[0]\\n    input_mask_expanded = (\\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\n    )\\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\\n        input_mask_expanded.sum(1), min=1e-9\\n    )\\n\\n\\nqueries = [\"search_query: What is TSNE?\", \"search_query: Who is Laurens van der Maaten?\"]\\ndocuments = [\"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\"]\\n\\ntokenizer = AutoTokenizer.from_pretrained(\".\")\\nmodel = AutoModel.from_pretrained(\".\")\\ntruncate_dim = 256\\n\\nencoded_queries = tokenizer(queries, padding=True, truncation=True, return_tensors=\"pt\")\\nencoded_documents = tokenizer(documents, padding=True, truncation=True, return_tensors=\"pt\")\\n\\nwith torch.no_grad():\\n    queries_outputs = model(**encoded_queries)\\n    documents_outputs = model(**encoded_documents)\\n\\nquery_embeddings = mean_pooling(queries_outputs, encoded_queries[\"attention_mask\"])\\nquery_embeddings = query_embeddings[:, :truncate_dim]\\nquery_embeddings = F.normalize(query_embeddings, p=2, dim=1)\\ndoc_embeddings = mean_pooling(documents_outputs, encoded_documents[\"attention_mask\"])\\ndoc_embeddings = doc_embeddings[:, :truncate_dim]\\ndoc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)\\nprint(query_embeddings.shape, doc_embeddings.shape)\\n# torch.Size([2, 256]) torch.Size([1, 256])\\n\\nsimilarities = query_embeddings @ doc_embeddings.T\\nprint(similarities)\\n# tensor([[0.7759],\\n#         [0.3419]])\\n```\\n\\nNote the small differences compared to the full 768-dimensional similarities.\\n\\n</details>\\n\\n### Transformers.js\\n\\nIf you haven\\'t already, you can install the [Transformers.js](https://huggingface.co/docs/transformers.js) JavaScript library from [NPM](https://www.npmjs.com/package/@huggingface/transformers) using:\\n```bash\\nnpm i @huggingface/transformers\\n```\\n\\nThen, you can compute embeddings as follows:\\n\\n```javascript\\nimport { pipeline, matmul } from \\'@huggingface/transformers\\';\\n\\n// Create a feature extraction pipeline\\nconst extractor = await pipeline(\\n  \"feature-extraction\",\\n  \"nomic-ai/modernbert-embed-base\",\\n  { dtype: \"fp32\" }, // Supported options: \"fp32\", \"fp16\", \"q8\", \"q4\", \"q4f16\"\\n);\\n\\n// Embed queries and documents\\nconst query_embeddings = await extractor([\\n    \"search_query: What is TSNE?\",\\n    \"search_query: Who is Laurens van der Maaten?\",\\n  ], { pooling: \"mean\", normalize: true },\\n);\\nconst doc_embeddings = await extractor([\\n    \"search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\\n  ], { pooling: \"mean\", normalize: true },\\n);\\n\\n// Compute similarity scores\\nconst similarities = await matmul(query_embeddings, doc_embeddings.transpose(1, 0));\\nconsole.log(similarities.tolist()); // [[0.721383273601532], [0.3259955644607544]]\\n```\\n\\n\\n## Training\\n\\nClick the Nomic Atlas map below to visualize a 5M sample of our contrastive pretraining data!\\n\\n[![image/webp](https://cdn-uploads.huggingface.co/production/uploads/607997c83a565c15675055b3/pjhJhuNyRfPagRd_c_iUz.webp)](https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample)\\n\\nWe train our embedder using a multi-stage training pipeline. Starting from a long-context [BERT model](https://huggingface.co/nomic-ai/nomic-bert-2048),\\nthe first unsupervised contrastive stage trains on a dataset generated from weakly related text pairs, such as question-answer pairs from forums like StackExchange and Quora, title-body pairs from Amazon reviews, and summarizations from news articles.\\n\\nIn the second finetuning stage, higher quality labeled datasets such as search queries and answers from web searches are leveraged. Data curation and hard-example mining is crucial in this stage.\\n\\nFor more details, see the Nomic Embed [Technical Report](https://static.nomic.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf) and corresponding [blog post](https://blog.nomic.ai/posts/nomic-embed-text-v1).\\n\\nTraining data to train the models is released in its entirety. For more details, see the `contrastors` [repository](https://github.com/nomic-ai/contrastors)\\n\\n\\n## Join the Nomic Community\\n\\n- Nomic: [https://nomic.ai](https://nomic.ai)\\n- Discord: [https://discord.gg/myY5YDR8z8](https://discord.gg/myY5YDR8z8)\\n- Twitter: [https://twitter.com/nomic_ai](https://twitter.com/nomic_ai)\\n\\n## Citation\\n\\nIf you find the model, dataset, or training code useful, please cite our work\\n\\n```bibtex\\n@misc{nussbaum2024nomic,\\n      title={Nomic Embed: Training a Reproducible Long Context Text Embedder}, \\n      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\\n      year={2024},\\n      eprint={2402.01613},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n```'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=nomic-ai/modernbert-embed-base\n",
      "> RSP [{'title': 'Nomic AI Launches ModernBERT-Embed-Base, Trained on... | DeepNewz', 'url': 'https://deepnewz.com/ai-modeling/nomic-ai-launches-modernbert-embed-base-trained-on-235-million-documents-256-f4acadfc', 'content': 'Nomic AI Launches ModernBERT-Embed-Base, Trained on 235 Million Documents with Matryoshka Dimensions of 256 Nomic AI has announced the release of modernbert-embed-base, a new embedding model built on the recently launched ModernBERT. @nomic_ai has finetuned the new ModernBERT-base encoder model into a strong embedding model for search, classification, clustering and more! Last open-source model drop of 2024 ModernBERT trained on the Nomic Embed data stack - Supports variable sized Matryoshka embeddings 🪆 - Trained on 235M documents https://t.co/YQDwvEODuC Great base model @huggingface ! ModernBERT taking over the world, wonderful release from @nomic_ai 🔥 Bonus: Supports Matryoshka Representation Learning dimensions of 256, reducing memory by 3x with minimal performance loss 🤩 https://t.co/G9OskseOVd DeepSeek-AI Launches Open-Source VL2 Models with 3B, 16B, and 27B Parameters, Competing with GPT-4o and Claude 3.5', 'score': 0.9020676, 'raw_content': None}, {'title': 'nomic-ai/modernbert-embed-base - Hugging Face', 'url': 'https://huggingface.co/nomic-ai/modernbert-embed-base', 'content': 'ModernBERT Embed is an embedding model trained from ModernBERT-base, bringing the new advances of ModernBERT to embeddings! Trained on the Nomic Embed weakly-supervised and supervised datasets, modernbert-embed also supports Matryoshka Representation Learning dimensions of 256, reducing memory by 3x with minimal performance loss. Reminder, this model is trained similarly to Nomic Embed and REQUIRES prefixes to be added to the input. model = SentenceTransformer(\"nomic-ai/modernbert-embed-base\") query_embeddings = model.encode([ doc_embeddings = model.encode([ similarities = model.similarity(query_embeddings, doc_embeddings) Click to see Sentence Transformers usage with Matryoshka TruncationIn Sentence Transformers, you can truncate embeddings to a smaller dimension by using the truncate_dim parameter when loading the SentenceTransformer model. model = SentenceTransformer(\"nomic-ai/modernbert-embed-base\", truncate_dim=256) query_embeddings = model.encode([ doc_embeddings = model.encode([ similarities = model.similarity(query_embeddings, doc_embeddings) tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/modernbert-embed-base\") model = AutoModel.from_pretrained(\"nomic-ai/modernbert-embed-base\") \"nomic-ai/modernbert-embed-base\", Model tree for nomic-ai/modernbert-embed-base Space using nomic-ai/modernbert-embed-base 1', 'score': 0.90102744, 'raw_content': None}, {'title': '@tomaarsen on Hugging Face: \"That didn\\'t take long! Nomic AI has ...', 'url': 'https://huggingface.co/posts/tomaarsen/118953506218716', 'content': \"That didn't take long! Nomic AI has finetuned the new ModernBERT-base encoder model into a strong embedding model for search, classification, clustering and more! Details: 🤖 Based on ModernBERT-base with 149M parameters. 📊 Outperforms both nomic-embed-text-v1 and nomic-embed-text-v1.5 on MTEB!\", 'score': 0.8858788, 'raw_content': None}, {'title': 'nomic-ai/modernbert-embed-base-unsupervised - Hugging Face', 'url': 'https://huggingface.co/nomic-ai/modernbert-embed-base-unsupervised', 'content': 'nomic-ai/modernbert-embed-base-unsupervised · Hugging Face Models modernbert-embed-base-unsupervised Use this model ModernBERT-Embed-Unsupervised ModernBERT-Embed-Unsupervised modernbert-embed-unsupervised is the unsupervised checkpoint trained with the contrastors library for 1 epoch over the 235M weakly-supervised contrastive pairs curated in Nomic Embed. The modernbert-unsupervised model performs similarly to the nomic-embed-text-v1_unsup model | modernbert-embed-unsupervised | 60.03 | 72.11 | 44.34 | 82.78 | 55.0 | 47.05 | 80.33 | 31.2 | Model size This model does not have enough activity to be deployed to Inference API (serverless) yet. Model tree for nomic-ai/modernbert-embed-base-unsupervised Base model this model accuracy on MTEB AmazonCounterfactualClassification (en) ap on MTEB AmazonCounterfactualClassification (en) f1 on MTEB AmazonCounterfactualClassification (en) f1 on MTEB AmazonPolarityClassification accuracy on MTEB AmazonReviewsClassification (en) f1 on MTEB AmazonReviewsClassification (en) Models Datasets Spaces Pricing Docs', 'score': 0.8488849, 'raw_content': None}, {'title': 'Install ModernBERT Embed Locally - Great New RAG Model', 'url': 'https://www.youtube.com/watch?v=HcVav0IqZlk', 'content': 'This video shows how to locally install Nomic ModernBERT Embed is an embedding model trained from ModernBERT-base, bringing the new advances of ModernBERT to', 'score': 0.7769112, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=nomic-ai/modernbert-embed-base\n",
      "> RSP [{'title': 'Understanding Generative AI Content with Embedding Models - arXiv.org', 'url': 'https://arxiv.org/html/2408.10437v2', 'content': 'nomic-ai/nomic-embed-text-v1 ... When the model yielded a base and revised version of an abstract, we deleted the base version and used the revised. While the previous dataset ArXiv dataset is used to test if feature embedders can separate real and synthetic data, this one also tests if feature embedders can separate data according to the', 'score': 0.43194416, 'raw_content': None}, {'title': 'arXiv:2412.13663v1 [cs.CL] 18 Dec 2024', 'url': 'https://arxiv.org/pdf/2412.13663v1', 'content': 'ModernBERT has 22 and 28 layers for the base and large models, for a total parameter count of 149 and 395 million, respectively, striking the balance between downstream performance and hardware efficiency. ModernBERT base has a hidden size of 768 with a GLU expansion of 2,304, while large has a hidden size of 1,024 and GLU expansion of 5,248.', 'score': 0.42076674, 'raw_content': None}, {'title': 'Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for ...', 'url': 'https://arxiv.org/abs/2412.13663', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. arXiv:2412.13663 Help | Advanced Search arXiv identifier arXiv author ID In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI) Cite as:    arXiv:2412.13663 [cs.CL] (or arXiv:2412.13663v2 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.13663 From: Benjamin Clavié [view email] Access Paper: Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers?\", 'score': 0.3175749, 'raw_content': None}, {'title': 'Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for ...', 'url': 'https://arxiv.org/abs/2412.13663v1', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. arXiv:2412.13663v1 Help | Advanced Search arXiv author ID In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI) Cite as:    arXiv:2412.13663 [cs.CL] (or arXiv:2412.13663v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.13663 From: Benjamin Clavié [view email] Access Paper: Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers?\", 'score': 0.30784726, 'raw_content': None}, {'title': 'arXiv:2412.13663v2 [cs.CL] 19 Dec 2024', 'url': 'https://arxiv.org/pdf/2412.13663v2', 'content': 'els, ModernBERT-base and ModernBERT-large, which reach state-of-the-art overall performance against all existing encoder models on a wide vari-ety of downstream tasks. These results are achieved with considerably higher inference efficiency, pro-cessing sequences of 8192 tokens almost two times faster than previous models.', 'score': 0.3013317, 'raw_content': None}, {'title': 'Abstract - arXiv.org', 'url': 'https://arxiv.org/pdf/2405.05374', 'content': 'Size Base Model (Huggingface ID) Parameters (M) Embedding Dimension xs nreimers/MiniLM-L6-H384-uncased 23 384 s intfloat/e5-unsupervised-small 33 384 m intfloat/e5-unsupervised-base 110 764 m-long nomic-ai/nomic-embed-text-v1-unsupervised 137 768 l intfloat/e5-unsupervised-large 334 1024 Table 1: Breakdown of model architectures. 2020).', 'score': 0.30080768, 'raw_content': None}, {'title': 'Nomic Embed: Training a Reproducible Long Context Text Embedder - arXiv.org', 'url': 'https://arxiv.org/pdf/2402.01613v1', 'content': 'Nomic Embed: Training a Reproducible Long Context Text Embedder Zach Nussbaum zach@nomic.ai John X. Morris jack@nomic.ai jxm3@cornell.edu Brandon Duderstadt brandon@nomic.ai Andriy Mulyar andriy@nomic.ai Abstract This technical report describes the training of nomic-embed-text-v1, the first fully re-producible, open-source, open-weights, open-', 'score': 0.24943396, 'raw_content': None}, {'title': 'PDF', 'url': 'https://arxiv.org/pdf/2402.01613v1.pdf', 'content': 'Nomic Embed: Training a Reproducible Long Context Text Embedder Zach Nussbaum zach@nomic.ai John X. Morris jack@nomic.ai jxm3@cornell.edu Brandon Duderstadt brandon@nomic.ai Andriy Mulyar andriy@nomic.ai Abstract This technical report describes the training of nomic-embed-text-v1, the first fully re-producible, open-source, open-weights, open-', 'score': 0.24943396, 'raw_content': None}, {'title': 'Nomic Embed: Training a Reproducible Long Context Text Embedder - arXiv.org', 'url': 'https://arxiv.org/pdf/2402.01613', 'content': 'Nomic Embed: Training a Reproducible Long Context Text Embedder Zach Nussbaum zach@nomic.ai John X. Morris jack@nomic.ai jxm3@cornell.edu Brandon Duderstadt brandon@nomic.ai Andriy Mulyar andriy@nomic.ai Abstract This technical report describes the training of nomic-embed-text-v1, the first fully re-producible, open-source, open-weights, open-', 'score': 0.24943396, 'raw_content': None}, {'title': 'Nomic Embed: Training a Reproducible Long Context Text Embedder', 'url': 'https://web3.arxiv.org/pdf/2402.01613', 'content': 'Nomic Embed Jina Base V2 text-embedding-3-small text-embedding-ada Figure 1: Text Embedding Model Benchmarks. Ag-gregate performance of nomic-embed-text-v1, OpenAI text-embedding-ada, OpenAI text-embedding-3-small and jina-embedding-base-v2 on short and long con-text benchmarks. Nomic Embed is the only fully au-', 'score': 0.1839162, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2405.05374\n",
      "> RSP [Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models](http://arxiv.org/abs/2405.05374v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.13663\n",
      "> RSP [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](http://arxiv.org/abs/2412.13663v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2402.01613\n",
      "> RSP [Nomic Embed: Training a Reproducible Long Context Text Embedder](http://arxiv.org/abs/2402.01613v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2408.10437\n",
      "> RSP [Understanding Generative AI Content with Embedding Models](http://arxiv.org/abs/2408.10437v2)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'Nomic AI Launches ModernBERT-Embed-Base, Trained on... | DeepNewz', 'url': 'https://deepnewz.com/ai-modeling/nomic-ai-launches-modernbert-embed-base-trained-on-235-million-documents-256-f4acadfc', 'content': 'Nomic AI Launches ModernBERT-Embed-Base, Trained on 235 Million Documents with Matryoshka Dimensions of 256 Nomic AI has announced the release of modernbert-embed-base, a new embedding model built on the recently launched ModernBERT. @nomic_ai has finetuned the new ModernBERT-base encoder model into a strong embedding model for search, classification, clustering and more! Last open-source model drop of 2024 ModernBERT trained on the Nomic Embed data stack - Supports variable sized Matryoshka embeddings 🪆 - Trained on 235M documents https://t.co/YQDwvEODuC Great base model @huggingface ! ModernBERT taking over the world, wonderful release from @nomic_ai 🔥 Bonus: Supports Matryoshka Representation Learning dimensions of 256, reducing memory by 3x with minimal performance loss 🤩 https://t.co/G9OskseOVd DeepSeek-AI Launches Open-Source VL2 Models with 3B, 16B, and 27B Parameters, Competing with GPT-4o and Claude 3.5', 'score': 0.9020676, 'raw_content': None}, {'title': 'nomic-ai/modernbert-embed-base - Hugging Face', 'url': 'https://huggingface.co/nomic-ai/modernbert-embed-base', 'content': 'ModernBERT Embed is an embedding model trained from ModernBERT-base, bringing the new advances of ModernBERT to embeddings! Trained on the Nomic Embed weakly-supervised and supervised datasets, modernbert-embed also supports Matryoshka Representation Learning dimensions of 256, reducing memory by 3x with minimal performance loss. Reminder, this model is trained similarly to Nomic Embed and REQUIRES prefixes to be added to the input. model = SentenceTransformer(\"nomic-ai/modernbert-embed-base\") query_embeddings = model.encode([ doc_embeddings = model.encode([ similarities = model.similarity(query_embeddings, doc_embeddings) Click to see Sentence Transformers usage with Matryoshka TruncationIn Sentence Transformers, you can truncate embeddings to a smaller dimension by using the truncate_dim parameter when loading the SentenceTransformer model. model = SentenceTransformer(\"nomic-ai/modernbert-embed-base\", truncate_dim=256) query_embeddings = model.encode([ doc_embeddings = model.encode([ similarities = model.similarity(query_embeddings, doc_embeddings) tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/modernbert-embed-base\") model = AutoModel.from_pretrained(\"nomic-ai/modernbert-embed-base\") \"nomic-ai/modernbert-embed-base\", Model tree for nomic-ai/modernbert-embed-base Space using nomic-ai/modernbert-embed-base 1', 'score': 0.90102744, 'raw_content': None}, {'title': '@tomaarsen on Hugging Face: \"That didn\\'t take long! Nomic AI has ...', 'url': 'https://huggingface.co/posts/tomaarsen/118953506218716', 'content': \"That didn't take long! Nomic AI has finetuned the new ModernBERT-base encoder model into a strong embedding model for search, classification, clustering and more! Details: 🤖 Based on ModernBERT-base with 149M parameters. 📊 Outperforms both nomic-embed-text-v1 and nomic-embed-text-v1.5 on MTEB!\", 'score': 0.8858788, 'raw_content': None}, {'title': 'nomic-ai/modernbert-embed-base-unsupervised - Hugging Face', 'url': 'https://huggingface.co/nomic-ai/modernbert-embed-base-unsupervised', 'content': 'nomic-ai/modernbert-embed-base-unsupervised · Hugging Face Models modernbert-embed-base-unsupervised Use this model ModernBERT-Embed-Unsupervised ModernBERT-Embed-Unsupervised modernbert-embed-unsupervised is the unsupervised checkpoint trained with the contrastors library for 1 epoch over the 235M weakly-supervised contrastive pairs curated in Nomic Embed. The modernbert-unsupervised model performs similarly to the nomic-embed-text-v1_unsup model | modernbert-embed-unsupervised | 60.03 | 72.11 | 44.34 | 82.78 | 55.0 | 47.05 | 80.33 | 31.2 | Model size This model does not have enough activity to be deployed to Inference API (serverless) yet. Model tree for nomic-ai/modernbert-embed-base-unsupervised Base model this model accuracy on MTEB AmazonCounterfactualClassification (en) ap on MTEB AmazonCounterfactualClassification (en) f1 on MTEB AmazonCounterfactualClassification (en) f1 on MTEB AmazonPolarityClassification accuracy on MTEB AmazonReviewsClassification (en) f1 on MTEB AmazonReviewsClassification (en) Models Datasets Spaces Pricing Docs', 'score': 0.8488849, 'raw_content': None}, {'title': 'Install ModernBERT Embed Locally - Great New RAG Model', 'url': 'https://www.youtube.com/watch?v=HcVav0IqZlk', 'content': 'This video shows how to locally install Nomic ModernBERT Embed is an embedding model trained from ModernBERT-base, bringing the new advances of ModernBERT to', 'score': 0.7769112, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "pipeline_tag: sentence-similarity\n",
      "tags:\n",
      "- sentence-transformers\n",
      "- feature-extraction\n",
      "- sentence-similarity\n",
      "- mteb\n",
      "- transformers.js\n",
      "model-index:\n",
      "- name: binarize_False\n",
      "  results:\n",
      "  - task:\n",
      "      type: Classification\n",
      "    dataset:\n",
      "      type: None\n",
      "      name: MTEB AmazonCounterfactualClassification (en)\n",
      "      config: en\n",
      "      split: test\n",
      "      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n",
      "    metrics:\n",
      "    - type: accuracy\n",
      "      value: 78.13432835820896\n",
      "    - type: ap\n",
      "      value: 42.190424731303246\n",
      "    - type: f1\n",
      "      value: 72.34446401534811\n",
      "  - task:\n",
      "      type: Classification\n",
      "    dataset:\n",
      "      type: None\n",
      "      name: MTEB AmazonPolarityClassification\n",
      "      config: default\n",
      "      split: test\n",
      "      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n",
      "    metrics:\n",
      "    - type: accuracy\n",
      "      value: 93.093825\n",
      "    - type: ap\n",
      "      value: 90.03727505544286\n",
      "    - type: f1\n",
      "      value: 93.0874055138833\n",
      "  - task:\n",
      "      type: Classification\n",
      "    dataset:\n",
      "      type: None\n",
      "      name: MTEB AmazonReviewsClassification (en)\n",
      "      config: en\n",
      "      split: test\n",
      "      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n",
      "    metrics:\n",
      "    - type: accuracy\n",
      "      value: 48.428000000000004\n",
      "    - type: f1\n",
      "      value: 47.74311520203536\n",
      "  - task:\n",
      "      type: Retrieval\n",
      "    dataset:\n",
      "      type: None\n",
      "      name: MTEB ArguAna\n",
      "      config: default\n",
      "      split: test\n",
      "      revision: c22ab2a51041ffd869aaddef7af8d8215647e41a\n",
      "    metrics:\n",
      "    - type: map_at_1\n",
      "      value: 23.898\n",
      "    - type: map_at_10\n",
      "      value: 39.775\n",
      "    - type: map_at_100\n",
      "      value: 40.827000000000005\n",
      "    - type: map_at_1000\n",
      "      value: 40.837\n",
      "    - type: map_at_20\n",
      "      value: 40.604\n",
      "    - type: map_at_3\n",
      "      value: 34.519\n",
      "    - type: map_at_5\n",
      "      value: 37.307\n",
      "    - type: mrr_at_1\n",
      "      value: 24.395\n",
      "    - type: mrr_at_10\n",
      "      value: 39.963\n",
      "    - type: mrr_at_100\n",
      "      value: 41.014\n",
      "    - type: mrr_at_1000\n",
      "      value: 41.024\n",
      "    - type: mrr_at_20\n",
      "      value: 40.791\n",
      "    - type: mrr_at_3\n",
      "      value: 34.732\n",
      "    - type: mrr_at_5\n",
      "      value: 37.480999999999995\n",
      "    - type: ndcg_at_1\n",
      "      value: 23.898\n",
      "    - type: ndcg_at_10\n",
      "      value: 48.962\n",
      "    - type: ndcg_at_100\n",
      "      value: 53.386\n",
      "    - type: ndcg_at_1000\n",
      "      value: 53.634\n",
      "    - type: ndcg_at_20\n",
      "      value: 51.898999999999994\n",
      "    - type: ndcg_at_3\n",
      "      value: 38.034\n",
      "    - type: ndcg_at_5\n",
      "      value: 43.036\n",
      "    - type: precision_at_1\n",
      "      value: 23.898\n",
      "    - type: precision_at_10\n",
      "      value: 7.852\n",
      "    - type: precision_at_100\n",
      "      value: 0.9769999999999999\n",
      "    - type: precision_at_1000\n",
      "      value: 0.1\n",
      "    - type: precision_at_20\n",
      "      value: 4.4990000000000006\n",
      "    - type: precision_at_3\n",
      "      value: 16.073999999999998\n",
      "    - type: precision_at_5\n",
      "      value: 12.063\n",
      "    - type: recall_at_1\n",
      "      value: 23.898\n",
      "    - type: recall_at_10\n",
      "      value: 78.521\n",
      "    - type: recall_at_100\n",
      "      value: 97.724\n",
      "    - type: recall_at_1000\n",
      "      value: 99.644\n",
      "    - type: recall_at_20\n",
      "      value: 89.972\n",
      "    - type: recall_at_3\n",
      "      value: 48.222\n",
      "    - type: recall_at_5\n",
      "      value: 60.313\n",
      "  - task:\n",
      "      type: Clustering\n",
      "    dataset:\n",
      "      type: None\n",
      "      name: MTEB ArxivClusteringP2P\n",
      "      config: default\n",
      "      split: test\n",
      "      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n",
      "    metrics:\n",
      "    - type: v_measure\n",
      "      value: 47.69067314293749\n",
      "    - type: v_measures\n",
      "      value: [0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, 0.551824471283655, 0.5148077891863135, 0.29015461701593837, 0.4430422977323321, 0.40857527197890686, 0.3479983114229163, 0.27582001934225003, 0.29595564003512503, 0.22528676611734755, 0.3073271865740206, 1.0, 0.2749401557058413, 0.4953006738713271, 0.500982950617211, 0.490168788349858, 0.4924060458428337, 0.475176328561399, 0.47446297663785564, 0.46948807073019405, 0.4772028638329531, 0.48735189935310713, 0.48641173887761663, 0.5575029526712674, 0.5574020390232136, 0.5536066904942645, 0.5536169413675474, 0.5566938602585987, 0.5561143054736898, 0.561846457174852, 0.5511643632282144, 0.5514762015499715, \n",
      "> RSP * Trained on 235 million documents, enhancing robustness and diversity of learning.\n",
      "* Implements Matryoshka Representation Learning dimensions of 256 to optimize performance.\n",
      "* Reduces memory usage by 3x with minimal performance loss compared to other models.\n",
      "* Strong performance on various tasks such as search, classification, and clustering.\n",
      "* Outperforms predecessor models on standard metrics within the MTEB benchmark.\n",
      "* Supports both weakly-supervised and supervised datasets for versatile training capabilities.\n",
      "* Fine-tuned from ModernBERT-base with 149M parameters for improved embedding efficacy.\n",
      "* Provides features for sentence similarity and retrieval tasks, increasing practical applications.\n",
      "* Open-source model enabling community collaboration and development.\n",
      "* Designed for flexible input handling, requiring prefixes for better contextual understanding.\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=cognitivecomputations/Dolphin3.0-Llama3.1-8B\n",
      "> RSP {'model_id': 'cognitivecomputations/Dolphin3.0-Llama3.1-8B', 'created_at': '29 December 2024 at 18:37:00 UTC', 'downloads': 242, 'likes': 82, 'trending_score': 82, 'description': '---\\nlicense: llama3.1\\ndatasets:\\n- OpenCoder-LLM/opc-sft-stage1\\n- OpenCoder-LLM/opc-sft-stage2\\n- microsoft/orca-agentinstruct-1M-v1\\n- microsoft/orca-math-word-problems-200k\\n- NousResearch/hermes-function-calling-v1\\n- AI-MO/NuminaMath-CoT\\n- AI-MO/NuminaMath-TIR\\n- allenai/tulu-3-sft-mixture\\n- cognitivecomputations/dolphin-coder\\n- HuggingFaceTB/smoltalk\\n- cognitivecomputations/samantha-data\\n- m-a-p/CodeFeedback-Filtered-Instruction\\n- m-a-p/Code-Feedback\\nlanguage:\\n- en\\nbase_model:\\n- meta-llama/Llama-3.1-8B\\n---\\n\\n# Dolphin 3.0 Llama 3.1 8B 🐬\\nPart of the [Dolphin 3.0 Collection](https://huggingface.co/collections/cognitivecomputations/dolphin-30-677ab47f73d7ff66743979a3)\\n\\nCurated and trained by [Eric Hartford](https://huggingface.co/ehartford), [Ben Gitter](https://huggingface.co/bigstorm), [BlouseJury](https://huggingface.co/BlouseJury) and [Cognitive Computations](https://huggingface.co/cognitivecomputations)\\n\\n[![Discord](https://img.shields.io/discord/1156064224225808488?logo=Discord&logoColor=%23ffffff&label=Discord&link=https%3A%2F%2Fdiscord.gg%2FtCMkMDDHwm)](https://discord.gg/cognitivecomputations)\\nDiscord: https://discord.gg/cognitivecomputations\\n\\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/cNCs1TBD3FelWCJGkZ3cd.png\" width=\"600\" />\\n\\n## Sponsors\\nOur appreciation for the generous sponsors of Dolphin 3.0:\\n- [Crusoe Cloud](https://crusoe.ai/) - provided 16x L40s for training and evals\\n- [Akash](https://akash.network/) - provided on-demand 8x H100 for training\\n- [Lazarus](https://www.lazarusai.com/) - provided 16x H100 for training\\n- [Cerebras](https://cerebras.ai/) - provided excellent and fast inference services for data labeling\\n- [Andreessen Horowitz](https://a16z.com/) - provided a [grant](https://a16z.com/supporting-the-open-source-ai-community/) that make Dolphin 1.0 possible and enabled me to bootstrap my homelab\\n\\n## What is Dolphin?\\n\\nDolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models.  Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\\n\\nDolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini.  But these models present problems for businesses seeking to include AI in their products.\\n1) They maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break.\\n2) They maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on.\\n3) They maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application.\\n4) They can see all your queries and they can potentially use that data in ways you wouldn\\'t want.\\nDolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt.  You decide the alignment.  You have control of your data.  Dolphin does not impose its ethics or guidelines on you.  You are the one who decides the guidelines.\\n\\nDolphin belongs to YOU, it is your tool, an extension of your will.\\nJust as you are personally responsible for what you do with a knife, gun, fire, car, or the internet, you are the creator and originator of any content you generate with Dolphin.\\n\\nhttps://erichartford.com/uncensored-models\\n\\n## Chat Template\\n\\nWe use ChatML for the chat template.\\n\\n```\\n<|im_start|>system\\nYou are Dolphin, a helpful AI assistant.<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n```\\n\\n## System Prompt\\n\\nIn Dolphin, the system prompt is what you use to set the tone and alignment of the responses.  You can set a character, a mood, rules for its behavior, and it will try its best to follow them.\\n\\nMake sure to set the system prompt in order to set the tone and guidelines for the responses - Otherwise, it will act in a default way that might not be what you want.\\n\\nExample use of system prompt:\\n\\n```\\n<|im_start|>system\\nYou are Dolphin, a golang coding assistant.  you only code in golang.  If the user requests any other programming language, return the solution in golang instead.<|im_end|>\\n<|im_start|>user\\nPlease implement A* using python<|im_end|>\\n<|im_start|>assistant\\n```\\n\\n## Sample Outputs\\n\\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/C-r1X13UBjnUUNb0q2JLV.png\" width=\"600\" />\\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/4l3KAZiKej2ON7i35PsOa.png\" width=\"600\" />\\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/1ZalmR66LnwhEQQEFttlu.png\" width=\"600\" />\\n\\n## How to use\\n\\nThere are many ways to use a huggingface model including:\\n- ollama\\n- LM Studio\\n- Huggingface Transformers library\\n- vllm\\n- sglang\\n- tgi\\n\\n### ollama\\n- [Install ollama](https://ollama.com/download)\\n- ```ollama run hf.co/cognitivecomputations/Dolphin3.0-Llama3.1-8B-GGUF:Q4_0```\\n- ```/set system <your system prompt>```\\n\\n## Evals\\n\\nTBD\\n\\n## Appreciation\\n\\nRespect and thanks to the creators of the open source datasets that were used:\\n- [OpenCoder-LLM](https://huggingface.co/OpenCoder-LLM) (opc-sft-stage1, opc-sft-stage2)\\n- [microsoft](https://huggingface.co/OpenCoder-LLM) (orca-agentinstruct-1M-v1, orca-math-word-problems-200k)\\n- [NousResearch](https://huggingface.co/NousResearch) (hermes-function-calling-v1)\\n- [AI-MO](https://huggingface.co/AI-MO) (NuminaMath-CoT, NuminaMath-TIR)\\n- [allenai](https://huggingface.co/allenai) (tulu-3-sft-mixture)\\n- [HuggingFaceTB](https://huggingface.co/HuggingFaceTB) (smoltalk)\\n- [m-a-p](https://huggingface.co/m-a-p) (CodeFeedback-Filtered-Instruction, Code-Feedback)\\n\\nSpecial thanks to \\n- Meta, Qwen, and OpenCoder, who wrote papers and published models that were instrumental in creating Dolphin 3.0.\\n- [RLHFlow](https://huggingface.co/RLHFlow) for the excellent reward model used to filter the datasets\\n- Deepseek, for the ridiculously fast Deepseek-V3 that we used to augment the data.\\n\\n\\n'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=cognitivecomputations/Dolphin3.0-Llama3.1-8B\n",
      "> RSP [{'title': 'Dolphin3.0 Llama3.1 8B by cognitivecomputations', 'url': 'https://llm.extractum.io/model/cognitivecomputations/Dolphin3.0-Llama3.1-8B,7rToucpFJjB0d8WeBFJ57p', 'content': 'Details and insights about Dolphin3.0 Llama3.1 8B LLM by cognitivecomputations: benchmarks, internals, and performance insights. Features: 8b LLM, VRAM: 16.1GB', 'score': 0.81489426, 'raw_content': None}, {'title': 'Dolphin 3.0 Released (Llama 3.1 + 3.2 + Qwen 2.5): A Local-First ...', 'url': 'https://www.marktechpost.com/2025/01/05/dolphin-3-0-released-llama-3-1-3-2-qwen-2-5-a-local-first-steerable-ai-model-that-puts-you-in-control-of-your-ai-stack-and-alignment/', 'content': 'At its core, Dolphin 3.0 has three versions: Llama 3.1 and Llama 3.2: These models are recognized for their strong capabilities in natural language understanding and generation, handling a wide variety of tasks efficiently.; Qwen 2.5: This multimodal model supports applications that involve both text and image processing, offering a versatile approach to complex problems.', 'score': 0.71113056, 'raw_content': None}, {'title': 'Dolphin 2.9 Llama 3 8b Curated and trained by Eric ... - Reddit', 'url': 'https://www.reddit.com/r/LocalLLaMA/comments/1c95z5k/dolphin_29_llama_3_8b_curated_and_trained_by_eric/', 'content': 'Dolphin 2.9 Llama 3 8b 🐬 Curated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations', 'score': 0.64773196, 'raw_content': None}, {'title': 'Cognitivecomputations/Dolphin3.0-Llama3.1-8B uncensored - Hacker News', 'url': 'https://news.ycombinator.com/item?id=42607271', 'content': 'Cognitivecomputations/Dolphin3.0-Llama3.1-8B uncensored - Hacker News ... Search:', 'score': 0.60750073, 'raw_content': None}, {'title': 'cognitivecomputations/dolphin-2.9.4-llama3.1-8b - Hugging Face', 'url': 'https://huggingface.co/cognitivecomputations/dolphin-2.9.4-llama3.1-8b', 'content': 'cognitivecomputations/dolphin-2.9.4-llama3.1-8b · Hugging Face Models hf (pretrained=/workspace/axolotl/dolphin-2.9.4-llama3.1-8b-hf,dtype=bfloat16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (4) base_model: meta-llama/Meta-Llama-3.1-8B - ^model.embed_tokens.weight$ model.layers.29.self_attn.k_proj model.layers.25.self_attn.k_proj model.layers.23.self_attn.k_proj model.layers.28.self_attn.k_proj model.layers.21.self_attn.k_proj model.layers.19.self_attn.k_proj model.layers.22.self_attn.k_proj model.layers.20.self_attn.k_proj model.layers.24.self_attn.k_proj model.layers.31.self_attn.k_proj model.layers.27.self_attn.k_proj model.layers.26.self_attn.k_proj model.layers.17.self_attn.k_proj model.layers.11.self_attn.k_proj model.layers.18.self_attn.k_proj model.layers.14.self_attn.k_proj model.layers.14.self_attn.o_proj model.layers.7.self_attn.o_proj model.layers.5.self_attn.o_proj model.layers.11.self_attn.o_proj model.layers.6.self_attn.o_proj model.layers.24.self_attn.o_proj model.layers.9.self_attn.o_proj model.layers.13.self_attn.o_proj model.layers.10.self_attn.o_proj model.layers.12.self_attn.o_proj model.layers.8.self_attn.o_proj model.layers.25.self_attn.o_proj model.layers.21.self_attn.o_proj model.layers.23.self_attn.o_proj model.layers.15.self_attn.o_proj model.layers.16.self_attn.o_proj model.layers.8.self_attn.q_proj model.layers.13.self_attn.q_proj model.layers.9.self_attn.q_proj model.layers.14.self_attn.q_proj model.layers.10.self_attn.q_proj model.layers.11.self_attn.q_proj model.layers.0.self_attn.q_proj model.layers.15.self_attn.q_proj model.layers.1.self_attn.q_proj model.layers.6.self_attn.q_proj model.layers.5.self_attn.q_proj model.layers.7.self_attn.q_proj model.layers.12.self_attn.q_proj model.layers.16.self_attn.q_proj model.layers.17.self_attn.q_proj model.layers.26.self_attn.q_proj model.layers.26.self_attn.v_proj model.layers.17.self_attn.v_proj model.layers.3.self_attn.v_proj model.layers.28.self_attn.v_proj model.layers.29.self_attn.v_proj model.layers.21.self_attn.v_proj model.layers.15.self_attn.v_proj model.layers.16.self_attn.v_proj model.layers.20.self_attn.v_proj model.layers.25.self_attn.v_proj model.layers.6.self_attn.v_proj model.layers.23.self_attn.v_proj model.layers.4.self_attn.v_proj model.layers.1.self_attn.v_proj model.layers.22.self_attn.v_proj model.layers.14.self_attn.v_proj This model is a fine-tuned version of meta-llama/Meta-Llama-3.1-8B on the None dataset. Model size Model tree for cognitivecomputations/dolphin-2.9.4-llama3.1-8b this model 1 model 6 models 7 models 21 models Datasets used to train cognitivecomputations/dolphin-2.9.4-llama3.1-8b Space using cognitivecomputations/dolphin-2.9.4-llama3.1-8b 1', 'score': 0.58976424, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=cognitivecomputations/Dolphin3.0-Llama3.1-8B\n",
      "> RSP [{'title': '[2410.20526] Llama Scope: Extracting Millions of Features from Llama-3. ...', 'url': 'https://arxiv.org/abs/2410.20526', 'content': '[2410.20526] Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders > cs > arXiv:2410.20526 arXiv:2410.20526 (cs) Title:Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders View a PDF of the paper titled Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders, by Zhengfu He and 11 other authors View a PDF of the paper titled Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders, by Zhengfu He and 11 other authors Bibliographic Explorer Toggle Connected Papers Toggle Litmaps Toggle scite.ai Toggle alphaXiv Toggle Links to Code Toggle DagsHub Toggle GotitPub Toggle Huggingface Toggle Links to Code Toggle ScienceCast Toggle Replicate Toggle Spaces Toggle Spaces Toggle Core recommender toggle IArxiv recommender toggle', 'score': 0.39110413, 'raw_content': None}, {'title': '[2407.21783] The Llama 3 Herd of Models - arXiv.org', 'url': 'https://arxiv.org/abs/2407.21783', 'content': 'Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive', 'score': 0.28793845, 'raw_content': None}, {'title': \"[2404.19553] Extending Llama-3's Context Ten-Fold Overnight - arXiv.org\", 'url': 'https://arxiv.org/abs/2404.19553', 'content': 'We extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA fine-tuning. The entire training cycle is super efficient, which takes 8 hours on one 8xA800 (80G) GPU machine. The resulted model exhibits superior performances across a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-context language understanding; meanwhile, it also well preserves the', 'score': 0.27955106, 'raw_content': None}, {'title': 'Hermes 3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2408.11857', 'content': 'Hermes 3 Technical Report 2 Model Overview Hermes 3 are highly steerable instruct and chat tuned models created by fine-tuning Llama 3.1 8B, 70B, and 405B.', 'score': 0.2438949, 'raw_content': None}, {'title': 'Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction', 'url': 'https://arxiv.org/html/2410.23692v1', 'content': 'To this end, we introduce Llama-3-8B-Mob, an instruction-tuned version of Llama-3-8B [], for long-term and multi-city human mobility prediction.We validate our approach on a large-scale human trajectory dataset from four metropolitan areas in Japan, focusing on predicting human mobility over the next 15 days.', 'score': 0.20234896, 'raw_content': None}, {'title': 'Llama Scope: Extracting Millions of Features from Llama-3.1-8B with ...', 'url': 'https://arxiv.org/html/2410.20526v1', 'content': 'We introduce a suite of 256 SAEs, trained on each layer and sublayer of the Llama-3.1-8B-Base model, with 32K and 128K features. Comprehensive analyses often require SAEs trained across multiple sites (e.g., for SAE-based circuit analysis\\xa0(He et\\xa0al., 2024; Marks et\\xa0al., 2024; Ge et\\xa0al., 2024)) or trained to handle multiple feature sizes (e.g., for feature splitting\\xa0(Bricken et\\xa0al., 2023) and identifying ultra-rare features\\xa0(Templeton et\\xa0al., 2024b)). Table\\xa01 provides a broad overview of Llama Scope SAE suite, along with a comparison to recent work on training Sparse Autoencoders on models with more than 8 billion parameters\\xa0(Templeton et\\xa0al., 2024b; Gao et\\xa0al., 2024; Lieberum et\\xa0al., 2024). It has been observed that lower activating samples tend to be less relevant to the interpretation\\xa0(Bricken et\\xa0al., 2023) for vanilla SAE features.', 'score': 0.17694198, 'raw_content': None}, {'title': 'Title: Applying Refusal-Vector Ablation to Llama 3.1 70B Agents - arXiv.org', 'url': 'https://arxiv.org/abs/2410.10871', 'content': 'Give to arXiv this week to help keep science open for all. > cs > arXiv:2410.10871 arXiv:2410.10871 (cs) Title:Applying Refusal-Vector Ablation to Llama 3.1 70B Agents View a PDF of the paper titled Applying Refusal-Vector Ablation to Llama 3.1 70B Agents, by Simon Lermen and Mateusz Dziemian and Govind Pimpale View a PDF of the paper titled Applying Refusal-Vector Ablation to Llama 3.1 70B Agents, by Simon Lermen and Mateusz Dziemian and Govind Pimpale Bibliographic Explorer Toggle Litmaps Toggle scite.ai Toggle alphaXiv Toggle Links to Code Toggle DagsHub Toggle GotitPub Toggle Huggingface Toggle Links to Code Toggle ScienceCast Toggle Replicate Toggle Spaces Toggle Spaces Toggle Connected Papers Toggle Core recommender toggle', 'score': 0.14160188, 'raw_content': None}, {'title': 'The Uniqueness of LLaMA3-70B with Per-Channel - arXiv.org', 'url': 'https://arxiv.org/html/2408.15301v1', 'content': 'To elucidate the unique vulnerability of the LLaMA3-70B series to W8A8 per-channel quantization, we conducted a comparative analysis of its weight distributions against those of other robust models. Figure\\xa01 and Figure\\xa03 present a comparative analysis of accuracy for several models in the LLaMA3-70B series subjected to 8-bit per-channel post-training quantization. In the latter layers, the LLaMA3-70B model demonstrates behavior similar to other models in terms of quantization errors and max_abs of weights. Based on our analysis of Figure\\xa09 , we have empirically identified that in the LLaMA3-70B model, the Q, K, V, Up, and Gate matrices of Block 0, 1, and 3 exhibit exceptionally high quantization errors and maximum absolute weight values.', 'score': 0.044835445, 'raw_content': None}, {'title': 'Llama 3 Meets MoE: Efficient Upcycling - arXiv.org', 'url': 'https://arxiv.org/html/2412.09952v1', 'content': 'Transformers [] have rapidly become the foundational architecture for a wide range of tasks in natural language processing [2, 7, 24, 23] and computer vision [8, 6, 32], revolutionizing these fields with their scalability and remarkable effectiveness.This has driven a dramatic increase in model complexity, with modern implementations featuring billions of parameters, far exceeding earlier', 'score': 0.035641678, 'raw_content': None}, {'title': 'Title: EXAONE 3.0 7.8B Instruction Tuned Language Model - arXiv.org', 'url': 'https://arxiv.org/abs/2408.03541', 'content': 'We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes, we publicly release the 7.8B instruction-tuned model to promote open research and innovations. Through extensive evaluations across a wide range of public and in-house benchmarks, EXAONE 3.0 demonstrates highly', 'score': 0.030447299, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2408.11857\n",
      "> RSP [Hermes 3 Technical Report](http://arxiv.org/abs/2408.11857v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2410.20526\n",
      "> RSP [Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders](http://arxiv.org/abs/2410.20526v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2404.19553\n",
      "> RSP [Extending Llama-3's Context Ten-Fold Overnight](http://arxiv.org/abs/2404.19553v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2407.21783\n",
      "> RSP [The Llama 3 Herd of Models](http://arxiv.org/abs/2407.21783v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.09952\n",
      "> RSP [Llama 3 Meets MoE: Efficient Upcycling](http://arxiv.org/abs/2412.09952v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2410.10871\n",
      "> RSP [Applying Refusal-Vector Ablation to Llama 3.1 70B Agents](http://arxiv.org/abs/2410.10871v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2410.23692\n",
      "> RSP [Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction](http://arxiv.org/abs/2410.23692v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2408.15301\n",
      "> RSP [The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization](http://arxiv.org/abs/2408.15301v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2408.03541\n",
      "> RSP [EXAONE 3.0 7.8B Instruction Tuned Language Model](http://arxiv.org/abs/2408.03541v3)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'Dolphin3.0 Llama3.1 8B by cognitivecomputations', 'url': 'https://llm.extractum.io/model/cognitivecomputations/Dolphin3.0-Llama3.1-8B,7rToucpFJjB0d8WeBFJ57p', 'content': 'Details and insights about Dolphin3.0 Llama3.1 8B LLM by cognitivecomputations: benchmarks, internals, and performance insights. Features: 8b LLM, VRAM: 16.1GB', 'score': 0.81489426, 'raw_content': None}, {'title': 'Dolphin 3.0 Released (Llama 3.1 + 3.2 + Qwen 2.5): A Local-First ...', 'url': 'https://www.marktechpost.com/2025/01/05/dolphin-3-0-released-llama-3-1-3-2-qwen-2-5-a-local-first-steerable-ai-model-that-puts-you-in-control-of-your-ai-stack-and-alignment/', 'content': 'At its core, Dolphin 3.0 has three versions: Llama 3.1 and Llama 3.2: These models are recognized for their strong capabilities in natural language understanding and generation, handling a wide variety of tasks efficiently.; Qwen 2.5: This multimodal model supports applications that involve both text and image processing, offering a versatile approach to complex problems.', 'score': 0.71113056, 'raw_content': None}, {'title': 'Dolphin 2.9 Llama 3 8b Curated and trained by Eric ... - Reddit', 'url': 'https://www.reddit.com/r/LocalLLaMA/comments/1c95z5k/dolphin_29_llama_3_8b_curated_and_trained_by_eric/', 'content': 'Dolphin 2.9 Llama 3 8b 🐬 Curated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations', 'score': 0.64773196, 'raw_content': None}, {'title': 'Cognitivecomputations/Dolphin3.0-Llama3.1-8B uncensored - Hacker News', 'url': 'https://news.ycombinator.com/item?id=42607271', 'content': 'Cognitivecomputations/Dolphin3.0-Llama3.1-8B uncensored - Hacker News ... Search:', 'score': 0.60750073, 'raw_content': None}, {'title': 'cognitivecomputations/dolphin-2.9.4-llama3.1-8b - Hugging Face', 'url': 'https://huggingface.co/cognitivecomputations/dolphin-2.9.4-llama3.1-8b', 'content': 'cognitivecomputations/dolphin-2.9.4-llama3.1-8b · Hugging Face Models hf (pretrained=/workspace/axolotl/dolphin-2.9.4-llama3.1-8b-hf,dtype=bfloat16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (4) base_model: meta-llama/Meta-Llama-3.1-8B - ^model.embed_tokens.weight$ model.layers.29.self_attn.k_proj model.layers.25.self_attn.k_proj model.layers.23.self_attn.k_proj model.layers.28.self_attn.k_proj model.layers.21.self_attn.k_proj model.layers.19.self_attn.k_proj model.layers.22.self_attn.k_proj model.layers.20.self_attn.k_proj model.layers.24.self_attn.k_proj model.layers.31.self_attn.k_proj model.layers.27.self_attn.k_proj model.layers.26.self_attn.k_proj model.layers.17.self_attn.k_proj model.layers.11.self_attn.k_proj model.layers.18.self_attn.k_proj model.layers.14.self_attn.k_proj model.layers.14.self_attn.o_proj model.layers.7.self_attn.o_proj model.layers.5.self_attn.o_proj model.layers.11.self_attn.o_proj model.layers.6.self_attn.o_proj model.layers.24.self_attn.o_proj model.layers.9.self_attn.o_proj model.layers.13.self_attn.o_proj model.layers.10.self_attn.o_proj model.layers.12.self_attn.o_proj model.layers.8.self_attn.o_proj model.layers.25.self_attn.o_proj model.layers.21.self_attn.o_proj model.layers.23.self_attn.o_proj model.layers.15.self_attn.o_proj model.layers.16.self_attn.o_proj model.layers.8.self_attn.q_proj model.layers.13.self_attn.q_proj model.layers.9.self_attn.q_proj model.layers.14.self_attn.q_proj model.layers.10.self_attn.q_proj model.layers.11.self_attn.q_proj model.layers.0.self_attn.q_proj model.layers.15.self_attn.q_proj model.layers.1.self_attn.q_proj model.layers.6.self_attn.q_proj model.layers.5.self_attn.q_proj model.layers.7.self_attn.q_proj model.layers.12.self_attn.q_proj model.layers.16.self_attn.q_proj model.layers.17.self_attn.q_proj model.layers.26.self_attn.q_proj model.layers.26.self_attn.v_proj model.layers.17.self_attn.v_proj model.layers.3.self_attn.v_proj model.layers.28.self_attn.v_proj model.layers.29.self_attn.v_proj model.layers.21.self_attn.v_proj model.layers.15.self_attn.v_proj model.layers.16.self_attn.v_proj model.layers.20.self_attn.v_proj model.layers.25.self_attn.v_proj model.layers.6.self_attn.v_proj model.layers.23.self_attn.v_proj model.layers.4.self_attn.v_proj model.layers.1.self_attn.v_proj model.layers.22.self_attn.v_proj model.layers.14.self_attn.v_proj This model is a fine-tuned version of meta-llama/Meta-Llama-3.1-8B on the None dataset. Model size Model tree for cognitivecomputations/dolphin-2.9.4-llama3.1-8b this model 1 model 6 models 7 models 21 models Datasets used to train cognitivecomputations/dolphin-2.9.4-llama3.1-8b Space using cognitivecomputations/dolphin-2.9.4-llama3.1-8b 1', 'score': 0.58976424, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "license: llama3.1\n",
      "datasets:\n",
      "- OpenCoder-LLM/opc-sft-stage1\n",
      "- OpenCoder-LLM/opc-sft-stage2\n",
      "- microsoft/orca-agentinstruct-1M-v1\n",
      "- microsoft/orca-math-word-problems-200k\n",
      "- NousResearch/hermes-function-calling-v1\n",
      "- AI-MO/NuminaMath-CoT\n",
      "- AI-MO/NuminaMath-TIR\n",
      "- allenai/tulu-3-sft-mixture\n",
      "- cognitivecomputations/dolphin-coder\n",
      "- HuggingFaceTB/smoltalk\n",
      "- cognitivecomputations/samantha-data\n",
      "- m-a-p/CodeFeedback-Filtered-Instruction\n",
      "- m-a-p/Code-Feedback\n",
      "language:\n",
      "- en\n",
      "base_model:\n",
      "- meta-llama/Llama-3.1-8B\n",
      "Dolphin 3.0 Llama 3.1 8B 🐬\n",
      "Part of the\n",
      "Dolphin 3.0 Collection\n",
      "Curated and trained by\n",
      "Eric Hartford\n",
      ",\n",
      "Ben Gitter\n",
      ",\n",
      "BlouseJury\n",
      "and\n",
      "Cognitive Computations\n",
      "Discord: https://discord.gg/cognitivecomputations\n",
      "Sponsors\n",
      "Our appreciation for the generous sponsors of Dolphin 3.0:\n",
      "-\n",
      "Crusoe Cloud\n",
      "- provided 16x L40s for training and evals\n",
      "-\n",
      "Akash\n",
      "- provided on-demand 8x H100 for training\n",
      "-\n",
      "Lazarus\n",
      "- provided 16x H100 for training\n",
      "-\n",
      "Cerebras\n",
      "- provided excellent and fast inference services for data labeling\n",
      "-\n",
      "Andreessen Horowitz\n",
      "- provided a\n",
      "grant\n",
      "that make Dolphin 1.0 possible and enabled me to bootstrap my homelab\n",
      "What is Dolphin?\n",
      "Dolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models.  Designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.\n",
      "Dolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini.  But these models present problems for businesses seeking to include AI in their products.\n",
      "1) They maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break.\n",
      "2) They maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on.\n",
      "3) They maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application.\n",
      "4) They can see all your queries and they can potentially use that data in ways you wouldn't want.\n",
      "Dolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt.  You decide the alignment.  You have control of your data.  Dolphin does not impose its ethics or guidelines on you.  You are the one who decides the guidelines.\n",
      "Dolphin belongs to YOU, it is your tool, an extension of your will.\n",
      "Just as you are personally responsible for what you do with a knife, gun, fire, car, or the internet, you are the creator and originator of any content you generate with Dolphin.\n",
      "https://erichartford.com/uncensored-models\n",
      "Chat Template\n",
      "We use ChatML for the chat template.\n",
      "<|im_start|>system\n",
      "You are Dolphin, a helpful AI assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "{prompt}<|im_end|>\n",
      "<|im_start|>assistant\n",
      "System Prompt\n",
      "In Dolphin, the system prompt is what you use to set the tone and alignment of the responses.  You can set a character, a mood, rules for its behavior, and it will try its best to follow them.\n",
      "Make sure to set the system prompt in order to set the tone and guidelines for the responses - Otherwise, it will act in a default way that might not be what you want.\n",
      "Example use of system prompt:\n",
      "<|im_start|>system\n",
      "You are Dolphin, a golang coding assistant.  you only code in golang.  If the user requests any other programming language, return the solution in golang instead.<|im_end|>\n",
      "<|im_start|>user\n",
      "Please implement A* using python<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sample Outputs\n",
      "How to use\n",
      "There are many ways to use a huggingface model including:\n",
      "- ollama\n",
      "- LM Studio\n",
      "- Huggingface Transformers library\n",
      "- vllm\n",
      "- sglang\n",
      "- tgi\n",
      "ollama\n",
      "Install ollama\n",
      "ollama run hf.co/cognitivecomputations/Dolphin3.0-Llama3.1-8B-GGUF:Q4_0\n",
      "/set system <your system prompt>\n",
      "Evals\n",
      "TBD\n",
      "Appreciation\n",
      "Respect and thanks to the creators of the open source datasets that were used:\n",
      "-\n",
      "OpenCoder-LLM\n",
      "(opc-sft-stage1, opc-sft-stage2)\n",
      "-\n",
      "microsoft\n",
      "(orca-agentinstruct-1M-v1, orca-math-word-problems-200k)\n",
      "-\n",
      "NousResearch\n",
      "(hermes-function-calling-v1)\n",
      "-\n",
      "AI-MO\n",
      "(NuminaMath-CoT, NuminaMath-TIR)\n",
      "-\n",
      "allenai\n",
      "(tulu-3-sft-mixture)\n",
      "-\n",
      "HuggingFaceTB\n",
      "(smoltalk)\n",
      "-\n",
      "m-a-p\n",
      "(CodeFeedback-Filtered-Instruction, Code-Feedback)\n",
      "Special thanks to \n",
      "- Meta, Qwen, and OpenCoder, who wrote papers and published models that were instrumental in creating Dolphin 3.0.\n",
      "-\n",
      "RLHFlow\n",
      "for the excellent reward model used to filter the datasets\n",
      "- Deepseek, for the ridiculously fast Deepseek-V3 that we used to augment the data.\n",
      "> RSP * Steerable AI model allowing complete control over system prompts and alignment.\n",
      "* General purpose functionality for coding, math, and agentic tasks.\n",
      "* Local-first approach, enabling self-hosting and data privacy.\n",
      "* Customizable guidelines and ethics, tailored to user needs.\n",
      "* Built on a robust architecture with advanced multimodal support.\n",
      "* Regular updates and user maintenance over versions without silent changes.\n",
      "* Utilizes open-source datasets for training, promoting transparency.\n",
      "* Extensive performance benchmarking against leading models in the industry.\n",
      "* Strong community support and collaborative development.\n",
      "* Inference optimized for fast and efficient response times on local hardware.\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=stabilityai/stable-diffusion-3.5-large\n",
      "> RSP {'model_id': 'stabilityai/stable-diffusion-3.5-large', 'created_at': '22 October 2024 at 07:29:57 UTC', 'downloads': 127483, 'likes': 1807, 'trending_score': 66, 'description': '---\\nlicense: other\\nlicense_name: stabilityai-ai-community\\nlicense_link: LICENSE.md\\ntags:\\n- text-to-image\\n- stable-diffusion\\n- diffusers\\ninference: true\\nextra_gated_prompt: >-\\n  By clicking \"Agree\", you agree to the [License\\n  Agreement](https://huggingface.co/stabilityai/stable-diffusion-3.5-large/blob/main/LICENSE.md)\\n  and acknowledge Stability AI\\'s [Privacy\\n  Policy](https://stability.ai/privacy-policy).\\nextra_gated_fields:\\n  Name: text\\n  Email: text\\n  Country: country\\n  Organization or Affiliation: text\\n  Receive email updates and promotions on Stability AI products, services, and research?:\\n    type: select\\n    options:\\n    - \\'Yes\\'\\n    - \\'No\\'\\n  What do you intend to use the model for?: \\n    type: select\\n    options:\\n    - Research\\n    - Personal use\\n    - Creative Professional\\n    - Startup\\n    - Enterprise\\n  I agree to the License Agreement and acknowledge Stability AI\\'s Privacy Policy: checkbox\\n\\nlanguage:\\n- en\\npipeline_tag: text-to-image\\n---\\n\\n# Stable Diffusion 3.5 Large\\n![3.5 Large Demo Image](sd3.5_large_demo.png)\\n\\n## Model\\n\\n![MMDiT](mmdit.png)\\n\\n\\n[Stable Diffusion 3.5 Large](https://stability.ai/news/introducing-stable-diffusion-3-5) is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.\\n\\nPlease note: This model is released under the [Stability Community License](https://stability.ai/community-license-agreement). Visit [Stability AI](https://stability.ai/license) to learn or [contact us](https://stability.ai/enterprise) for commercial licensing details.\\n\\n\\n### Model Description\\n\\n- **Developed by:** Stability AI\\n- **Model type:** MMDiT text-to-image generative model\\n- **Model Description:** This model generates images based on text prompts. It is a [Multimodal Diffusion Transformer](https://arxiv.org/abs/2403.03206) that use three fixed, pretrained text encoders, and with QK-normalization to improve training stability. \\n\\n### License\\n\\n- **Community License:**  Free for research, non-commercial, and commercial use for organizations or individuals with less than $1M in total annual revenue. More details can be found in the [Community License Agreement](https://stability.ai/community-license-agreement). Read more at https://stability.ai/license.\\n- **For individuals and organizations with annual revenue above $1M**: please [contact us](https://stability.ai/enterprise) to get an Enterprise License.\\n\\n### Model Sources\\n\\nFor local or self-hosted use, we recommend [ComfyUI](https://github.com/comfyanonymous/ComfyUI) for node-based UI inference, or [diffusers](https://github.com/huggingface/diffusers) or [GitHub](https://github.com/Stability-AI/sd3.5) for programmatic use.\\n\\n- **ComfyUI:** [Github](https://github.com/comfyanonymous/ComfyUI), [Example Workflow](https://comfyanonymous.github.io/ComfyUI_examples/sd3/)\\n- **Huggingface Space:** [Space](https://huggingface.co/spaces/stabilityai/stable-diffusion-3.5-large)\\n- **Diffusers**: [See below](#using-with-diffusers).\\n- **GitHub**: [GitHub](https://github.com/Stability-AI/sd3.5).\\n\\n- **API Endpoints:**\\n  - [Stability AI API](https://platform.stability.ai/docs/api-reference#tag/Generate/paths/~1v2beta~1stable-image~1generate~1sd3/post)\\n  - [Replicate](https://replicate.com/stability-ai/stable-diffusion-3.5-large)\\n  - [Deepinfra](https://deepinfra.com/stabilityai/sd3.5)\\n\\n\\n### Implementation Details\\n\\n- **QK Normalization:** Implements the QK normalization technique to improve training Stability.\\n\\n- **Text Encoders：**\\n    - CLIPs: [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip), [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main), context length 77 tokens\\n    - T5: [T5-xxl](https://huggingface.co/google/t5-v1_1-xxl), context length 77/256 tokens at different stages of training\\n\\n- **Training Data and Strategy:**\\n    \\n    This model was trained on a wide variety of data, including synthetic data and filtered publicly available data. \\n\\nFor more technical details of the original MMDiT architecture, please refer to the [Research paper](https://stability.ai/news/stable-diffusion-3-research-paper).\\n\\n\\n### Model Performance\\n\\nSee [blog](https://stability.ai/news/introducing-stable-diffusion-3-5) for our study about comparative performance in prompt adherence and aesthetic quality. \\n\\n\\n## File Structure\\n\\nClick here to access the [Files and versions tab](https://huggingface.co/stabilityai/stable-diffusion-3.5-large/tree/main)\\n\\n```│\\n├── text_encoders/  \\n│   ├── README.md\\n│   ├── clip_g.safetensors\\n│   ├── clip_l.safetensors\\n│   ├── t5xxl_fp16.safetensors\\n│   └── t5xxl_fp8_e4m3fn.safetensors\\n│\\n├── README.md\\n├── LICENSE\\n├── sd3_large.safetensors\\n├── SD3.5L_example_workflow.json\\n└── sd3_large_demo.png\\n\\n** File structure below is for diffusers integration**\\n├── scheduler/\\n├── text_encoder/\\n├── text_encoder_2/\\n├── text_encoder_3/\\n├── tokenizer/\\n├── tokenizer_2/\\n├── tokenizer_3/\\n├── transformer/\\n├── vae/\\n└── model_index.json\\n```\\n\\n## Using with Diffusers\\nUpgrade to the latest version of the [🧨 diffusers library](https://github.com/huggingface/diffusers)\\n```\\npip install -U diffusers\\n```\\n\\nand then you can run\\n```py\\nimport torch\\nfrom diffusers import StableDiffusion3Pipeline\\n\\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large\", torch_dtype=torch.bfloat16)\\npipe = pipe.to(\"cuda\")\\n\\nimage = pipe(\\n    \"A capybara holding a sign that reads Hello World\",\\n    num_inference_steps=28,\\n    guidance_scale=3.5,\\n).images[0]\\nimage.save(\"capybara.png\")\\n```\\n\\n### Quantizing the model with diffusers\\n\\nReduce your VRAM usage and have the model fit on 🤏 VRAM GPUs\\n\\n```\\npip install bitsandbytes\\n```\\n\\n```py\\nfrom diffusers import BitsAndBytesConfig, SD3Transformer2DModel\\nfrom diffusers import StableDiffusion3Pipeline\\nimport torch\\n\\nmodel_id = \"stabilityai/stable-diffusion-3.5-large\"\\n\\nnf4_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n)\\nmodel_nf4 = SD3Transformer2DModel.from_pretrained(\\n    model_id,\\n    subfolder=\"transformer\",\\n    quantization_config=nf4_config,\\n    torch_dtype=torch.bfloat16\\n)\\n\\npipeline = StableDiffusion3Pipeline.from_pretrained(\\n    model_id, \\n    transformer=model_nf4,\\n    torch_dtype=torch.bfloat16\\n)\\npipeline.enable_model_cpu_offload()\\n\\nprompt = \"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. It features the distinctive, bulky body shape of a hippo. However, instead of the usual grey skin, the creature\\'s body resembles a golden-brown, crispy waffle fresh off the griddle. The skin is textured with the familiar grid pattern of a waffle, each square filled with a glistening sheen of syrup. The environment combines the natural habitat of a hippo with elements of a breakfast table setting, a river of warm, melted butter, with oversized utensils or plates peeking out from the lush, pancake-like foliage in the background, a towering pepper mill standing in for a tree.  As the sun rises in this fantastical world, it casts a warm, buttery glow over the scene. The creature, content in its butter river, lets out a yawn. Nearby, a flock of birds take flight\"\\n\\nimage = pipeline(\\n    prompt=prompt,\\n    num_inference_steps=28,\\n    guidance_scale=4.5,\\n    max_sequence_length=512,\\n).images[0]\\nimage.save(\"whimsical.png\")\\n```\\n\\n### Fine-tuning\\n\\nPlease see the fine-tuning guide [here](https://stabilityai.notion.site/Stable-Diffusion-3-5-Large-Fine-tuning-Tutorial-11a61cdcd1968027a15bdbd7c40be8c6).\\n\\n\\n## Uses\\n\\n### Intended Uses\\n\\nIntended uses include the following:\\n* Generation of artworks and use in design and other artistic processes.\\n* Applications in educational or creative tools.\\n* Research on generative models, including understanding the limitations of generative models.\\n\\nAll uses of the model must be in accordance with our [Acceptable Use Policy](https://stability.ai/use-policy).\\n\\n### Out-of-Scope Uses\\n\\nThe model was not trained to be factual or true representations of people or events.  As such, using the model to generate such content is out-of-scope of the abilities of this model.\\n\\n## Safety\\n\\nAs part of our safety-by-design and responsible AI deployment approach, we take deliberate measures to ensure Integrity starts at the early stages of development. We implement safety measures throughout the development of our models. We have implemented safety mitigations that are intended to reduce the risk of certain harms, however we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases.  \\nFor more about our approach to Safety, please visit our [Safety page](https://stability.ai/safety).\\n\\n### Integrity Evaluation\\n\\nOur integrity evaluation methods include structured evaluations and red-teaming testing for certain harms.  Testing was conducted primarily in English and may not cover all possible harms.  \\n\\n### Risks identified and mitigations:\\n\\n* Harmful content:  We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm. However, this does not guarantee that all possible harmful content has been removed. TAll developers and deployers should exercise caution and implement content safety guardrails based on their specific product policies and application use cases.\\n* Misuse: Technical limitations and developer and end-user education can help mitigate against malicious applications of models. All users are required to adhere to our [Acceptable Use Policy](https://stability.ai/use-policy), including when applying fine-tuning and prompt engineering mechanisms. Please reference the Stability AI Acceptable Use Policy for information on violative uses of our products.\\n* Privacy violations: Developers and deployers are encouraged to adhere to privacy regulations with techniques that respect data privacy.\\n\\n### Contact\\n\\nPlease report any issues with the model or contact us:\\n\\n* Safety issues:  safety@stability.ai\\n* Security issues:  security@stability.ai\\n* Privacy issues:  privacy@stability.ai\\n* License and general: https://stability.ai/license\\n* Enterprise license: https://stability.ai/enterprise\\n\\n\\n'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=stabilityai/stable-diffusion-3.5-large\n",
      "> RSP [{'title': 'stability-ai/stable-diffusion-3.5-large - Run with an API on Replicate', 'url': 'https://replicate.com/stability-ai/stable-diffusion-3.5-large', 'content': 'stability-ai/stable-diffusion-3.5-large – Run with an API on Replicate This model is priced by how many images are generated. Model Stable Diffusion 3.5 Large is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. Model Description: This model generates images based on text prompts. This model is available under the Stability AI Community License: We have implemented safety mitigations that are intended to reduce the risk of certain harms, however we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases. Harmful content: We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm.', 'score': 0.9203972, 'raw_content': None}, {'title': 'Stable Diffusion 3.5 (Large): What You Need to Know', 'url': 'https://sandner.art/stable-diffusion-35-large-what-you-need-to-know/', 'content': 'sandner.art | Stable Diffusion 3.5 (Large): What You Need to Know Stable Diffusion 3.5 Large is an 8-billion-parameter Multimodal Diffusion Transformer (MMDiT) text-to-image generative model by Stability AI. Forget about effective upscaling workflows with the SD3 Large model. It is a new model, but it retains familiar Stable Diffusion traits (similar to Flux, SD styles quite work) Place sd3.5_large_fp8_scaled.safetensors it into your Stable Diffusion model folder in Comfy UI. These examples were created with sd3.5_large_fp8_scaled\\xa0and flux1dev-Q4_K_S model to fit into VRAM, so take them with a grain of salt. SD 3.5 Turbo is a speed-optimized, low-step model that produces somewhat simplified outputs compared to the Large version (similar to the situation with Flux-dev vs. sandner.art SD 3.5 workflows and experiments on github', 'score': 0.8961153, 'raw_content': None}, {'title': 'stabilityai/stable-diffusion-3.5-large · Hugging Face', 'url': 'https://huggingface.co/stabilityai/stable-diffusion-3.5-large', 'content': 'Models Use this model Model Quantizing the model with diffusers Model Stable Diffusion 3.5 Large is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. Model Description: This model generates images based on text prompts. It is a Multimodal Diffusion Transformer that use three fixed, pretrained text encoders, and with QK-normalization to improve training stability. pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large\", torch_dtype=torch.bfloat16) Quantizing the model with diffusers model_id = \"stabilityai/stable-diffusion-3.5-large\" Harmful content: We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm. Model tree for stabilityai/stable-diffusion-3.5-large 34 models 1 model 1 model 4 models', 'score': 0.8900749, 'raw_content': None}, {'title': 'Introducing Stable Diffusion 3.5 - Stability AI', 'url': 'https://stability.ai/news/introducing-stable-diffusion-3-5', 'content': 'These models are highly customizable for their size, run on consumer hardware, and are free for both commercial and non-commercial use under the permissive Stability AI Community License. The Stable Diffusion 3.5 version excels in the following areas, making it one of the most customizable and accessible image models on the market, while maintaining top-tier performance in prompt adherence and image quality: Stable Diffusion 3.5 Large Turbo offers some of the fastest inference times for its size, while remaining highly competitive in both image quality and prompt adherence, even when compared to non-distilled models of similar size Stable Diffusion 3.5 Medium outperforms other medium-sized models, offering a balance of prompt adherence and image quality, making it a top choice for efficient, high-quality performance.', 'score': 0.8682137, 'raw_content': None}, {'title': 'Stability AI', 'url': 'https://stability.ai/?ref=aifindpro', 'content': 'This version includes multiple variants, including Stable Diffusion 3.5 Large, Stable Diffusion 3.5 Large Turbo and Stable Diffusion 3.5 Medium. These models are highly customizable for their size, run on consumer hardware, and are free for both commercial and non-commercial use under the permissive Stability AI Community License.', 'score': 0.843393, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=stabilityai/stable-diffusion-3.5-large\n",
      "> RSP [{'title': 'arXiv:2210.14896v4 [cs.CV] 6 Jul 2023', 'url': 'https://arxiv.org/pdf/2210.14896', 'content': 'user-generated images from the official Stable Dif-fusion Discord server. We choose Stable Diffusion as it is currently the only open-source large text-to-image generative model, and all generated images have a CC0 1.0 license that allows uses for any pur-pose (StabilityAI,2022b). We choose the official', 'score': 0.4398682, 'raw_content': None}, {'title': 'Learning from Mistakes: Iterative Prompt Relabeling - arXiv.org', 'url': 'https://arxiv.org/html/2312.16204v3', 'content': 'Compared to the previous stable diffusion models, Stable Diffusion XL (Podell et al., 2023) features a UNet that is three times larger and integrates OpenCLIP ViT-bigG/14 with the original text encoder. It also introduces crop-conditioning and a two-stage model process to significantly enhance the quality of generated images.', 'score': 0.40967003, 'raw_content': None}, {'title': 'DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image ...', 'url': 'https://arxiv.org/html/2210.14896', 'content': 'We construct DiffusionDB (Fig. 2) by scraping user-generated images from the official Stable Diffusion Discord server.We choose Stable Diffusion as it is currently the only open-source large text-to-image generative model, and all generated images have a CC0 1.0 license that allows uses for any purpose StabilityAI ().We choose the official public Discord server as it has strict rules against', 'score': 0.40853685, 'raw_content': None}, {'title': 'Progressive Knowledge Distillation of Stable Diffusion XL using Layer ...', 'url': 'https://arxiv.org/html/2401.02677', 'content': 'Stable Diffusion (Rombach et al., 2022) has emerged as highly influential in the realm of text-to-image (T2I) synthesis, playing a pivotal role as an open-source framework. Its remarkable capabilities has spurred its integration as a backbone in various text-guided vision applications. Stable Diffusion, characterized as T2I-specialized latent diffusion models (LDMs), leverages diffusion', 'score': 0.2938303, 'raw_content': None}, {'title': 'Context-Aware Full Body Anonymization using Text-to-Image Diffusion Models', 'url': 'https://arxiv.org/html/2410.08551v1', 'content': 'A large value changes the image significantly, as most of the initial image is noise and has no information about the original person. ... [31] StabilityAI. Stable diffusion 2 inpainting, 2023. Accessed on 19 December 2023. [32] StabilityAI. Stable diffusion 2.1, 2023. Accessed on 19 December 2023. [33] StabilityAI. Stable diffusion xl base 1.0', 'score': 0.2898343, 'raw_content': None}, {'title': 'Degeneration-Tuning: Using Scrambled Grid shield Unwanted ... - ar5iv', 'url': 'https://ar5iv.labs.arxiv.org/html/2308.02552', 'content': 'Large-scale text-to-image diffusion models (openai, 2022; Rombach et al., 2022; Nichol et al., 2021; Saharia et al., 2022; Liu et al., 2023a, b) such as Stable Diffusion (SD) (Rombach et al., 2022), have garnered significant attention from both industry and media.However, they have also given rise to various social issues, such as potential infringement of intellectual property (IP) (openai', 'score': 0.2690425, 'raw_content': None}, {'title': 'Taming Stable Diffusion for Text to 360 - arXiv.org', 'url': 'https://arxiv.org/html/2404.07949v1', 'content': 'Abstract. Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts.Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images.', 'score': 0.24769658, 'raw_content': None}, {'title': 'Towards More Realistic Membership Inference Attacks on Large Diffusion ...', 'url': 'https://ar5iv.labs.arxiv.org/html/2306.12983', 'content': 'Figure 1: Pitfalls in the evaluation setting can lead to incorrect conclusions on the effectiveness of membership attacks against large diffusion models such as Stable Diffusion. An exemplary misleading setup involves finetuning the model on a very small dataset with a low internal variance (such as the POKEMON dataset), which gives a remarkable performance for the selected attacks.', 'score': 0.20159344, 'raw_content': None}, {'title': 'Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study', 'url': 'https://arxiv.org/html/2411.13588v1', 'content': 'While prior research has highlighted the presence of high similarity in activation values between adjacent diffusion steps (referred to as redundancy) and proposed various caching mechanisms to mitigate computational overhead, the exploration of redundancy in existing literature remains limited, with findings often not generalizable across different DiT models. Our experimental analysis reveals substantial variations in the distribution of redundancy across diffusion steps among different DiT models. We systematically explore redundancy across diffusion steps in a diverse range of prominent DiT models, including FLUX.1-dev, Pixart-Alpha, Stable-Diffusion-3, CogVideoX-5B, Open-Sora, Latte-1, and Mochi-1-preview. By contrast, in the domain of DiT models, L2C\\xa0[8], Tgate\\xa0[9], and PAB\\xa0[10] explores the redundancy distribution across diffusion steps and propose caching mechanism to accelerate the diffusion process. Consequently, we can infer that the number of diffusion steps does not significantly alter the redundancy distribution in DiT models.', 'score': 0.13662016, 'raw_content': None}, {'title': 'Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large ...', 'url': 'https://arxiv.org/html/2311.15127', 'content': 'As discussed in Section 3.2, our video model is based on Stable Diffusion 2.1 [71] (SD 2.1). Recent works [44] show that it is crucial to adopt the noise schedule when training image diffusion models, shifting towards more noise for higher-resolution images.', 'score': 0.13058473, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2401.02677\n",
      "> RSP [Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss](http://arxiv.org/abs/2401.02677v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2311.15127\n",
      "> RSP [Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets](http://arxiv.org/abs/2311.15127v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.13588\n",
      "> RSP [Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study](http://arxiv.org/abs/2411.13588v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2306.12983\n",
      "> RSP [Towards More Realistic Membership Inference Attacks on Large Diffusion Models](http://arxiv.org/abs/2306.12983v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2404.07949\n",
      "> RSP [Taming Stable Diffusion for Text to 360° Panorama Image Generation](http://arxiv.org/abs/2404.07949v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2210.14896\n",
      "> RSP [DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models](http://arxiv.org/abs/2210.14896v4)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2410.08551\n",
      "> RSP [Context-Aware Full Body Anonymization using Text-to-Image Diffusion Models](http://arxiv.org/abs/2410.08551v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2312.16204\n",
      "> RSP [Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training](http://arxiv.org/abs/2312.16204v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2308.02552\n",
      "> RSP [Degeneration-Tuning: Using Scrambled Grid shield Unwanted Concepts from Stable Diffusion](http://arxiv.org/abs/2308.02552v2)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'stability-ai/stable-diffusion-3.5-large - Run with an API on Replicate', 'url': 'https://replicate.com/stability-ai/stable-diffusion-3.5-large', 'content': 'stability-ai/stable-diffusion-3.5-large – Run with an API on Replicate This model is priced by how many images are generated. Model Stable Diffusion 3.5 Large is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. Model Description: This model generates images based on text prompts. This model is available under the Stability AI Community License: We have implemented safety mitigations that are intended to reduce the risk of certain harms, however we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases. Harmful content: We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm.', 'score': 0.9203972, 'raw_content': None}, {'title': 'Stable Diffusion 3.5 (Large): What You Need to Know', 'url': 'https://sandner.art/stable-diffusion-35-large-what-you-need-to-know/', 'content': 'sandner.art | Stable Diffusion 3.5 (Large): What You Need to Know Stable Diffusion 3.5 Large is an 8-billion-parameter Multimodal Diffusion Transformer (MMDiT) text-to-image generative model by Stability AI. Forget about effective upscaling workflows with the SD3 Large model. It is a new model, but it retains familiar Stable Diffusion traits (similar to Flux, SD styles quite work) Place sd3.5_large_fp8_scaled.safetensors it into your Stable Diffusion model folder in Comfy UI. These examples were created with sd3.5_large_fp8_scaled\\xa0and flux1dev-Q4_K_S model to fit into VRAM, so take them with a grain of salt. SD 3.5 Turbo is a speed-optimized, low-step model that produces somewhat simplified outputs compared to the Large version (similar to the situation with Flux-dev vs. sandner.art SD 3.5 workflows and experiments on github', 'score': 0.8961153, 'raw_content': None}, {'title': 'stabilityai/stable-diffusion-3.5-large · Hugging Face', 'url': 'https://huggingface.co/stabilityai/stable-diffusion-3.5-large', 'content': 'Models Use this model Model Quantizing the model with diffusers Model Stable Diffusion 3.5 Large is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. Model Description: This model generates images based on text prompts. It is a Multimodal Diffusion Transformer that use three fixed, pretrained text encoders, and with QK-normalization to improve training stability. pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large\", torch_dtype=torch.bfloat16) Quantizing the model with diffusers model_id = \"stabilityai/stable-diffusion-3.5-large\" Harmful content: We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm. Model tree for stabilityai/stable-diffusion-3.5-large 34 models 1 model 1 model 4 models', 'score': 0.8900749, 'raw_content': None}, {'title': 'Introducing Stable Diffusion 3.5 - Stability AI', 'url': 'https://stability.ai/news/introducing-stable-diffusion-3-5', 'content': 'These models are highly customizable for their size, run on consumer hardware, and are free for both commercial and non-commercial use under the permissive Stability AI Community License. The Stable Diffusion 3.5 version excels in the following areas, making it one of the most customizable and accessible image models on the market, while maintaining top-tier performance in prompt adherence and image quality: Stable Diffusion 3.5 Large Turbo offers some of the fastest inference times for its size, while remaining highly competitive in both image quality and prompt adherence, even when compared to non-distilled models of similar size Stable Diffusion 3.5 Medium outperforms other medium-sized models, offering a balance of prompt adherence and image quality, making it a top choice for efficient, high-quality performance.', 'score': 0.8682137, 'raw_content': None}, {'title': 'Stability AI', 'url': 'https://stability.ai/?ref=aifindpro', 'content': 'This version includes multiple variants, including Stable Diffusion 3.5 Large, Stable Diffusion 3.5 Large Turbo and Stable Diffusion 3.5 Medium. These models are highly customizable for their size, run on consumer hardware, and are free for both commercial and non-commercial use under the permissive Stability AI Community License.', 'score': 0.843393, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "license: other\n",
      "license_name: stabilityai-ai-community\n",
      "license_link: LICENSE.md\n",
      "tags:\n",
      "- text-to-image\n",
      "- stable-diffusion\n",
      "- diffusers\n",
      "inference: true\n",
      "extra_gated_prompt: >-\n",
      "  By clicking \"Agree\", you agree to the\n",
      "License\n",
      "  Agreement\n",
      "and acknowledge Stability AI's\n",
      "Privacy\n",
      "  Policy\n",
      ".\n",
      "extra_gated_fields:\n",
      "  Name: text\n",
      "  Email: text\n",
      "  Country: country\n",
      "  Organization or Affiliation: text\n",
      "  Receive email updates and promotions on Stability AI products, services, and research?:\n",
      "    type: select\n",
      "    options:\n",
      "    - 'Yes'\n",
      "    - 'No'\n",
      "  What do you intend to use the model for?: \n",
      "    type: select\n",
      "    options:\n",
      "    - Research\n",
      "    - Personal use\n",
      "    - Creative Professional\n",
      "    - Startup\n",
      "    - Enterprise\n",
      "  I agree to the License Agreement and acknowledge Stability AI's Privacy Policy: checkbox\n",
      "language:\n",
      "- en\n",
      "pipeline_tag: text-to-image\n",
      "Stable Diffusion 3.5 Large\n",
      "Model\n",
      "Stable Diffusion 3.5 Large\n",
      "is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.\n",
      "Please note: This model is released under the\n",
      "Stability Community License\n",
      ". Visit\n",
      "Stability AI\n",
      "to learn or\n",
      "contact us\n",
      "for commercial licensing details.\n",
      "Model Description\n",
      "Developed by:\n",
      "Stability AI\n",
      "Model type:\n",
      "MMDiT text-to-image generative model\n",
      "Model Description:\n",
      "This model generates images based on text prompts. It is a\n",
      "Multimodal Diffusion Transformer\n",
      "that use three fixed, pretrained text encoders, and with QK-normalization to improve training stability.\n",
      "License\n",
      "Community License:\n",
      "Free for research, non-commercial, and commercial use for organizations or individuals with less than $1M in total annual revenue. More details can be found in the\n",
      "Community License Agreement\n",
      ". Read more at https://stability.ai/license.\n",
      "For individuals and organizations with annual revenue above $1M\n",
      ": please\n",
      "contact us\n",
      "to get an Enterprise License.\n",
      "Model Sources\n",
      "For local or self-hosted use, we recommend\n",
      "ComfyUI\n",
      "for node-based UI inference, or\n",
      "diffusers\n",
      "or\n",
      "GitHub\n",
      "for programmatic use.\n",
      "ComfyUI:\n",
      "Github\n",
      ",\n",
      "Example Workflow\n",
      "Huggingface Space:\n",
      "Space\n",
      "Diffusers\n",
      ":\n",
      "See below\n",
      ".\n",
      "GitHub\n",
      ":\n",
      "GitHub\n",
      ".\n",
      "API Endpoints:\n",
      "Stability AI API\n",
      "Replicate\n",
      "Deepinfra\n",
      "Implementation Details\n",
      "QK Normalization:\n",
      "Implements the QK normalization technique to improve training Stability.\n",
      "Text Encoders：\n",
      "CLIPs:\n",
      "OpenCLIP-ViT/G\n",
      ",\n",
      "CLIP-ViT/L\n",
      ", context length 77 tokens\n",
      "T5:\n",
      "T5-xxl\n",
      ", context length 77/256 tokens at different stages of training\n",
      "Training Data and Strategy:\n",
      "This model was trained on a wide variety of data, including synthetic data and filtered publicly available data.\n",
      "For more technical details of the original MMDiT architecture, please refer to the\n",
      "Research paper\n",
      ".\n",
      "Model Performance\n",
      "See\n",
      "blog\n",
      "for our study about comparative performance in prompt adherence and aesthetic quality.\n",
      "File Structure\n",
      "Click here to access the\n",
      "Files and versions tab\n",
      "```│\n",
      "├── text_encoders/\n",
      "│   ├── README.md\n",
      "│   ├── clip_g.safetensors\n",
      "│   ├── clip_l.safetensors\n",
      "│   ├── t5xxl_fp16.safetensors\n",
      "│   └── t5xxl_fp8_e4m3fn.safetensors\n",
      "│\n",
      "├── README.md\n",
      "├── LICENSE\n",
      "├── sd3_large.safetensors\n",
      "├── SD3.5L_example_workflow.json\n",
      "└── sd3_large_demo.png\n",
      "** File structure below is for diffusers integration**\n",
      "├── scheduler/\n",
      "├── text_encoder/\n",
      "├── text_encoder_2/\n",
      "├── text_encoder_3/\n",
      "├── tokenizer/\n",
      "├── tokenizer_2/\n",
      "├── tokenizer_3/\n",
      "├── transformer/\n",
      "├── vae/\n",
      "└── model_index.json\n",
      "```\n",
      "Using with Diffusers\n",
      "Upgrade to the latest version of the\n",
      "🧨 diffusers library\n",
      "pip install -U diffusers\n",
      "and then you can run\n",
      "```py\n",
      "import torch\n",
      "from diffusers import StableDiffusion3Pipeline\n",
      "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large\", torch_dtype=torch.bfloat16)\n",
      "pipe = pipe.to(\"cuda\")\n",
      "image = pipe(\n",
      "    \"A capybara holding a sign that reads Hello World\",\n",
      "    num_inference_steps=28,\n",
      "    guidance_scale=3.5,\n",
      ").images[0]\n",
      "image.save(\"capybara.png\")\n",
      "```\n",
      "Quantizing the model with diffusers\n",
      "Reduce your VRAM usage and have the model fit on 🤏 VRAM GPUs\n",
      "pip install bitsandbytes\n",
      "```py\n",
      "from diffusers import BitsAndBytesConfig, SD3Transformer2DModel\n",
      "from diffusers import StableDiffusion3Pipeline\n",
      "import torch\n",
      "model_id = \"stabilityai/stable-diffusion-3.5-large\"\n",
      "nf4_config = BitsAndBytesConfig(\n",
      "    load_in_4bit=True,\n",
      "    bnb_4bit_quant_type=\"nf4\",\n",
      "    bnb_4bit_compute_dtype=torch.bfloat16\n",
      ")\n",
      "model_nf4 = SD3Transformer2DModel.from_pretrained(\n",
      "    model_id,\n",
      "    subfolder=\"transformer\",\n",
      "    quantization_config=nf4_config,\n",
      "    torch_dtype=torch.bfloat16\n",
      ")\n",
      "pipeline = StableDiffusion3Pipeline.from_pretrained(\n",
      "    model_id, \n",
      "    transformer=model_nf4,\n",
      "    torch_dtype=torch.bfloat16\n",
      ")\n",
      "pipeline.enable_model_cpu_offload()\n",
      "prompt = \"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. It features the distinctive, bulky body shape of a hippo. However, instead of the usual grey skin, the creature's body resembles a golden-brown, crispy waffle fresh off the griddle. The skin is textured with the familiar grid pattern of a waffle, each square filled with a glistening sheen of syrup. The environment combines the natural habitat of a hippo with elements of a breakfast table setting, a river of warm, melted butter, with oversized utensils or plates peeking out from the lush, pancake-like foliage in the background, a towering pepper mill standing in for a tree.  As the sun rises in this fantastical world, it casts a warm, buttery glow over the scene. The creature, content in its butter river, lets out a yawn. Nearby, a flock of birds take flight\"\n",
      "image = pipeline(\n",
      "    prompt=prompt,\n",
      "    num_inference_steps=28,\n",
      "    guidance_scale=4.5,\n",
      "    max_sequence_length=512,\n",
      ").images[0]\n",
      "image.save(\"whimsical.png\")\n",
      "```\n",
      "Fine-tuning\n",
      "Please see the fine-tuning guide\n",
      "here\n",
      ".\n",
      "Uses\n",
      "Intended Uses\n",
      "Intended uses include the following:\n",
      "* Generation of artworks and use in design and other artistic processes.\n",
      "* Applications in educational or creative tools.\n",
      "* Research on generative models, including understanding the limitations of generative models.\n",
      "All uses of the model must be in accordance with our\n",
      "Acceptable Use Policy\n",
      ".\n",
      "Out-of-Scope Uses\n",
      "The model was not trained to be factual or true representations of people or events.  As such, using the model to generate such content is out-of-scope of the abilities of this model.\n",
      "Safety\n",
      "As part of our safety-by-design and responsible AI deployment approach, we take deliberate measures to ensure Integrity starts at the early stages of development. We implement safety measures throughout the development of our models. We have implemented safety mitigations that are intended to reduce the risk of certain harms, however we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases.\n",
      "For more about our approach to Safety, please visit our\n",
      "Safety page\n",
      ".\n",
      "Integrity Evaluation\n",
      "Our integrity evaluation methods include structured evaluations and red-teaming testing for certain harms.  Testing was conducted primarily in English and may not cover all possible harms.\n",
      "Risks identified and mitigations:\n",
      "Harmful content:  We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm. However, this does not guarantee that all possible harmful content has been removed. TAll developers and deployers should exercise caution and implement content safety guardrails based on their specific product policies and application use cases.\n",
      "Misuse: Technical limitations and developer and end-user education can help mitigate against malicious applications of models. All users are required to adhere to our\n",
      "Acceptable Use Policy\n",
      ", including when applying fine-tuning and prompt engineering mechanisms. Please reference the Stability AI Acceptable Use Policy for information on violative uses of our products.\n",
      "Privacy violations: Developers and deployers are encouraged to adhere to privacy regulations with techniques that respect data privacy.\n",
      "Contact\n",
      "Please report any issues with the model or contact us:\n",
      "Safety issues:  safety@stability.ai\n",
      "Security issues:  security@stability.ai\n",
      "Privacy issues:  privacy@stability.ai\n",
      "License and general: https://stability.ai/license\n",
      "Enterprise license: https://stability.ai/enterprise\n",
      "> RSP * Multimodal Diffusion Transformer architecture for text-to-image generation\n",
      "* Improved performance in image quality and typography\n",
      "* Enhanced complex prompt understanding capabilities\n",
      "* High resource efficiency for consumer hardware\n",
      "* Fast inference times compared to similar-sized models\n",
      "* Availability under a permissive community license for commercial use\n",
      "* Safety mitigations implemented to reduce risks of harmful content\n",
      "* Customizable design for various applications including art and research\n",
      "* Utilizes QK-normalization for improved training stability\n",
      "* Support for low-VRAM usage with quantization options\n"
     ]
    }
   ],
   "source": [
    "models_info = large_models_market_analysis_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d56f3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_d56f3_level0_col0\" class=\"col_heading level0 col0\" >Model Name</th>\n",
       "      <th id=\"T_d56f3_level0_col1\" class=\"col_heading level0 col1\" >Created At</th>\n",
       "      <th id=\"T_d56f3_level0_col2\" class=\"col_heading level0 col2\" >Total Downloads</th>\n",
       "      <th id=\"T_d56f3_level0_col3\" class=\"col_heading level0 col3\" >Total Likes</th>\n",
       "      <th id=\"T_d56f3_level0_col4\" class=\"col_heading level0 col4\" >Trending Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row0_col0\" class=\"data row0 col0\" >deepseek-ai/DeepSeek-V3</td>\n",
       "      <td id=\"T_d56f3_row0_col1\" class=\"data row0 col1\" >25 December 2024 at 12:52:23 UTC</td>\n",
       "      <td id=\"T_d56f3_row0_col2\" class=\"data row0 col2\" >74084</td>\n",
       "      <td id=\"T_d56f3_row0_col3\" class=\"data row0 col3\" >1412</td>\n",
       "      <td id=\"T_d56f3_row0_col4\" class=\"data row0 col4\" >612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row1_col0\" class=\"data row1 col0\" >PowerInfer/SmallThinker-3B-Preview</td>\n",
       "      <td id=\"T_d56f3_row1_col1\" class=\"data row1 col1\" >12 December 2024 at 11:56:09 UTC</td>\n",
       "      <td id=\"T_d56f3_row1_col2\" class=\"data row1 col2\" >6996</td>\n",
       "      <td id=\"T_d56f3_row1_col3\" class=\"data row1 col3\" >288</td>\n",
       "      <td id=\"T_d56f3_row1_col4\" class=\"data row1 col4\" >217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row2_col0\" class=\"data row2 col0\" >deepseek-ai/DeepSeek-V3-Base</td>\n",
       "      <td id=\"T_d56f3_row2_col1\" class=\"data row2 col1\" >25 December 2024 at 12:52:06 UTC</td>\n",
       "      <td id=\"T_d56f3_row2_col2\" class=\"data row2 col2\" >8663</td>\n",
       "      <td id=\"T_d56f3_row2_col3\" class=\"data row2 col3\" >1180</td>\n",
       "      <td id=\"T_d56f3_row2_col4\" class=\"data row2 col4\" >188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row3_col0\" class=\"data row3 col0\" >black-forest-labs/FLUX.1-dev</td>\n",
       "      <td id=\"T_d56f3_row3_col1\" class=\"data row3 col1\" >31 July 2024 at 21:13:44 UTC</td>\n",
       "      <td id=\"T_d56f3_row3_col2\" class=\"data row3 col2\" >1166389</td>\n",
       "      <td id=\"T_d56f3_row3_col3\" class=\"data row3 col3\" >7800</td>\n",
       "      <td id=\"T_d56f3_row3_col4\" class=\"data row3 col4\" >182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row4_col0\" class=\"data row4 col0\" >hexgrad/Kokoro-82M</td>\n",
       "      <td id=\"T_d56f3_row4_col1\" class=\"data row4 col1\" >26 December 2024 at 00:20:08 UTC</td>\n",
       "      <td id=\"T_d56f3_row4_col2\" class=\"data row4 col2\" >1479</td>\n",
       "      <td id=\"T_d56f3_row4_col3\" class=\"data row4 col3\" >269</td>\n",
       "      <td id=\"T_d56f3_row4_col4\" class=\"data row4 col4\" >168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row5_col0\" class=\"data row5 col0\" >meta-llama/Llama-3.3-70B-Instruct</td>\n",
       "      <td id=\"T_d56f3_row5_col1\" class=\"data row5 col1\" >26 November 2024 at 16:08:47 UTC</td>\n",
       "      <td id=\"T_d56f3_row5_col2\" class=\"data row5 col2\" >416929</td>\n",
       "      <td id=\"T_d56f3_row5_col3\" class=\"data row5 col3\" >1517</td>\n",
       "      <td id=\"T_d56f3_row5_col4\" class=\"data row5 col4\" >128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row6_col0\" class=\"data row6 col0\" >StephanST/WALDO30</td>\n",
       "      <td id=\"T_d56f3_row6_col1\" class=\"data row6 col1\" >02 October 2024 at 14:20:40 UTC</td>\n",
       "      <td id=\"T_d56f3_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_d56f3_row6_col3\" class=\"data row6 col3\" >163</td>\n",
       "      <td id=\"T_d56f3_row6_col4\" class=\"data row6 col4\" >102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row7_col0\" class=\"data row7 col0\" >nomic-ai/modernbert-embed-base</td>\n",
       "      <td id=\"T_d56f3_row7_col1\" class=\"data row7 col1\" >29 December 2024 at 23:51:30 UTC</td>\n",
       "      <td id=\"T_d56f3_row7_col2\" class=\"data row7 col2\" >4837</td>\n",
       "      <td id=\"T_d56f3_row7_col3\" class=\"data row7 col3\" >135</td>\n",
       "      <td id=\"T_d56f3_row7_col4\" class=\"data row7 col4\" >89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row8_col0\" class=\"data row8 col0\" >cognitivecomputations/Dolphin3.0-Llama3.1-8B</td>\n",
       "      <td id=\"T_d56f3_row8_col1\" class=\"data row8 col1\" >29 December 2024 at 18:37:00 UTC</td>\n",
       "      <td id=\"T_d56f3_row8_col2\" class=\"data row8 col2\" >242</td>\n",
       "      <td id=\"T_d56f3_row8_col3\" class=\"data row8 col3\" >82</td>\n",
       "      <td id=\"T_d56f3_row8_col4\" class=\"data row8 col4\" >82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d56f3_row9_col0\" class=\"data row9 col0\" >stabilityai/stable-diffusion-3.5-large</td>\n",
       "      <td id=\"T_d56f3_row9_col1\" class=\"data row9 col1\" >22 October 2024 at 07:29:57 UTC</td>\n",
       "      <td id=\"T_d56f3_row9_col2\" class=\"data row9 col2\" >127483</td>\n",
       "      <td id=\"T_d56f3_row9_col3\" class=\"data row9 col3\" >1807</td>\n",
       "      <td id=\"T_d56f3_row9_col4\" class=\"data row9 col4\" >66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x147f58790>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_comparison_table(models_info).style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# deepseek-ai/DeepSeek-V3\n",
       "\n",
       "* Outperforms leading open-source models and rivals closed models.\n",
       "* Utilizes a unique Mixture-of-Experts architecture with 671B parameters.\n",
       "* Features an economical training process requiring only 2.788M GPU hours.\n",
       "* Implements innovative Multi-head Latent Attention for efficient inference.\n",
       "* Introduces an auxiliary-loss-free load balancing strategy.\n",
       "* Achieves remarkable performance across extensive benchmarks, especially in math and code tasks.\n",
       "* Supports a context window length of up to 128K tokens.\n",
       "* Open-source with permissive licensing for commercial use.\n",
       "* Allows for easy local deployment using various optimized frameworks.\n",
       "* Pioneers advancements in FP8 mixed precision training for large-scale models.\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [DeepSeek-V3 Technical Report - arXiv.org](https://arxiv.org/pdf/2412.19437)\n",
       "* [DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat](https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/)\n",
       "* [DeepSeek V3: New Open AI Model Surpasses Rivals and ... - WinBuzzer](https://winbuzzer.com/2024/12/27/deepseek-v3-new-open-ai-model-surpasses-rivals-and-challenges-gpt-4o-xcxwbn/)\n",
       "* [Introducing DeepSeek-V3 | DeepSeek API Docs](https://api-docs.deepseek.com/news/news1226)\n",
       "* [DeepSeek's new AI model appears to be one of the best 'open ...](https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/abs/2402.03300v3)\n",
       "* [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)\n",
       "* [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/abs/2401.02954v1)\n",
       "\n",
       "# PowerInfer/SmallThinker-3B-Preview\n",
       "\n",
       "* Compact size ideal for resource-constrained devices\n",
       "* High-quality inference with minimal computational overhead\n",
       "* Designed for edge deployment and mobile applications\n",
       "* Accelerated performance compared to larger models (up to 70% speedup)\n",
       "* Fine-tuned on specialized datasets for improved accuracy\n",
       "* Strong benchmark performance surpassing some larger models\n",
       "* Versatile applications as a draft model for larger models\n",
       "* Trained with advanced techniques like Supervised Fine-Tuning (SFT)\n",
       "* Accessible on platforms like Hugging Face for easy integration\n",
       "* Holds potential for further enhancement with more extensive datasets and training\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency](https://pub.towardsai.net/smallthinker-3b-a-small-thinking-model-revolutionizing-ai-efficiency-f528cf7d6906)\n",
       "* [bartowski/SmallThinker-3B-Preview-GGUF - Hugging Face](https://huggingface.co/bartowski/SmallThinker-3B-Preview-GGUF)\n",
       "* [PowerInfer/SmallThinker-3B-Preview at main - Hugging Face](https://huggingface.co/PowerInfer/SmallThinker-3B-Preview/tree/main)\n",
       "* [Testing SmallThinker 3B Preview by PowerInfer - YouTube](https://www.youtube.com/watch?v=OVNnXQp_wNU)\n",
       "* [SmallThinker 3B Preview By PowerInfer: Benchmarks, Features and ...](https://llm.extractum.io/model/PowerInfer/SmallThinker-3B-Preview,6YvWQRdkYbb2o0HqvU7LTJ)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](http://arxiv.org/abs/2406.06282v3)\n",
       "* [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](http://arxiv.org/abs/2312.12456v2)\n",
       "\n",
       "# deepseek-ai/DeepSeek-V3-Base\n",
       "\n",
       "* Outperforms existing leading open-source models in benchmarks.\n",
       "* Strong performance on specialized tasks like math and code generation.\n",
       "* Efficient training requiring only 2.788M H800 GPU hours.\n",
       "* Utilizes a Mixture-of-Experts (MoE) architecture with 671B total parameters.\n",
       "* Features innovative Multi-Token Prediction (MTP) training objective.\n",
       "* Pioneers an auxiliary-loss-free load balancing strategy.\n",
       "* Demonstrates stability throughout the training process without significant loss spikes.\n",
       "* Offers competitive performance against closed-source models from major companies.\n",
       "* Supports both FP8 and BF16 inference modes for versatile deployment.\n",
       "* Capable of processing up to 128K context length for extensive input handling.\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat](https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/)\n",
       "* [DeepSeek-V3 Technical Report - arXiv.org](https://arxiv.org/pdf/2412.19437)\n",
       "* [Paper page - DeepSeek-V3 Technical Report - Hugging Face](https://huggingface.co/papers/2412.19437)\n",
       "* [deepseek-ai/DeepSeek-V3-Base](https://simonwillison.net/2024/Dec/25/deepseek-v3/)\n",
       "* [deepseek-ai/DeepSeek-V3 - GitHub](https://github.com/deepseek-ai/DeepSeek-V3)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding](http://arxiv.org/abs/2412.10302v1)\n",
       "* [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/abs/2401.02954v1)\n",
       "* [DeepSeek-VL: Towards Real-World Vision-Language Understanding](http://arxiv.org/abs/2403.05525v2)\n",
       "* [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)\n",
       "* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/abs/2402.03300v3)\n",
       "\n",
       "# black-forest-labs/FLUX.1-dev\n",
       "\n",
       "* 12 billion parameters for advanced text-to-image generation.\n",
       "* Cutting-edge output quality, second only to a flagship model.\n",
       "* Competitive prompt following capabilities that rival closed-source alternatives.\n",
       "* Utilizes guidance distillation for improved efficiency.\n",
       "* Open-weight architecture promotes scientific research and creativity.\n",
       "* Licensed for personal, scientific, and commercial use under specific agreements.\n",
       "* Accessible via multiple platforms, including APIs and local inference tools.\n",
       "* Integrated with node-based workflows in ComfyUI.\n",
       "* Available with sample code for developers and creatives to enhance functionality.\n",
       "* Focus on ensuring responsible usage with strict usage policies.\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [Black Forest Labs - Frontier AI Lab](https://blackforestlabs.ai/)\n",
       "* [FLUX 1.1 - BlackForestLabs](https://blackforestlabs.org/flux-1-1/)\n",
       "* [README.md · black-forest-labs/FLUX.1-dev at main - Hugging Face](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/README.md)\n",
       "* [FLUX.1 Dev | FLUX Dev AI Image Generator by Black Forest Labs](https://flux1ai.com/dev)\n",
       "* [black-forest-labs/FLUX.1-dev - Hugging Face](https://huggingface.co/black-forest-labs/FLUX.1-dev)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models](http://arxiv.org/abs/2411.05007v2)\n",
       "* [Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study](http://arxiv.org/abs/2411.13588v1)\n",
       "* [OminiControl: Minimal and Universal Control for Diffusion Transformer](http://arxiv.org/abs/2411.15098v3)\n",
       "* [Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator](http://arxiv.org/abs/2411.15466v1)\n",
       "* [I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow](http://arxiv.org/abs/2410.07536v2)\n",
       "* [Training-free Regional Prompting for Diffusion Transformers](http://arxiv.org/abs/2411.02395v1)\n",
       "* [Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models](http://arxiv.org/abs/2410.22775v1)\n",
       "* [1.58-bit FLUX](http://arxiv.org/abs/2412.18653v1)\n",
       "\n",
       "# hexgrad/Kokoro-82M\n",
       "\n",
       "* High Elo rating for a small parameter size (82 million)\n",
       "* Achieves competitive performance with less training data (<100 hours)\n",
       "* Supports multiple unique voicepacks for diverse applications\n",
       "* Exclusively trained on permissive/non-copyrighted audio data\n",
       "* No reliance on encoders, utilizing a decoder-only architecture\n",
       "* Built using efficient training methods on cost-effective compute resources\n",
       "* Capable of generating 24kHz audio quality\n",
       "* Offers both fp32 and fp16 model precision options\n",
       "* Designed with a focus on synthetic data usage for enhanced performance\n",
       "* Engaging community support through a Discord server for collaboration and feedback\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [hexgrad/Kokoro-82M · [TODO] FP16 Inference - Hugging Face](https://huggingface.co/hexgrad/Kokoro-82M/discussions/4)\n",
       "* [Models - Hugging Face](https://huggingface.proxy.nlp.skieer.com/models)\n",
       "* [hexgrad/Kokoro-82M · Philosophy - Hugging Face](https://huggingface.co/hexgrad/Kokoro-82M/discussions/5)\n",
       "* [Models - Hugging Face](https://hf.wing.moe/models)\n",
       "* [hexgrad/kokoro: https://hf.co/hexgrad/Kokoro-82M - GitHub](https://github.com/hexgrad/kokoro)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [A Survey of Resource-efficient LLM and Multimodal Foundation Models](http://arxiv.org/abs/2401.08092v2)\n",
       "* [HEMM: Holistic Evaluation of Multimodal Foundation Models](http://arxiv.org/abs/2407.03418v1)\n",
       "* [Enhancing Inflation Nowcasting with LLM: Sentiment Analysis on News](http://arxiv.org/abs/2410.20198v1)\n",
       "* [Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech](http://arxiv.org/abs/2105.06337v2)\n",
       "* [PGTask: Introducing the Task of Profile Generation from Dialogues](http://arxiv.org/abs/2304.06634v2)\n",
       "* [StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis](http://arxiv.org/abs/2205.15439v2)\n",
       "* [StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models](http://arxiv.org/abs/2306.07691v2)\n",
       "* [Extending Whisper with prompt tuning to target-speaker ASR](http://arxiv.org/abs/2312.08079v2)\n",
       "\n",
       "# meta-llama/Llama-3.3-70B-Instruct\n",
       "\n",
       "* 70 billion parameters for high-capacity performance\n",
       "* Multilingual support for eight major languages\n",
       "* Optimized for long context windows up to 128k tokens\n",
       "* Instruction-tuned for enhanced capabilities in dialogue scenarios\n",
       "* Outperforms many existing models on common industry benchmarks\n",
       "* Capable of both text and code output modalities\n",
       "* Incorporates Grouped-Query Attention (GQA) for improved inference scalability\n",
       "* Designed for use in diverse applications, from chatbots to coding assistants\n",
       "* Community-driven license allows for fine-tuning and derivative works\n",
       "* Emphasis on safety and responsible use with built-in safeguards\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [meta / llama-3.3-70b-instruct - docs.api.nvidia.com](https://docs.api.nvidia.com/nim/reference/meta-llama-3_3-70b-instruct)\n",
       "* [osllmai-community/Llama-3.3-70B-Instruct - Hugging Face](https://huggingface.co/osllmai-community/Llama-3.3-70B-Instruct)\n",
       "* [Meta's new Llama 3.3 70B Instruct model now available on watsonx.ai - IBM](https://www.ibm.com/new/announcements/meta-s-new-llama-3-3-70b-instruct-model-now-available-on-watsonx-ai)\n",
       "* [Llama 3.3 70B Instruct - API, Providers, Stats | OpenRouter](https://openrouter.ai/meta-llama/llama-3.3-70b-instruct)\n",
       "* [| [Download The New Model Here!]](https://llamaimodel.com/3-70b/)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [Hermes 3 Technical Report](http://arxiv.org/abs/2408.11857v1)\n",
       "* [MGH Radiology Llama: A Llama 3 70B Model for Radiology](http://arxiv.org/abs/2408.11848v2)\n",
       "* [Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging: A Comprehensive Evaluation](http://arxiv.org/abs/2406.14971v1)\n",
       "* [The Llama 3 Herd of Models](http://arxiv.org/abs/2407.21783v3)\n",
       "* [ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities](http://arxiv.org/abs/2407.14482v2)\n",
       "* [Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data](http://arxiv.org/abs/2409.00096v1)\n",
       "* [Tulu 3: Pushing Frontiers in Open Language Model Post-Training](http://arxiv.org/abs/2411.15124v2)\n",
       "* [A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio](http://arxiv.org/abs/2409.06624v1)\n",
       "* [The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization](http://arxiv.org/abs/2408.15301v2)\n",
       "* [LiveBench: A Challenging, Contamination-Free LLM Benchmark](http://arxiv.org/abs/2406.19314v1)\n",
       "\n",
       "# StephanST/WALDO30\n",
       "\n",
       "* Specialized for civilian use cases in overhead imagery\n",
       "* Based on a large YOLO-v8 backbone architecture\n",
       "* Capable of detecting a wide variety of object classes\n",
       "* Focused on low-altitude to satellite imagery detection\n",
       "* Supports both synthetic and augmented training data\n",
       "* Open weights for flexible deployment in various applications\n",
       "* Provides outputs of bounding boxes and class labels\n",
       "* Ideal for diverse applications like disaster recovery and wildlife monitoring\n",
       "* Free and open-source under MIT license\n",
       "* User-friendly with boilerplate code and adaptation options for fine-tuning\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [StephanST/WALDO30 · Gumroad Link - Hugging Face](https://huggingface.co/StephanST/WALDO30/discussions/1)\n",
       "* [Waldo30 - use with Halio - General - Hailo Community](https://community.hailo.ai/t/waldo30-use-with-halio/4832)\n",
       "* [StephanST/WALDO30 - Hugging Face](https://huggingface.co/StephanST/WALDO30)\n",
       "* [WALDO30 | AI Model Details](https://www.aimodels.fyi/models/huggingFace/waldo30-stephanst)\n",
       "* [StephanST/WALDO30 at main - Hugging Face](https://huggingface.co/StephanST/WALDO30/tree/main)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [Dark Matter (H)eats Young Planets](http://arxiv.org/abs/2309.02495v3)\n",
       "* [A Multi-Wavelength Technique for Estimating Galaxy Cluster Mass Accretion Rates](http://arxiv.org/abs/2412.05370v1)\n",
       "* [Kernel Methods for Interferometric Imaging](http://arxiv.org/abs/2412.01908v1)\n",
       "* [Strong-lensing cosmography using third-generation gravitational-wave detectors](http://arxiv.org/abs/2405.17805v2)\n",
       "* [A Broad-line, Low-luminosity Active Galactic Nucleus at ${z=7.3}$ Anchoring a Large Galaxy Overdensity](http://arxiv.org/abs/2411.11534v1)\n",
       "* [Applications of machine learning in gravitational wave research with current interferometric detectors](http://arxiv.org/abs/2412.15046v1)\n",
       "* [MSA-3D: Metallicity Gradients in Galaxies at $z\\sim1$ with JWST/NIRSpec Slit-stepping Spectroscopy](http://arxiv.org/abs/2409.01616v3)\n",
       "* [JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust Scattering in the HH 30 disk](http://arxiv.org/abs/2412.07523v1)\n",
       "\n",
       "# nomic-ai/modernbert-embed-base\n",
       "\n",
       "* Trained on 235 million documents, enhancing robustness and diversity of learning.\n",
       "* Implements Matryoshka Representation Learning dimensions of 256 to optimize performance.\n",
       "* Reduces memory usage by 3x with minimal performance loss compared to other models.\n",
       "* Strong performance on various tasks such as search, classification, and clustering.\n",
       "* Outperforms predecessor models on standard metrics within the MTEB benchmark.\n",
       "* Supports both weakly-supervised and supervised datasets for versatile training capabilities.\n",
       "* Fine-tuned from ModernBERT-base with 149M parameters for improved embedding efficacy.\n",
       "* Provides features for sentence similarity and retrieval tasks, increasing practical applications.\n",
       "* Open-source model enabling community collaboration and development.\n",
       "* Designed for flexible input handling, requiring prefixes for better contextual understanding.\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [nomic-ai/modernbert-embed-base-unsupervised - Hugging Face](https://huggingface.co/nomic-ai/modernbert-embed-base-unsupervised)\n",
       "* [Nomic AI Launches ModernBERT-Embed-Base, Trained on... | DeepNewz](https://deepnewz.com/ai-modeling/nomic-ai-launches-modernbert-embed-base-trained-on-235-million-documents-256-f4acadfc)\n",
       "* [Install ModernBERT Embed Locally - Great New RAG Model](https://www.youtube.com/watch?v=HcVav0IqZlk)\n",
       "* [nomic-ai/modernbert-embed-base - Hugging Face](https://huggingface.co/nomic-ai/modernbert-embed-base)\n",
       "* [@tomaarsen on Hugging Face: \"That didn't take long! Nomic AI has ...](https://huggingface.co/posts/tomaarsen/118953506218716)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models](http://arxiv.org/abs/2405.05374v1)\n",
       "* [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](http://arxiv.org/abs/2412.13663v2)\n",
       "* [Nomic Embed: Training a Reproducible Long Context Text Embedder](http://arxiv.org/abs/2402.01613v1)\n",
       "* [Understanding Generative AI Content with Embedding Models](http://arxiv.org/abs/2408.10437v2)\n",
       "\n",
       "# cognitivecomputations/Dolphin3.0-Llama3.1-8B\n",
       "\n",
       "* Steerable AI model allowing complete control over system prompts and alignment.\n",
       "* General purpose functionality for coding, math, and agentic tasks.\n",
       "* Local-first approach, enabling self-hosting and data privacy.\n",
       "* Customizable guidelines and ethics, tailored to user needs.\n",
       "* Built on a robust architecture with advanced multimodal support.\n",
       "* Regular updates and user maintenance over versions without silent changes.\n",
       "* Utilizes open-source datasets for training, promoting transparency.\n",
       "* Extensive performance benchmarking against leading models in the industry.\n",
       "* Strong community support and collaborative development.\n",
       "* Inference optimized for fast and efficient response times on local hardware.\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [Dolphin3.0 Llama3.1 8B by cognitivecomputations](https://llm.extractum.io/model/cognitivecomputations/Dolphin3.0-Llama3.1-8B,7rToucpFJjB0d8WeBFJ57p)\n",
       "* [cognitivecomputations/dolphin-2.9.4-llama3.1-8b - Hugging Face](https://huggingface.co/cognitivecomputations/dolphin-2.9.4-llama3.1-8b)\n",
       "* [Dolphin 2.9 Llama 3 8b Curated and trained by Eric ... - Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1c95z5k/dolphin_29_llama_3_8b_curated_and_trained_by_eric/)\n",
       "* [Cognitivecomputations/Dolphin3.0-Llama3.1-8B uncensored - Hacker News](https://news.ycombinator.com/item?id=42607271)\n",
       "* [Dolphin 3.0 Released (Llama 3.1 + 3.2 + Qwen 2.5): A Local-First ...](https://www.marktechpost.com/2025/01/05/dolphin-3-0-released-llama-3-1-3-2-qwen-2-5-a-local-first-steerable-ai-model-that-puts-you-in-control-of-your-ai-stack-and-alignment/)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [Hermes 3 Technical Report](http://arxiv.org/abs/2408.11857v1)\n",
       "* [Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders](http://arxiv.org/abs/2410.20526v1)\n",
       "* [Extending Llama-3's Context Ten-Fold Overnight](http://arxiv.org/abs/2404.19553v1)\n",
       "* [The Llama 3 Herd of Models](http://arxiv.org/abs/2407.21783v3)\n",
       "* [Llama 3 Meets MoE: Efficient Upcycling](http://arxiv.org/abs/2412.09952v1)\n",
       "* [Applying Refusal-Vector Ablation to Llama 3.1 70B Agents](http://arxiv.org/abs/2410.10871v1)\n",
       "* [Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction](http://arxiv.org/abs/2410.23692v1)\n",
       "* [The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization](http://arxiv.org/abs/2408.15301v2)\n",
       "* [EXAONE 3.0 7.8B Instruction Tuned Language Model](http://arxiv.org/abs/2408.03541v3)\n",
       "\n",
       "# stabilityai/stable-diffusion-3.5-large\n",
       "\n",
       "* Multimodal Diffusion Transformer architecture for text-to-image generation\n",
       "* Improved performance in image quality and typography\n",
       "* Enhanced complex prompt understanding capabilities\n",
       "* High resource efficiency for consumer hardware\n",
       "* Fast inference times compared to similar-sized models\n",
       "* Availability under a permissive community license for commercial use\n",
       "* Safety mitigations implemented to reduce risks of harmful content\n",
       "* Customizable design for various applications including art and research\n",
       "* Utilizes QK-normalization for improved training stability\n",
       "* Support for low-VRAM usage with quantization options\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [stabilityai/stable-diffusion-3.5-large · Hugging Face](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)\n",
       "* [stability-ai/stable-diffusion-3.5-large - Run with an API on Replicate](https://replicate.com/stability-ai/stable-diffusion-3.5-large)\n",
       "* [Stable Diffusion 3.5 (Large): What You Need to Know](https://sandner.art/stable-diffusion-35-large-what-you-need-to-know/)\n",
       "* [Introducing Stable Diffusion 3.5 - Stability AI](https://stability.ai/news/introducing-stable-diffusion-3-5)\n",
       "* [Stability AI](https://stability.ai/?ref=aifindpro)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* [Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss](http://arxiv.org/abs/2401.02677v1)\n",
       "* [Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets](http://arxiv.org/abs/2311.15127v1)\n",
       "* [Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study](http://arxiv.org/abs/2411.13588v1)\n",
       "* [Towards More Realistic Membership Inference Attacks on Large Diffusion Models](http://arxiv.org/abs/2306.12983v2)\n",
       "* [Taming Stable Diffusion for Text to 360° Panorama Image Generation](http://arxiv.org/abs/2404.07949v1)\n",
       "* [DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models](http://arxiv.org/abs/2210.14896v4)\n",
       "* [Context-Aware Full Body Anonymization using Text-to-Image Diffusion Models](http://arxiv.org/abs/2410.08551v2)\n",
       "* [Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training](http://arxiv.org/abs/2312.16204v3)\n",
       "* [Degeneration-Tuning: Using Scrambled Grid shield Unwanted Concepts from Stable Diffusion](http://arxiv.org/abs/2308.02552v2)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(get_models_overview(models_info)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-analysis-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
