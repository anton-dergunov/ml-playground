{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Jinja2 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (3.1.5)\n",
      "Collecting markdown\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/market-analysis-agents/lib/python3.11/site-packages (from Jinja2) (3.0.2)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: markdown\n",
      "Successfully installed markdown-3.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Jinja2 markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from tavily import TavilyClient\n",
    "from openai import OpenAI\n",
    "import arxiv\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "from IPython.display import Markdown\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import re\n",
    "from inspect import signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = \"\"\n",
    "OPENAI_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api = HfApi()\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "arxiv_client = arxiv.Client()\n",
    "llm_client = OpenAI(api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tool_request(request):\n",
    "    print(f\"< REQ {request}\")\n",
    "\n",
    "def log_tool_response(response):\n",
    "    print(f\"> RSP {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tool_invocation(description=None):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Extract argument names and values\n",
    "            func_sig = signature(func)\n",
    "            bound_args = func_sig.bind(*args, **kwargs)\n",
    "            bound_args.apply_defaults()\n",
    "            args_str = \", \".join(f\"{key}={value}\" for key, value in bound_args.arguments.items())\n",
    "            \n",
    "            log_tool_request(f\"{description} {args_str}\")\n",
    "            \n",
    "            result = func(*args, **kwargs)\n",
    "            log_tool_response(result)\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving top large models from HuggingFace Hub\")\n",
    "def get_top_models(n = 10):\n",
    "    models = hf_api.list_models(sort=\"trending_score\", limit=n)\n",
    "    return [model.modelId for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_datetime_readable(dt):\n",
    "    return dt.strftime('%d %B %Y at %H:%M:%S %Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving model information from HuggingFace Hub\")\n",
    "def get_model_info_from_hugging_face(model_id):\n",
    "    model_info = hf_api.model_info(repo_id=model_id, expand=[\"createdAt\", \"downloads\", \"likes\", \"trendingScore\"])\n",
    "\n",
    "    description = read_file(hf_api.hf_hub_download(model_id, 'README.md'))\n",
    "\n",
    "    model_info = {\n",
    "        \"model_id\": model_id,\n",
    "        \"created_at\": format_datetime_readable(model_info.created_at),\n",
    "        \"downloads\": model_info.downloads,\n",
    "        \"likes\": model_info.likes,\n",
    "        \"trending_score\": model_info.trending_score,\n",
    "        \"description\": description\n",
    "        }\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving model information on the web using Tavily\")\n",
    "def get_model_info_on_the_web(model_id):\n",
    "    response = tavily_client.search(model_id)\n",
    "    return response['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving model information on arxiv documents using Tavily\")\n",
    "def get_model_info_on_arxiv(model_id):\n",
    "    response = tavily_client.search(model_id, include_domains=[\"arxiv.org\"], max_results=10)\n",
    "    return response['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arxiv_ids_from_search_results(search_results):\n",
    "    arxiv_ids = set()\n",
    "    for result in search_results:\n",
    "        url = result['url']\n",
    "\n",
    "        # Extract the ArXiv ID from the URL\n",
    "        match = re.search(r'arxiv\\.org/(abs|pdf|html)/([\\d.]+)(v\\d+)?', url)\n",
    "        if match:\n",
    "            arxiv_id = match.group(2)\n",
    "            arxiv_ids.add(arxiv_id)\n",
    "    return arxiv_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Retrieving paper information using Arxiv API\")\n",
    "def get_arxiv_links_by_id(arxiv_id):\n",
    "    search = arxiv.Search(id_list=[arxiv_id])\n",
    "    for paper in arxiv_client.results(search):\n",
    "        return f\"[{paper.title}]({paper})\" # assume there is only one paper\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_tool_invocation(\"Calling OpenAI gpt-4o-mini model\")\n",
    "def call_llm(system_prompt, message):\n",
    "    completion = llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]}\n",
    "        ])\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_markdown_formatting(markdown_content):\n",
    "    html_content = markdown.markdown(markdown_content)\n",
    "    \n",
    "    # Use BeautifulSoup to remove HTML tags and get plain text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text(separator='\\n', strip=True)\n",
    "    return text[:50_000] # trim so that LLM calls don't fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT = \"\"\"You are a helpful assistant that performs short and concise summarization of the large machine learning models.\n",
    "# Below you would see the contents of the README FILE (MODEL CARD), as well as the WEB SEARCH RESULTS for this model.\n",
    "# Use both of these sources to construct the summary, but prioritize web search results.\n",
    "# Use bullets to summarize the model competitive characteristics, and pay special attention to mention those characteristics which best differenciate this model from any others.\"\"\"\n",
    "\n",
    "# SYSTEM_PROMPT = \"\"\"Summarize using bullets.\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
    "\n",
    "Example of the required output:\n",
    "* Characteristic 1\n",
    "* Characteristic 2\n",
    "* Characteristic 3\n",
    "...\"\"\"\n",
    "\n",
    "def get_model_competitive_overview(model_description, model_web_info):\n",
    "    model_info = f\"# WEB SEARCH RESULTS\\n{model_web_info}\\n\\n# README FILE (MODEL CARD)\\n{remove_markdown_formatting(model_description)}\"\n",
    "    return call_llm(SYSTEM_PROMPT, model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_links_from_search_results(search_results):\n",
    "    return set([f\"[{result['title']}]({result['url']})\" for result in search_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[DeepSeek v3 - Advanced AI & LLM Model Online](https://deepseekv3.org/)',\n",
       " '[DeepSeek-V3 Technical Report - arXiv.org](https://arxiv.org/pdf/2412.19437)',\n",
       " '[DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat](https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/)',\n",
       " '[Introducing DeepSeek-V3 | DeepSeek API Docs](https://api-docs.deepseek.com/news/news1226)',\n",
       " '[deepseek-ai/DeepSeek-V3 at main - Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3/tree/main)'}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_web_links_from_search_results(web_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'deepseek-ai/DeepSeek-V3',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': 'DeepSeek-V3 is Now The Best Open Source AI Model - Analytics India Magazine',\n",
       "   'url': 'https://analyticsindiamag.com/ai-news-updates/deepseek-v3-is-the-best-open-source-ai-model/',\n",
       "   'content': 'DeepSeek-V3 is Now The Best Open Source AI Model In AI News DeepSeek-V3 is Now The Best Open Source AI Model DeepSeek, a Chinese AI research lab backed by High-Flyer Capital Management has released DeepSeek-V3, the latest version of their frontier model. DeepSeek AI also released the benchmark scores, and it outperformed Meta’s flagship Llama 3.1 405B parameter model, among many other closed-source models. DeepSeek-V3 is Now The Best Open Source AI Model Rising 2025 | DE&I in Tech & AI AI Startups Conference. AI Forum for India ADaSci Corporate training program on Generative AI provides a unique opportunity to empower, retain and advance your talent AIM Research produces a series of annual reports on AI & Data Science covering every aspect of the industry.',\n",
       "   'score': 0.8619225,\n",
       "   'raw_content': None},\n",
       "  {'title': 'DeepSeek-V3 Technical Report - arXiv.org',\n",
       "   'url': 'https://arxiv.org/pdf/2412.19437',\n",
       "   'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-',\n",
       "   'score': 0.8479807,\n",
       "   'raw_content': None},\n",
       "  {'title': 'DeepSeek v3 - State-of-the-Art Large Language Model',\n",
       "   'url': 'https://deepseekv3.org/',\n",
       "   'content': \"DeepSeek v3 DeepSeek v3: Advanced AI Language Model DeepSeek v3 Capabilities How to Use DeepSeek v3 Discover how DeepSeek v3 is advancing the field of AI language models DeepSeek v3 combines a massive 671B parameter MoE architecture with innovative features like Multi-Token Prediction and auxiliary-loss-free load balancing, delivering exceptional performance across various tasks. How does DeepSeek v3 compare to other language models? How was DeepSeek v3 trained? What makes DeepSeek v3's training efficient? About DeepSeek v3 DeepSeek v3 represents the latest advancement in large language models, featuring a groundbreaking Mixture-of-Experts architecture with 671B total parameters. Trained on 14.8 trillion diverse tokens and incorporating advanced techniques like Multi-Token Prediction, DeepSeek v3 sets new standards in AI language modeling. DeepSeek v3\",\n",
       "   'score': 0.8095324,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Introducing DeepSeek-V3 | DeepSeek API Docs',\n",
       "   'url': 'https://api-docs.deepseek.com/news/news1226',\n",
       "   'content': '🚀 Introducing DeepSeek-V3 | DeepSeek API Docs DeepSeek API Docs DeepSeek Platform Your First API Call Introducing DeepSeek-V3 2024/12/26 DeepSeek-V2.5-1210 Release 2024/12/10 DeepSeek-V2.5 Release 2024/09/05 Context Caching is Available 2024/08/02 New API Features 2024/07/25 API Reference API Guides Context Caching API Status Page Introducing DeepSeek-V3 2024/12/26 🚀 Introducing DeepSeek-V3 ⚡ 60 tokens/second (3x faster than V2!) Model 👉 https://github.com/deepseek-ai/DeepSeek-V3 Paper 👉 https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf 💰 API Pricing Update\\u200b 🎉 Until Feb 8: same as V2! | Input (cache miss) | Input (cache hit) | Output | Look forward to multimodal support and other cutting-edge features in the DeepSeek ecosystem. Previous Error CodesNext 🚀 DeepSeek V2.5: The Grand Finale 🎉 🎉 What’s new in V3 💰 API Pricing Update Copyright © 2024 DeepSeek, Inc.',\n",
       "   'score': 0.780947,\n",
       "   'raw_content': None},\n",
       "  {'title': 'PDF',\n",
       "   'url': 'https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf',\n",
       "   'content': 'deepseek-ai / DeepSeek-V3 Public. Notifications You must be signed in to change notification settings; Fork 1.2k; Star 16.3k. Code; Issues 46; Pull requests 6; Actions; Projects 0; Security; Insights; Files main. Breadcrumbs. DeepSeek-V3 / DeepSeek_V3.pdf. Latest commit History',\n",
       "   'score': 0.68318754,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.68}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_model_competitive_overview(get_model_info_from_hugging_face(\"deepseek-ai/DeepSeek-V3\")['description'], web_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 671 billion total parameters with a Mixture-of-Experts architecture.\n",
      "* 37 billion activated parameters for efficient resource utilization.\n",
      "* Introduces an auxiliary-loss-free load balancing strategy.\n",
      "* Utilizes Multi-Token Prediction for enhanced performance and inference acceleration.\n",
      "* Trained on a diverse dataset of 14.8 trillion tokens for comprehensive understanding.\n",
      "* Achieves performance superior to other open-source models and rivals closed-source counterparts.\n",
      "* Requires only 2.788 million GPU hours for full training, highlighting efficiency.\n",
      "* Offers stable training with no incidents of loss spikes or rollbacks.\n",
      "* Supports multiple inference frameworks, including SGLang, LMDeploy, and TensorRT-LLM.\n",
      "* Capable of handling context windows up to 128k tokens for versatile applications.\n"
     ]
    }
   ],
   "source": [
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< REQ Retrieving top large models from HuggingFace Hub n=10\n",
      "> RSP ['deepseek-ai/DeepSeek-V3', 'PowerInfer/SmallThinker-3B-Preview', 'deepseek-ai/DeepSeek-V3-Base', 'black-forest-labs/FLUX.1-dev', 'hexgrad/Kokoro-82M', 'meta-llama/Llama-3.3-70B-Instruct', 'StephanST/WALDO30', 'nomic-ai/modernbert-embed-base', 'cognitivecomputations/Dolphin3.0-Llama3.1-8B', 'stabilityai/stable-diffusion-3.5-large']\n",
      "deepseek-ai/DeepSeek-V3\n",
      "PowerInfer/SmallThinker-3B-Preview\n",
      "deepseek-ai/DeepSeek-V3-Base\n",
      "black-forest-labs/FLUX.1-dev\n",
      "hexgrad/Kokoro-82M\n",
      "meta-llama/Llama-3.3-70B-Instruct\n",
      "StephanST/WALDO30\n",
      "nomic-ai/modernbert-embed-base\n",
      "cognitivecomputations/Dolphin3.0-Llama3.1-8B\n",
      "stabilityai/stable-diffusion-3.5-large\n"
     ]
    }
   ],
   "source": [
    "top_models = get_top_models()\n",
    "print(\"\\n\".join(top_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'deepseek-ai/DeepSeek-V3',\n",
       " 'created_at': '25 December 2024 at 12:52:23 UTC',\n",
       " 'downloads': 71747,\n",
       " 'likes': 1321,\n",
       " 'trending_score': 599,\n",
       " 'description': '<!-- markdownlint-disable first-line-h1 -->\\n<!-- markdownlint-disable html -->\\n<!-- markdownlint-disable no-duplicate-header -->\\n\\n<div align=\"center\">\\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\\n</div>\\n<hr>\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n\\n<p align=\"center\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>👁️</a>\\n</p>\\n\\n\\n## 1. Introduction\\n\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \\nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \\nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \\nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \\nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\\nIn addition, its training process is remarkably stable. \\nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/benchmark.png\">\\n</p>\\n\\n## 2. Model Summary\\n\\n---\\n\\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\\n\\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \\n    It can also be used for speculative decoding for inference acceleration. \\n\\n---\\n\\n**Pre-Training: Towards Ultimate Training Efficiency**\\n\\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \\n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \\n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \\n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\\n\\n---\\n\\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\\n\\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\\n\\n---\\n\\n\\n## 3. Model Downloads\\n\\n<div align=\"center\">\\n\\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\\n| :------------: | :------------: | :------------: | :------------: | :------------: |\\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\\n\\n</div>\\n\\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\\n\\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\\n\\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\\n\\n## 4. Evaluation Results\\n### Base Model\\n#### Standard Benchmarks\\n\\n<div align=\"center\">\\n\\n\\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\\n|---|-------------------|----------|--------|-------------|---------------|---------|\\n| | Architecture | - | MoE | Dense | Dense | MoE |\\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\\n| | # Total Params | - | 236B | 72B | 405B | 671B |\\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\\n\\n</div>\\n\\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\\nFor more evaluation details, please check our paper. \\n\\n#### Context Window\\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/niah.png\">\\n</p>\\n\\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \\n\\n### Chat Model\\n#### Standard Benchmarks (Models larger than 67B)\\n<div align=\"center\">\\n\\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\\n\\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\\n\\n</div>\\n\\n\\n####  Open Ended Generation Evaluation\\n\\n<div align=\"center\">\\n\\n\\n\\n| Model | Arena-Hard | AlpacaEval 2.0 |\\n|-------|------------|----------------|\\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\\n| LLaMA-3.1 405B | 69.3 | 40.5 |\\n| GPT-4o-0513 | 80.4 | 51.1 |\\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\\n| DeepSeek-V3 | **85.5** | **70.0** |\\n\\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\\n</div>\\n\\n\\n## 5. Chat Website & API Platform\\nYou can chat with DeepSeek-V3 on DeepSeek\\'s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\\n\\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\\n\\n## 6. How to Run Locally\\n\\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\\n\\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\\n\\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\\n\\nHere is an example of converting FP8 weights to BF16:\\n\\n```shell\\ncd inference\\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\\n```\\n\\n**NOTE: Huggingface\\'s Transformers has not been directly supported yet.**\\n\\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\\n\\n#### Model Weights & Demo Code Preparation\\n\\nFirst, clone our DeepSeek-V3 GitHub repository:\\n\\n```shell\\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\\n```\\n\\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\\n\\n```shell\\ncd DeepSeek-V3/inference\\npip install -r requirements.txt\\n```\\n\\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\\n\\n#### Model Weights Conversion\\n\\nConvert HuggingFace model weights to a specific format:\\n\\n```shell\\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\\n```\\n\\n#### Run\\n\\nThen you can chat with DeepSeek-V3:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\\n```\\n\\nOr batch inference on a given file:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\\n```\\n\\n### 6.2 Inference with SGLang (recommended)\\n\\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\\n\\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\\n\\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\\n\\n### 6.3 Inference with LMDeploy (recommended)\\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\\n\\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\\n\\n\\n### 6.4 Inference with TRT-LLM (recommended)\\n\\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \\n\\n### 6.5 Inference with vLLM (recommended)\\n\\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\\n\\n### 6.6 Recommended Inference Functionality with AMD GPUs\\n\\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\\n\\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\\n\\n\\n## 7. License\\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\\n\\n## 8. Citation\\n```\\n@misc{deepseekai2024deepseekv3technicalreport,\\n      title={DeepSeek-V3 Technical Report}, \\n      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},\\n      year={2024},\\n      eprint={2412.19437},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.19437}, \\n}\\n```\\n\\n## 9. Contact\\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\\n'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_info_from_hugging_face(\"deepseek-ai/DeepSeek-V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_info = get_model_info_on_the_web(\"deepseek-ai/DeepSeek-V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat',\n",
       "  'url': 'https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/',\n",
       "  'content': 'DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch | VentureBeat DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B.',\n",
       "  'score': 0.8727061,\n",
       "  'raw_content': None},\n",
       " {'title': 'DeepSeek-V3 Technical Report - arXiv.org',\n",
       "  'url': 'https://arxiv.org/pdf/2412.19437',\n",
       "  'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-',\n",
       "  'score': 0.8479807,\n",
       "  'raw_content': None},\n",
       " {'title': 'DeepSeek v3 - Advanced AI & LLM Model Online',\n",
       "  'url': 'https://deepseekv3.org/',\n",
       "  'content': \"DeepSeek v3 DeepSeek v3: Advanced AI Language Model DeepSeek v3 Capabilities How to Use DeepSeek v3 Discover how DeepSeek v3 is advancing the field of AI language models DeepSeek v3 combines a massive 671B parameter MoE architecture with innovative features like Multi-Token Prediction and auxiliary-loss-free load balancing, delivering exceptional performance across various tasks. How does DeepSeek v3 compare to other language models? How was DeepSeek v3 trained? What makes DeepSeek v3's training efficient? About DeepSeek v3 DeepSeek v3 represents the latest advancement in large language models, featuring a groundbreaking Mixture-of-Experts architecture with 671B total parameters. Trained on 14.8 trillion diverse tokens and incorporating advanced techniques like Multi-Token Prediction, DeepSeek v3 sets new standards in AI language modeling. DeepSeek v3\",\n",
       "  'score': 0.8095324,\n",
       "  'raw_content': None},\n",
       " {'title': 'Introducing DeepSeek-V3 | DeepSeek API Docs',\n",
       "  'url': 'https://api-docs.deepseek.com/news/news1226',\n",
       "  'content': '🚀 Introducing DeepSeek-V3 | DeepSeek API Docs DeepSeek API Docs DeepSeek Platform Your First API Call Introducing DeepSeek-V3 2024/12/26 DeepSeek-V2.5-1210 Release 2024/12/10 DeepSeek-V2.5 Release 2024/09/05 Context Caching is Available 2024/08/02 New API Features 2024/07/25 API Reference API Guides Context Caching API Status Page Introducing DeepSeek-V3 2024/12/26 🚀 Introducing DeepSeek-V3 ⚡ 60 tokens/second (3x faster than V2!) Model 👉 https://github.com/deepseek-ai/DeepSeek-V3 Paper 👉 https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf 💰 API Pricing Update\\u200b 🎉 Until Feb 8: same as V2! | Input (cache miss) | Input (cache hit) | Output | Look forward to multimodal support and other cutting-edge features in the DeepSeek ecosystem. Previous Error CodesNext 🚀 DeepSeek V2.5: The Grand Finale 🎉 🎉 What’s new in V3 💰 API Pricing Update Copyright © 2024 DeepSeek, Inc.',\n",
       "  'score': 0.780947,\n",
       "  'raw_content': None},\n",
       " {'title': 'deepseek-ai/DeepSeek-V3 at main - Hugging Face',\n",
       "  'url': 'https://huggingface.co/deepseek-ai/DeepSeek-V3/tree/main',\n",
       "  'content': 'deepseek-ai/DeepSeek-V3 at main Release DeepSeek-V3 11 days ago Fix fp8_cast_bf16.py: https://github.com/deepseek-ai/DeepSeek-V3/commit/8f1c9488b53068992f9525fab03b1868e6f7c8c1 10 days ago 1.06 kBRelease DeepSeek-V3 11 days ago *   LICENSE-MODEL 13.8 kBRelease DeepSeek-V3 11 days ago 22.6 kBUpdate README.md 7 days ago 3.66 kBRelease DeepSeek-V3 11 days ago 1.73 kBAdd files using upload-large-folder tool 11 days ago 10.6 kBAdd files using upload-large-folder tool 11 days ago 5.23 GB LFSAdd files using upload-large-folder tool 11 days ago 4.3 GB LFSAdd files using upload-large-folder tool 11 days ago 4.3 GB LFSAdd files using upload-large-folder tool 11 days ago 4.3 GB LFSAdd files using upload-large-folder tool 11 days ago 4.3 GB LFSAdd files using upload-large-folder tool 11 days ago 4.37 GB LFSAdd files using upload-large-folder tool 11 days ago',\n",
       "  'score': 0.7646988,\n",
       "  'raw_content': None}]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[DeepSeek-VL: Towards Real-World Vision-Language Understanding](http://arxiv.org/abs/2403.05525v2)', '[DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)', '[DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/abs/2401.02954v1)']\n"
     ]
    }
   ],
   "source": [
    "arxiv_search_results = get_model_info_on_arxiv(\"deepseek-ai/DeepSeek-V3\")\n",
    "arxiv_ids = get_arxiv_ids_from_search_results(arxiv_search_results)\n",
    "print([get_arxiv_links_by_id(arxiv_id) for arxiv_id in arxiv_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Add logs to all the tool invocations\n",
    "* Write code to combine all calls\n",
    "* Call LLM to summarize information for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_models_market_analysis_agent():\n",
    "    top_models = get_top_models()\n",
    "\n",
    "    models_info = {}\n",
    "    for model_id in top_models:\n",
    "        models_info[model_id] = get_model_info_from_hugging_face(model_id)\n",
    "\n",
    "        models_info[model_id]['web_info'] = get_model_info_on_the_web(model_id)\n",
    "        models_info[model_id]['web_links'] = get_web_links_from_search_results(models_info[model_id]['web_info'])\n",
    "\n",
    "        arxiv_search_results = get_model_info_on_arxiv(model_id)\n",
    "        arxiv_ids = get_arxiv_ids_from_search_results(arxiv_search_results)\n",
    "        models_info[model_id]['papers'] = [get_arxiv_links_by_id(arxiv_id) for arxiv_id in arxiv_ids]\n",
    "\n",
    "        models_info[model_id]['competitive_overview'] = get_model_competitive_overview(models_info[model_id]['description'], models_info[model_id]['web_info'])\n",
    "\n",
    "    return models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison_table(models_info):\n",
    "    required_columns = {\n",
    "        \"created_at\": \"Created At\",\n",
    "        \"downloads\": \"Total Downloads\",\n",
    "        \"likes\": \"Total Likes\",\n",
    "        \"trending_score\": \"Trending Score\",\n",
    "    }\n",
    "    \n",
    "    # Extract only the required columns\n",
    "    filtered_data = [\n",
    "        {**{\"Model Name\": model}, **{required_columns[key]: details.get(key) for key in required_columns}}\n",
    "        for model, details in models_info.items()\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_overview(models_info):\n",
    "    md_content = \"\"\n",
    "\n",
    "    for model_id, details in models_info.items():\n",
    "        md_content += f\"\\n# {model_id}\\n\\n\"\n",
    "        md_content += details['competitive_overview']\n",
    "        md_content += f\"\\n\\nMentioned in the following web pages:\\n\"\n",
    "        for web_link in details['web_links']:\n",
    "            md_content += f\"* {web_link}\\n\"\n",
    "        md_content += f\"\\nMentioned in the following papers:\\n\"\n",
    "        for paper in details['papers']:\n",
    "            md_content += f\"* {paper}\\n\"\n",
    "    return md_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< REQ Retrieving top large models from HuggingFace Hub n=10\n",
      "> RSP ['deepseek-ai/DeepSeek-V3', 'PowerInfer/SmallThinker-3B-Preview', 'deepseek-ai/DeepSeek-V3-Base', 'black-forest-labs/FLUX.1-dev', 'hexgrad/Kokoro-82M', 'meta-llama/Llama-3.3-70B-Instruct', 'StephanST/WALDO30', 'nomic-ai/modernbert-embed-base', 'cognitivecomputations/Dolphin3.0-Llama3.1-8B', 'stabilityai/stable-diffusion-3.5-large']\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=deepseek-ai/DeepSeek-V3\n",
      "> RSP {'model_id': 'deepseek-ai/DeepSeek-V3', 'created_at': '25 December 2024 at 12:52:23 UTC', 'downloads': 74084, 'likes': 1412, 'trending_score': 612, 'description': '<!-- markdownlint-disable first-line-h1 -->\\n<!-- markdownlint-disable html -->\\n<!-- markdownlint-disable no-duplicate-header -->\\n\\n<div align=\"center\">\\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\\n</div>\\n<hr>\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n\\n<p align=\"center\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>👁️</a>\\n</p>\\n\\n\\n## 1. Introduction\\n\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \\nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \\nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \\nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \\nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\\nIn addition, its training process is remarkably stable. \\nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/benchmark.png\">\\n</p>\\n\\n## 2. Model Summary\\n\\n---\\n\\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\\n\\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \\n    It can also be used for speculative decoding for inference acceleration. \\n\\n---\\n\\n**Pre-Training: Towards Ultimate Training Efficiency**\\n\\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \\n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \\n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \\n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\\n\\n---\\n\\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\\n\\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\\n\\n---\\n\\n\\n## 3. Model Downloads\\n\\n<div align=\"center\">\\n\\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\\n| :------------: | :------------: | :------------: | :------------: | :------------: |\\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\\n\\n</div>\\n\\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\\n\\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\\n\\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\\n\\n## 4. Evaluation Results\\n### Base Model\\n#### Standard Benchmarks\\n\\n<div align=\"center\">\\n\\n\\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\\n|---|-------------------|----------|--------|-------------|---------------|---------|\\n| | Architecture | - | MoE | Dense | Dense | MoE |\\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\\n| | # Total Params | - | 236B | 72B | 405B | 671B |\\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\\n\\n</div>\\n\\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\\nFor more evaluation details, please check our paper. \\n\\n#### Context Window\\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/niah.png\">\\n</p>\\n\\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \\n\\n### Chat Model\\n#### Standard Benchmarks (Models larger than 67B)\\n<div align=\"center\">\\n\\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\\n\\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\\n\\n</div>\\n\\n\\n####  Open Ended Generation Evaluation\\n\\n<div align=\"center\">\\n\\n\\n\\n| Model | Arena-Hard | AlpacaEval 2.0 |\\n|-------|------------|----------------|\\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\\n| LLaMA-3.1 405B | 69.3 | 40.5 |\\n| GPT-4o-0513 | 80.4 | 51.1 |\\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\\n| DeepSeek-V3 | **85.5** | **70.0** |\\n\\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\\n</div>\\n\\n\\n## 5. Chat Website & API Platform\\nYou can chat with DeepSeek-V3 on DeepSeek\\'s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\\n\\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\\n\\n## 6. How to Run Locally\\n\\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\\n\\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\\n\\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\\n\\nHere is an example of converting FP8 weights to BF16:\\n\\n```shell\\ncd inference\\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\\n```\\n\\n**NOTE: Huggingface\\'s Transformers has not been directly supported yet.**\\n\\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\\n\\n#### Model Weights & Demo Code Preparation\\n\\nFirst, clone our DeepSeek-V3 GitHub repository:\\n\\n```shell\\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\\n```\\n\\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\\n\\n```shell\\ncd DeepSeek-V3/inference\\npip install -r requirements.txt\\n```\\n\\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\\n\\n#### Model Weights Conversion\\n\\nConvert HuggingFace model weights to a specific format:\\n\\n```shell\\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\\n```\\n\\n#### Run\\n\\nThen you can chat with DeepSeek-V3:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\\n```\\n\\nOr batch inference on a given file:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\\n```\\n\\n### 6.2 Inference with SGLang (recommended)\\n\\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\\n\\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\\n\\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\\n\\n### 6.3 Inference with LMDeploy (recommended)\\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\\n\\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\\n\\n\\n### 6.4 Inference with TRT-LLM (recommended)\\n\\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \\n\\n### 6.5 Inference with vLLM (recommended)\\n\\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\\n\\n### 6.6 Recommended Inference Functionality with AMD GPUs\\n\\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\\n\\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\\n\\n\\n## 7. License\\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\\n\\n## 8. Citation\\n```\\n@misc{deepseekai2024deepseekv3technicalreport,\\n      title={DeepSeek-V3 Technical Report}, \\n      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},\\n      year={2024},\\n      eprint={2412.19437},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.19437}, \\n}\\n```\\n\\n## 9. Contact\\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\\n'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=deepseek-ai/DeepSeek-V3\n",
      "> RSP [{'title': 'DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat', 'url': 'https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/', 'content': 'DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch | VentureBeat DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B.', 'score': 0.8727061, 'raw_content': None}, {'title': \"DeepSeek's new AI model appears to be one of the best 'open ...\", 'url': 'https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/', 'content': \"DeepSeek's new AI model appears to be one of the best 'open' challengers yet | TechCrunch DeepSeek's new AI model appears to be one of the best 'open' challengers yet | TechCrunch DeepSeek’s new AI model appears to be one of the best ‘open’ challengers yet The model, DeepSeek V3, was developed by the AI firm DeepSeek and was released on Wednesday under a permissive license that allows developers to download and modify it for most applications, including commercial ones. According to DeepSeek’s internal benchmark testing, DeepSeek V3 outperforms both downloadable, “openly” available models and “closed” AI models that can only be accessed through an API.\", 'score': 0.86108357, 'raw_content': None}, {'title': 'DeepSeek V3: New Open AI Model Surpasses Rivals and ... - WinBuzzer', 'url': 'https://winbuzzer.com/2024/12/27/deepseek-v3-new-open-ai-model-surpasses-rivals-and-challenges-gpt-4o-xcxwbn/', 'content': 'DeepSeek V3: New Open AI Model Surpasses Rivals and Challenges GPT-4o - WinBuzzer DeepSeek V3: New Open AI Model Surpasses Rivals and Challenges GPT-4o DeepSeek V3 is an open-source AI model that outperformes Meta’s Llama 3.1 and comes close to OpenAI’s GPT-4o in key benchmarks. DeepSeek V3’s technical advancements place it among the most powerful AI systems to, rivaling both open-source competitors like Meta’s Llama 3.1 and proprietary models like OpenAI’s GPT-4o. DeepSeek V3’s benchmark results showcase its exceptional capabilities across a broad spectrum of tasks, solidifying its position as a leader among open-source AI models. Related: DeepSeek AI Open Sources VL2 Series of Vision Language Models', 'score': 0.8593928, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-', 'score': 0.8479807, 'raw_content': None}, {'title': 'Introducing DeepSeek-V3 | DeepSeek API Docs', 'url': 'https://api-docs.deepseek.com/news/news1226', 'content': '🚀 Introducing DeepSeek-V3 | DeepSeek API Docs DeepSeek API Docs DeepSeek Platform Your First API Call Introducing DeepSeek-V3 2024/12/26 DeepSeek-V2.5-1210 Release 2024/12/10 DeepSeek-V2.5 Release 2024/09/05 Context Caching is Available 2024/08/02 New API Features 2024/07/25 API Reference API Guides Context Caching API Status Page Introducing DeepSeek-V3 2024/12/26 🚀 Introducing DeepSeek-V3 ⚡ 60 tokens/second (3x faster than V2!) Model 👉 https://github.com/deepseek-ai/DeepSeek-V3 Paper 👉 https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf 💰 API Pricing Update\\u200b 🎉 Until Feb 8: same as V2! | Input (cache miss) | Input (cache hit) | Output | Look forward to multimodal support and other cutting-edge features in the DeepSeek ecosystem. Previous Error CodesNext 🚀 DeepSeek V2.5: The Grand Finale 🎉 🎉 What’s new in V3 💰 API Pricing Update Copyright © 2024 DeepSeek, Inc.', 'score': 0.780947, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=deepseek-ai/DeepSeek-V3\n",
      "> RSP [{'title': '[2412.19437] DeepSeek-V3 Technical Report - export.arxiv.org', 'url': 'http://export.arxiv.org/abs/2412.19437', 'content': 'cs > arXiv:2412.19437 cs.CL cs cs.AI We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite\\xa0as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) Which authors of this paper are endorsers?', 'score': 0.72907686, 'raw_content': None}, {'title': '[2412.19437] DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/abs/2412.19437', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. arXiv:2412.19437 arXiv author ID DeepSeek-V3 Technical Report We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.19437 Bibliographic and Citation Tools Connected Papers Toggle\", 'score': 0.65252984, 'raw_content': None}, {'title': 'arXiv:2401.02954v1 [cs.CL] 5 Jan 2024', 'url': 'https://arxiv.org/pdf/2401.02954', 'content': 'n\\n0007315414 00000 n\\n0007315565 00000 n\\n0005088444 00000 n\\n0007315714 00000 n\\n0007315777 00000 n\\n0007315840 00000 n\\n0007316008 00000 n\\n0007316159 00000 n\\n0005091379 00000 n\\n0007316300 00000 n\\n0005095039 00000 n\\n0005095316 00000 n\\n0005095595 00000 n\\n0005095874 00000 n\\n0005096153 00000 n\\n0005096431 00000 n\\n0005096709 00000 n\\n0005096987 00000 n\\n0005097266 00000 n\\n0005097544 00000 n\\n0005097822 00000 n\\n0005098100 00000 n\\n0005098378 00000 n\\n0005098656 00000 n\\n0005098934 00000 n\\n0005099213 00000 n\\n0005099492 00000 n\\n0005099769 00000 n\\n0005100048 00000 n\\n0005100327 00000 n\\n0005100605 00000 n\\n0005100882 00000 n\\n0005101160 00000 n\\n0005101438 00000 n\\n0005101716 00000 n\\n0005101994 00000 n\\n0005102270 00000 n\\n0005102548 00000 n\\n0005102826 00000 n\\n0005103103 00000 n\\n0005103381 00000 n\\n0005103660 00000 n\\n0007316363 00000 n\\n0007316531 00000 n\\n0007316699 00000 n\\n0007317326 00000 n\\n0005103938 00000 n\\n0007317475 00000 n\\n0007317538 00000 n\\n0007317601 00000 n\\n0007317766 00000 n\\n0007317932 00000 n\\n0007318097 00000 n\\n0007318262 00000 n\\n0007318429 00000 n\\n0007318594 00000 n\\n0007318759 00000 n\\n0007318924 00000 n\\n0007319089 00000 n\\n0007319254 00000 n\\n0007319420 00000 n\\n0007319586 00000 n\\n0007319751 00000 n\\n0007319918 00000 n\\n0007320083 00000 n\\n0007320250 00000 n\\n0007320417 00000 n\\n0007320582 00000 n\\n0007320748 00000 n\\n0007320914 00000 n\\n0007321080 00000 n\\n0007321246 00000 n\\n0007321412 00000 n\\n0007321577 00000 n\\n0007321742 00000 n\\n0007321938 00000 n\\n0007322102 00000 n\\n0007322268 00000 n\\n0007322433 00000 n\\n0007322598 00000 n\\n0007322763 00000 n\\n0007322930 00000 n\\n0007323097 00000 n\\n0005149229 00000 n\\n0007323160 00000 n\\n0007323330 00000 n\\n0007323496 00000 n\\n0007323662 00000 n\\n0007323830 00000 n\\n0007323995 00000 n\\n0007324159 00000 n\\n0007324323 00000 n\\n0007324485 00000 n\\n0007324648 00000 n\\n0007324811 00000 n\\n0007324975 00000 n\\n0007325139 00000 n\\n0007325304 00000 n\\n0005111289 00000 n\\n0007325533 00000 n\\n0007325596 00000 n\\n0007325659 00000 n\\n0007325722 00000 n\\n0007325785 00000 n\\n0007325848 00000 n\\n0007326013 00000 n\\n0007326178 00000 n\\n0007326340 00000 n\\n0007326502 00000 n\\n0007326663 00000 n\\n0007326824 00000 n\\n0007326991 00000 n\\n0005114771 00000 n\\n0005115049 00000 n\\n0005115327 00000 n\\n0005115605 00000 n\\n0005115884 00000 n\\n0005116163 00000 n\\n0005116440 00000 n\\n0005116718 00000 n\\n0005116997 00000 n\\n0005117275 00000 n\\n0005117554 00000 n\\n0005117831 00000 n\\n0005118109 00000 n\\n0005118388 00000 n\\n0005118667 00000 n\\n0005118945 00000 n\\n0005119223 00000 n\\n0005119501 00000 n\\n0005119779 00000 n\\n0005120058 00000 n\\n0005120336 00000 n\\n0005120614 00000 n\\n0005120891 00000 n\\n0005121169 00000 n\\n0007327159 00000 n\\n0007327328 00000 n\\n0007327479 00000 n\\n0005121447 00000 n\\n0007327676 00000 n\\n0007327739 00000 n\\n0007327802 00000 n\\n0007327922 00000 n\\n0007327985 00000 n\\n0007328155 00000 n\\n0007328325 00000 n\\n0007328495 00000 n\\n0007328665 00000 n\\n0007328834 00000 n\\n0007329003 00000 n\\n0007329812 00000 n\\n0005124485 00000 n\\n0007329985 00000 n\\n0007330048 00000 n\\n0007330111 00000 n\\n0007330277 00000 n\\n0007330443 00000 n\\n0007330609 00000 n\\n0007330775 00000 n\\n0007330942 00000 n\\n0007331108 00000 n\\n0007331274 00000 n\\n0007331438 00000 n\\n0007331604 00000 n\\n0007331771 00000 n\\n0007331937 00000 n\\n0007332103 00000 n\\n0007332268 00000 n\\n0007332434 00000 n\\n0007332600 00000 n\\n0007332766 00000 n\\n0007332932 00000 n\\n0007333098 00000 n\\n0007333263 00000 n\\n0007333430 00000 n\\n0007333596 00000 n\\n0007333763 00000 n\\n0007333929 00000 n\\n0007334094 00000 n\\n0007334157 00000 n\\n0007334220 00000 n\\n0007334390 00000 n\\n0007334560 00000 n\\n0007334711 00000 n\\n0005129191 00000 n\\n0007334876 00000 n\\n0007334939 00000 n\\n0007335002 00000 n\\n0007335064 00000 n\\n0007335232 00000 n\\n0007335397 00000 n\\n0005132062 00000 n\\n0007335538 00000 n\\n0007335601 00000 n\\n0007335663 00000 n\\n0007335882 00000 n\\n0007336091 00000 n\\n0007336242 00000 n\\n0005135187 00000 n\\n0007336391 00000 n\\n0007336454 00000 n\\n0007336646 00000 n\\n0007336864 00000 n\\n0007337065 00000 n\\n0007337264 00000 n\\n0007337482 00000 n\\n0007337684 00000 n\\n0007337821 00000 n\\n0005138801 00000 n\\n0007337994 00000 n\\n0007338057 00000 n\\n0007338290 00000 n\\n0005144461 00000 n\\n0005144739 00000 n\\n0005145018 00000 n\\n0007338500 00000 n\\n0007338710 00000 n\\n0007338920 00000 n\\n0007339130 00000 n\\n0007339340 00000 n\\n0007339659 00000 n\\n0005145296 00000 n\\n0007339872 00000 n\\n0007339935 00000 n\\n0007340167 00000 n\\n0007340230 00000 n\\n0007340436 00000 n\\n0007340602 00000 n\\n0007340767 00000 n\\n0007340934 00000 n\\n0007341143 00000 n\\n0007341206 00000 n\\n0007341415 00000 n\\n0006175493 00000 n\\n0007341535 00000 n\\n0007341737 00000 n\\n0007341938 00000 n\\n0007342143 00000 n\\n0007342348 00000 n\\n0007342556 00000 n\\n0007342693 00000 n\\n0005152288 00000 n\\n0007342874 00000 n\\n0007342937 00000 n\\n0007343141 00000 n\\n0007343341 00000 n\\n0007343478 00000 n\\n0005157208 00000 n\\n0007343619 00000 n\\n0007343682 00000 n\\n0007343745 00000 n\\n0007343955 00000 n\\n0007344163 00000 n\\n0007344438 00000 n\\n0007344654 00000 n\\n0007344886 00000 n\\n0007345023 00000 n\\n0005161745 00000 n\\n0007345204 00000 n\\n0007345267 00000 n\\n0007345475 00000 n\\n0007345750 00000 n\\n0007345959 00000 n\\n0007346164 00000 n\\n0007346369 00000 n\\n0007346578 00000 n\\n0007346715 00000 n\\n0005166165 00000 n\\n0007346904 00000 n\\n0007346967 00000 n\\n0007347030 00000 n\\n0007347238 00000 n\\n0007347442 00000 n\\n0007347607 00000 n\\n0005173001 00000 n\\n0005510940 00000 n\\n0005853150 00000 n\\n0007347778 00000 n\\n0007348022 00000 n\\n0005170325 00000 n\\n0007348186 00000 n\\n0007348249 00000 n\\n0007348312 00000 n\\n0007348375 00000 n\\n0007348438 00000 n\\n0005501997 00000 n\\n0005844207 00000 n\\n0006166550 00000 n\\n0007348501 00000 n\\n0006563966 00000 n\\n0006179210 00000 n\\n0007348535 00000 n\\n0007348705 00000 n\\n0007348874 00000 n\\n0007349076 00000 n\\n0006177718 00000 n\\n0007349232 00000 n\\n0007349295 00000 n\\n0007349357 00000 n\\n0007349420 00000 n\\n0006526370 00000 n\\n0007349540 00000 n\\n0007349710 00000 n\\n0007349874 00000 n\\n0007350038 00000 n\\n0007350219 00000 n\\n0007350400 00000 n\\n0007350563 00000 n\\n0007350726 00000 n\\n0007350897 00000 n\\n0007351068 00000 n\\n0007351231 00000 n\\n0007351391 00000 n\\n0007351558 00000 n\\n0007351725 00000 n\\n0007351894 00000 n\\n0007352063 00000 n\\n0007352225 00000 n\\n0007352387 00000 n\\n0007352554 00000 n\\n0007352721 00000 n\\n0006531289 00000 n\\n0006531568 00000 n\\n0006531847 00000 n\\n0006532126 00000 n\\n0006532403 00000 n\\n0006532681 00000 n\\n0006532959 00000 n\\n0006533238 00000 n\\n0006533517 00000 n\\n0006533796 00000 n\\n0006534075 00000 n\\n0006534353 00000 n\\n0006534631 00000 n\\n0006534909 00000 n\\n0006535188 00000 n\\n0006535466 00000 n\\n0006535745 00000 n\\n0006536024 00000 n\\n0007352889 00000 n\\n0007353040 00000 n\\n0006536303 00000 n\\n0007353341 00000 n\\n0007353404 00000 n\\n0007353467 00000 n\\n0007353530 00000 n\\n0007353592 00000 n\\n0007353656 00000 n\\n0007354325 00000 n\\n0006538595 00000 n\\n0007354446 00000 n\\n0007354509 00000 n\\n0007354675 00000 n\\n0007354841 00000 n\\n0007355007 00000 n\\n0007355171 00000 n\\n0007355336 00000 n\\n0007355503 00000 n\\n0007355669 00000 n\\n0007355836 00000 n\\n0007356003 00000 n\\n0007356066 00000 n\\n0007356217 00000 n\\n0006540424 00000 n\\n0007356338 00000 n\\n0007356401 00000 n\\n0007356464 00000 n\\n0007357427 00000 n\\n0006541782 00000 n\\n0007357548 00000 n\\n0007357611 00000 n\\n0007357672 00000 n\\n0007357838 00000 n\\n0007358004 00000 n\\n0007358883 00000 n\\n0006544062 00000 n\\n0007359004 00000 n\\n0007359067 00000 n\\n0007359130 00000 n\\n0007359297 00000 n\\n0007359359 00000 n\\n0007359525 00000 n\\n0007359692 00000 n\\n0007360557 00000 n\\n0006545580 00000 n\\n0007360678 00000 n\\n0007360741 00000 n\\n0007360804 00000 n\\n0007360969 00000 n\\n0007361136 00000 n\\n0007361256 00000 n\\n0007362037 00000 n\\n0006549276 00000 n\\n0007362158 00000 n\\n0007362221 00000 n\\n0007362284 00000 n\\n0007362449 00000 n\\n0007362616 00000 n\\n0007363425 00000 n\\n0006550661 00000 n\\n0007363546 00000 n\\n0007363609 00000 n\\n0007363672 00000 n\\n0007363735 00000 n\\n0007364740 00000 n\\n0006552618 00000 n\\n0007364861 00000 n\\n0007364924 00000 n\\n0007364986 00000 n\\n0007365137 00000 n\\n0006554967 00000 n\\n0007365258 00000 n\\n0007365321 00000 n\\n0007365384 00000 n\\n0007365535 00000 n\\n0006557886 00000 n\\n0007365656 00000 n\\n0007365719 00000 n\\n0007365782 00000 n\\n0007365845 00000 n\\n0007365996 00000 n\\n0006558584 00000 n\\n0007366117 00000 n\\n0007366180 00000 n\\n0007366243 00000 n\\n0007366364 00000 n\\n0007366515 00000 n\\n0006560251 00000 n\\n0007366636 00000 n\\n0007366699 00000 n\\n0007366761 00000 n\\n0007366912 00000 n\\n0006561609 00000 n\\n0007367033 00000 n\\n0007367096 00000 n\\n0007367159 00000 n\\n0007367310 00000 n\\n0006562717 00000 n\\n0007367431 00000 n\\n0007367494 00000 n\\n0007367557 00000 n\\n0007367620 00000 n\\n0006571237 00000 n\\n0007367683 00000 n\\n0007367834 00000 n\\n0006566959 00000 n\\n0007367955 00000 n\\n0007368019 00000 n\\n0007368083 00000 n\\n0007368235 00000 n\\n0006569526 00000 n\\n0007368359 00000 n\\n0007368424 00000 n\\n0007368489 00000 n\\n0007368516 00000 n\\n0007369045 00000 n\\n0007369172 00000 n\\n0007369199 00000 n\\n0007369226 00000 n\\n0007369578 00000 n\\n0007369709 00000 n\\n0007370384 00000 n\\n0007370949 00000 n\\n0007370976 00000 n\\n0007371003 00000 n\\n0007371548 00000 n\\n0007371979 00000 n\\n0007372596 00000 n\\n0007372664 00000 n\\n0007373193 00000 n\\n0007373405 00000 n\\n0007373986 00000 n\\n0007374328 00000 n\\n0007375017 00000 n\\n0007375453 00000 n\\n0007375768 00000 n\\n0007376102 00000 n\\n0007376129 00000 n\\n0007376824 00000 n\\n0007376851 00000 n\\n0007377594 00000 n\\n0007377991 00000 n\\n0007378857 00000 n\\n0007379237 00000 n\\n0007379903 00000 n\\n0007380512 00000 n\\n0007380914 00000 n\\n0007381531 00000 n\\n0007381957 00000 n\\n0007382716 00000 n\\n0007383019 00000 n\\n0007383425 00000 n\\n0007383856 00000 n\\n0007384140 00000 n\\n0007384340 00000 n\\n0007384985 00000 n\\n0007385429 00000 n\\n0007385928 00000 n\\n0007386644 00000 n\\n0007387193 00000 n\\n0007387775 00000 n\\n0007388221 00000 n\\n0007388612 00000 n\\n0007389441 00000 n\\n0007390074 00000 n\\n0007390487 00000 n\\n0007391014 00000 n\\n0007391156 00000 n\\n0007391879 00000 n\\n0007392233 00000 n\\n0007393130 00000 n\\n0007393355 00000 n\\n0007394241 00000 n\\n0007395035 00000 n\\n0007395760 00000 n\\n0007396434 00000 n\\n0007397126 00000 n\\n0007398036 00000 n\\n0007398494 00000 n\\n0007398793 00000 n\\n0007399611 00000 n\\n0007400210 00000 n\\n0007401046 00000 n\\n0007401647 00000 n\\n0007402270 00000 n\\n0007403090 00000 n\\n0007403779 00000 n\\n0007404582 00000 n\\n0007404609 00000 n\\n0007404925 00000 n\\n0007405399 00000 n\\n0007406036 00000 n\\n0007406745 00000 n\\n0007407353 00000 n\\n0007408323 00000 n\\n0007408642 00000 n\\n0007408668 00000 n\\n0007408966 00000 n\\n0007182892 00000 n\\n0007409746 00000 n\\n0007410688 00000 n\\n0006574950 00000 n\\n0007411168 00000 n\\n0006581515 00000 n\\n0007411510 00000 n\\n0006583679 00000 n\\n0007411757 00000 n\\n0006599029 00000 n\\n0007412314 00000 n\\n0006608793 00000 n\\n0007412661 00000 n\\n0006615140 00000 n\\n0007412932 00000 n\\n0006625783 00000 n\\n0007413333 00000 n\\n0006636954 00000 n\\n0007413706 00000 n\\n0006650045 00000 n\\n0007414169 00000 n\\n0006659549 00000 n\\n0007414530 00000 n\\n0006663331 00000 n\\n0007414767 00000 n\\n0006668489 00000 n\\n0007415036 00000 n\\n0006678438 00000 n\\n0007415379 00000 n\\n0006681647 00000 n\\n0007415606 00000 n\\n0006688918 00000 n\\n0007415935 00000 n\\n0006692082 00000 n\\n0007416162 00000 n\\n0006701373 00000 n\\n0007416509 00000 n\\n0006706867 00000 n\\n0007416796 00000 n\\n0006711402 00000 n\\n0007417073 00000 n\\n0006719918 00000 n\\n0007417398 00000 n\\n0006730409 00000 n\\n0007417755 00000 n\\n0006739484 00000 n\\n0007418082 00000 n\\n0006742031 00000 n\\n0007418305 00000 n\\n0006751174 00000 n\\n0007418642 00000 n\\n0006763129 00000 n\\n0007418981 00000 n\\n0006765762 00000 n\\n0007419204 00000 n\\n0006775354 00000 n\\n0007419547 00000 n\\n0006781094 00000 n\\n0007419824 00000 n\\n0006788534 00000 n\\n0007420143 00000 n\\n0006794642 00000 n\\n0007420414 00000 n\\n0006797269 00000 n\\n0007420635 00000 n\\n0006804430 00000 n\\n0007420936 00000 n\\n0006809669 00000 n\\n0007421217 00000 n\\n0006815038 00000 n\\n0007421480 00000 n\\n0006818724 00000 n\\n0007421715 00000 n\\n0006822371 00000 n\\n0007421942 00000 n\\n0006826448 00000 n\\n0007422193 00000 n\\n0006828908 00000 n\\n0007422414 00000 n\\n0006834494 00000 n\\n0007422693 00000 n\\n0006839535 00000 n\\n0007422954 00000 n\\n0006843919 00000 n\\n0007423199 00000 n\\n0006850711 00000 n\\n0007423502 00000 n\\n0006856978 00000 n\\n0007423793 00000 n\\n0006862452 00000 n\\n0007424048 00000 n\\n0006865493 00000 n\\n0007424275 00000 n\\n0006871906 00000 n\\n0007424562 00000 n\\n0006878567 00000 n\\n0007424839 00000 n\\n0006886398 00000 n\\n0007425134 00000 n\\n0006891532 00000 n\\n0007425393 00000 n\\n0006904155 00000 n\\n0007425748 00000 n\\n0006910039 00000 n\\n0007426013 00000 n\\n0006919394 00000 n\\n0007426360 00000 n\\n0006925164 00000 n\\n0007426631 00000 n\\n0006931399 00000 n\\n0007426916 00000 n\\n0006935136 00000 n\\n0007427153 00000 n\\n0006939056 00000 n\\n0007427388 00000 n\\n0006941901 00000 n\\n0007427613 00000 n\\n0006944271 00000 n\\n0007427834 00000 n\\n0006946924 00000 n\\n0007428057 00000 n\\n0006952453 00000 n\\n0007428318 00000 n\\n0006958951 00000 n\\n0007428601 00000 n\\n0006961863 00000 n\\n0007428832 00000 n\\n0006973749 00000 n\\n0007429211 00000 n\\n0006978457 00000 n\\n0007429458 00000 n\\n0006987794 00000 n\\n0007429777 00000 n\\n0006990670 00000 n\\n0007430008 00000 n\\n0007000068 00000 n\\n0007430343 00000 n\\n0007009521 00000 n\\n0007430654 00000 n\\n0007013227 00000 n\\n0007430899 00000 n\\n0007016335 00000 n\\n0007431126 00000 n\\n0007021225 00000 n\\n0007431381 00000 n\\n0007028221 00000 n\\n0007431662 00000 n\\n0007032889 00000 n\\n0007431905 00000 n\\n0007037012 00000 n\\n0007432158 00000 n\\n0007039971 00000 n\\n0007432389 00000 n\\n0007045905 00000 n\\n0007432650 00000 n\\n0007048304 00000 n\\n0007432873 00000 n\\n0007051312 00000 n\\n0007433100 00000 n\\n0007053807 00000 n\\n0007433371 00000 n\\n0007081427 00000 n\\n0007433774 00000 n\\n0007091000 00000 n\\n0007434003 00000 n\\n0007096507 00000 n\\n0007434318 00000 n\\n0007119773 00000 n\\n0007434772 00000 n\\n0007132638 00000 n\\n0007435091 00000 n\\n0007160339 00000 n\\n0007435841 00000 n\\n0007176421 00000 n\\n0007177166 00000 n\\n0007177643 00000 n\\n0007178415 00000 n\\n0007179198 00000 n\\n0007179858 00000 n\\n0007180594 00000 n\\n0007181319 00000 n\\n0007182097 00000 n\\n0007189258 00000 n\\n0007240991 00000 n\\n0007190039 00000 n\\n0007190725 00000 n\\n0007191445 00000 n\\n0007192097 00000 n\\n0007192864 00000 n\\n0007193536 00000 n\\n0007194229 00000 n\\n0007194972 00000 n\\n0007195599 00000 n\\n0007196309 00000 n\\n0007197107 00000 n\\n0007197880 00000 n\\n0007198513 00000 n\\n0007199283 00000 n\\n0007200058 00000 n\\n0007200766 00000 n\\n0007201508 00000 n\\n0007202237 00000 n\\n0007203044 00000 n\\n0007203761 00000 n\\n0007204333 00000 n\\n0007204996 00000 n\\n0007205796 00000 n\\n0007206597 00000 n\\n0007207337 00000 n\\n0007207996 00000 n\\n0007208650 00000 n\\n0007209284 00000 n\\n0007209992 00000 n\\n0007210694 00000 n\\n0007211359 00000 n\\n0007212136 00000 n\\n0007212895 00000 n\\n0007213627 00000 n\\n0007214355 00000 n\\n0007215048 00000 n\\n0007215724 00000 n\\n0007216501 00000 n\\n0007217140 00000 n\\n0007217656 00000 n\\n0007218361 00000 n\\n0007219113 00000 n\\n0007219823 00000 n\\n0007220598 00000 n\\n0007221345 00000 n\\n0007222057 00000 n\\n0007222692 00000 n\\n0007223356 00000 n\\n0007224088 00000 n\\n0007224809 00000 n\\n0007225400 00000 n\\n0007225849 00000 n\\n0007226362 00000 n\\n0007226890 00000 n\\n0007227545 00000 n\\n0007228194 00000 n\\n0007228804 00000 n\\n0007229586 00000 n\\n0007230224 00000 n\\n0007230738 00000 n\\n0007231294 00000 n\\n0007232020 00000 n\\n0007232671 00000 n\\n0007233180 00000 n\\n0007233702 00000 n\\n0007234272 00000 n\\n0007234970 00000 n\\n0007235498 00000 n\\n0007235929 00000 n\\n0007236701 00000 n\\n0007237474 00000 n\\n0007238246 00000 n\\n0007239019 00000 n\\n0007239937 00000 n\\n0007436223 00000 n\\n0007436345 00000 n\\n0007436435 00000 n\\n0007436510 00000 n\\n0007244520 00000 n\\n0007436585 00000 n\\n0007436787 00000 n\\n0007437007 00000 n\\n0007437238 00000 n\\n0007437478 00000 n\\n0007437736 00000 n\\n0007437971 00000 n\\n0007438240 00000 n\\n0007438473 00000 n\\n0007438693 00000 n\\n0007438922 00000 n\\n0007439190 00000 n\\n0007439447 00000 n\\n0007439686 00000 n\\n0007439923 00000 n\\n0007440172 00000 n\\n0007440345 00000 n\\n0007440518 00000 n\\n0007440693 00000 n\\n0007440867 00000 n\\n0007441042 00000 n\\n0007441216 00000 n\\n0007441392 00000 n\\n0007441562 00000 n\\n0007441746 00000 n\\n0007441969 00000 n\\n0007442192 00000 n\\n0007442416 00000 n\\n0007442641 00000 n\\n0007442870 00000 n\\n0007443119 00000 n\\n0007443366 00000 n\\n0007443612 00000 n\\n0007443859 00000 n\\n0007444106 00000 n\\n0007444353 00000 n\\n0007444602 00000 n\\n0007444765 00000 n\\n0007444907 00000 n\\n0007445053 00000 n\\n0007445176 00000 n\\n0007445298 00000 n\\n0007445432 00000 n\\n0007445570 00000 n\\n0007445662 00000 n\\n0007445793 00000 n\\n0007445885 00000 n\\n0007445979 00000 n\\n0007446019 00000 n\\n0007244205 00000 n\\n0007247361 00000 n\\n0007446152 00000 n\\n0007446329 00000 n\\ntrailer\\n<< /ID /arXivStAmP >>\\nendobj\\nxref\\n0 1408\\n0000000000 65535 f\\n0007251237 00000 n\\n0007252151 00000 n\\n0007252173 00000 n\\n0007252195 00000 n\\n0007252249 00000 n\\n0000008852 00000 n\\n0007252294 00000 n\\n0007252354 00000 n\\n0007252427 00000 n\\n0007252471 00000 n\\n0007252517 00000 n\\n0007252579 00000 n\\n0007252704 00000 n\\n0007252749 00000 n\\n0007252800 00000 n\\n0007252862 00000 n\\n0007252936 00000 n\\n0007252965 00000 n\\n0007253016 00000 n\\n0007253078 00000 n\\n0007253165 00000 n\\n0007253210 00000 n\\n0007253261 00000 n\\n0007253323 00000 n\\n0007253410 00000 n\\n0007253461 00000 n\\n0007253512 00000 n\\n0007253574 00000 n\\n0007253648 00000 n\\n0007253699 00000 n\\n0007253745 00000 n\\n0007253807 00000 n\\n0007253933 00000 n\\n0007253978 00000 n\\n0007254029 00000 n\\n0007254090 00000 n\\n0007254164 00000 n\\n0007254249 00000 n\\n0007254300 00000 n\\n0007254362 00000 n\\n0007254449 00000 n\\n0007254552 00000 n\\n0007254603 00000 n\\n0007254665 00000 n\\n0007254739 00000 n\\n0007254824 00000 n\\n0007254870 00000 n\\n0007254932 00000 n\\n0007255021 00000 n\\n0007255060 00000 n\\n0007255106 00000 n\\n0007255168 00000 n\\n0007255294 00000 n\\n0007255335 00000 n\\n0007255386 00000 n\\n0007255448 00000 n\\n0007255559 00000 n\\n0007255634 00000 n\\n0007255690 00000 n\\n0007255752 00000 n\\n0007255826 00000 n\\n0007255867 00000 n\\n0007255923 00000 n\\n0007255985 00000 n\\n0007256059 00000 n\\n0007256100 00000 n\\n0007256151 00000 n\\n0007256213 00000 n\\n0007256337 00000 n\\n0007256400 00000 n\\n0007256456 00000 n\\n0007256518 00000 n\\n0007256592 00000 n\\n0007256671 00000 n\\n0007256727 00000 n\\n0007256789 00000 n\\n0007256863 00000 n\\n0007256942 00000 n\\n0007256993 00000 n\\n0007257055 00000 n\\n0007257142 00000 n\\n0007257201 00000 n\\n0007257252 00000 n\\n0007257314 00000 n\\n0007257401 00000 n\\n0007257456 00000 n\\n0007257507 00000 n\\n0007257569 00000 n\\n0007257643 00000 n\\n0007257684 00000 n\\n0007257730 00000 n\\n0007257792 00000 n\\n0007257881 00000 n\\n0007257980 00000 n\\n0007258027 00000 n\\n0007258088 00000 n\\n0007258203 00000 n\\n0007258240 00000 n\\n0007258291 00000 n\\n0007258353 00000 n\\n0007258430 00000 n\\n0007258482 00000 n\\n0007258534 00000 n\\n0007258596 00000 n\\n0007258688 00000 n\\n0007258784 00000 n\\n0007258836 00000 n\\n0007258899 00000 n\\n0007258991 00000 n\\n0007259061 00000 n\\n0007259113 00000 n\\n0007259176 00000 n\\n0007259268 00000 n\\n0007259378 00000 n\\n0007259430 00000 n\\n0007259493 00000 n\\n0007259585 00000 n\\n0007259667 00000 n\\n0007259719 00000 n\\n0007259782 00000 n\\n0007259860 00000 n\\n0007259918 00000 n\\n0007259970 00000 n\\n0000000012 00000 n\\n0000000671 00000 n\\n0000003778 00000 n\\n0007260119 00000 n\\n0000001514 00000 n\\n0007260305 00000 n\\n0007260368 00000 n\\n0007260444 00000 n\\n0007260506 00000 n\\n0007260568 00000 n\\n0007260763 00000 n\\n0007260958 00000 n\\n0007261078 00000 n\\n0000005542 00000 n\\n0007261354 00000 n\\n0007261376 00000 n\\n0007261438 00000 n\\n0007261598 00000 n\\n0007261759 00000 n\\n0007261926 00000 n\\n0007262093 00000 n\\n0007262260 00000 n\\n0007262427 00000 n\\n0007262588 00000 n\\n0007262753 00000 n\\n0007262920 00000 n\\n0007263087 00000 n\\n0007263248 00000 n\\n0007263408 00000 n\\n0007263574 00000 n\\n0007263747 00000 n\\n0007263920 00000 n\\n0007264087 00000 n\\n0007264260 00000 n\\n0007264433 00000 n\\n0007264600 00000 n\\n0007264767 00000 n\\n0007264934 00000 n\\n0007265094 00000 n\\n0007265256 00000 n\\n0007265422 00000 n\\n0007265587 00000 n\\n0007265754 00000 n\\n0007265921 00000 n\\n0007266087 00000 n\\n0007266253 00000 n\\n0007266390 00000 n\\n0000007802 00000 n\\n0007266755 00000 n\\n0007266818 00000 n\\n0007266986 00000 n\\n0007267156 00000 n\\n0007267322 00000 n\\n0007267488 00000 n\\n0007267653 00000 n\\n0007267818 00000 n\\n0007267980 00000 n\\n0007268143 00000 n\\n0007268305 00000 n\\n0007268464 00000 n\\n0007268625 00000 n\\n0007268786 00000 n\\n0007268951 00000 n\\n0007269116 00000 n\\n0007269279 00000 n\\n0007269442 00000 n\\n0007269606 00000 n\\n0007269773 00000 n\\n0007269940 00000 n\\n0007270102 00000 n\\n0007270264 00000 n\\n0007270427 00000 n\\n0007270595 00000 n\\n0007270763 00000 n\\n0007270931 00000 n\\n0007271099 00000 n\\n0007271268 00000 n\\n0007271436 00000 n\\n0007271605 00000 n\\n0007271774 00000 n\\n0007271951 00000 n\\n0007272125 00000 n\\n0007272287 00000 n\\n0007272448 00000 n\\n0007272585 00000 n\\n0000011279 00000 n\\n0007273006 00000 n\\n0007273069 00000 n\\n0007273232 00000 n\\n0001665152 00000 n\\n0007273398 00000 n\\n0007273461 00000 n\\n0007273524 00000 n\\n0007273587 00000 n\\n0007273650 00000 n\\n0007273713 00000 n\\n0007273776 00000 n\\n0007273839 00000 n\\n0007273902 00000 n\\n0007273965 00000 n\\n0007274027 00000 n\\n0007274090 00000 n\\n0007274152 00000 n\\n0007274215 00000 n\\n0007274278 00000 n\\n0007274440 00000 n\\n0007274603 00000 n\\n0007274766 00000 n\\n0007274929 00000 n\\n0007275092 00000 n\\n0007275260 00000 n\\n0007275428 00000 n\\n0007275591 00000 n\\n0007275751 00000 n\\n0007275919 00000 n\\n0007276088 00000 n\\n0007276252 00000 n\\n0007276416 00000 n\\n0007276585 00000 n\\n0007276754 00000 n\\n0007276923 00000 n\\n0007277085 00000 n\\n0007277247 00000 n\\n0007277410 00000 n\\n0007277571 00000 n\\n0007277735 00000 n\\n0007277886 00000 n\\n0000014903 00000 n\\n0007278147 00000 n\\n0007278210 00000 n\\n0007278273 00000 n\\n0007278437 00000 n\\n0007278500 00000 n\\n0007278563 00000 n\\n0007278626 00000 n\\n0007278689 00000 n\\n0007278851 00000 n\\n0007279015 00000 n\\n0007279172 00000 n\\n0007279331 00000 n\\n0007279499 00000 n\\n0007279667 00000 n\\n0007279840 00000 n\\n0007280012 00000 n\\n0007280182 00000 n\\n0007280355 00000 n\\n0007280527 00000 n\\n0007280700 00000 n\\n0007280869 00000 n\\n0007281032 00000 n\\n0007281195 00000 n\\n0007281360 00000 n\\n0007281527 00000 n\\n0007281706 00000 n\\n0000018136 00000 n\\n0007281999 00000 n\\n0007282062 00000 n\\n0007282125 00000 n\\n0007282298 00000 n\\n0007282463 00000 n\\n0007282526 00000 n\\n0007282589 00000 n\\n0007282652 00000 n\\n0007282715 00000 n\\n0007282778 00000 n\\n0000024330 00000 n\\n0007282841 00000 n\\n0000836114 00000 n\\n0007282917 00000 n\\n0007283085 00000 n\\n0007283250 00000 n\\n0007283415 00000 n\\n0007283582 00000 n\\n0007283749 00000 n\\n0007283917 00000 n\\n0007284084 00000 n\\n0007284251 00000 n\\n0007284418 00000 n\\n0007284599 00000 n\\n0007284780 00000 n\\n0007284960 00000 n\\n0007285140 00000 n\\n0007285302 00000 n\\n0007285464 00000 n\\n0007285664 00000 n\\n0000021529 00000 n\\n0007285948 00000 n\\n0007286011 00000 n\\n0007286074 00000 n\\n0007286136 00000 n\\n0000821926 00000 n\\n0001650964 00000 n\\n0007286200 00000 n\\n0007286263 00000 n\\n0007286326 00000 n\\n0007286389 00000 n\\n0005076832 00000 n\\n0007286452 00000 n\\n0007286515 00000 n\\n0007286578 00000 n\\n0007286641 00000 n\\n0007286804 00000 n\\n0007286967 00000 n\\n0007287141 00000 n\\n0007287315 00000 n\\n0007287493 00000 n\\n0007287671 00000 n\\n0007287840 00000 n\\n0007288008 00000 n\\n0007288176 00000 n\\n0007288345 00000 n\\n0007288521 00000 n\\n0007288697 00000 n\\n0007288860 00000 n\\n0007289023 00000 n\\n0007289199 00000 n\\n0007289372 00000 n\\n0007289544 00000 n\\n0007289716 00000 n\\n0007289885 00000 n\\n0007290054 00000 n\\n0007290222 00000 n\\n0007290391 00000 n\\n0007290560 00000 n\\n0007290728 00000 n\\n0007290904 00000 n\\n0007291080 00000 n\\n0007291262 00000 n\\n0007291444 00000 n\\n0007291621 00000 n\\n0007291800 00000 n\\n0007291971 00000 n\\n0007292142 00000 n\\n0007292319 00000 n\\n0007292498 00000 n\\n0007292667 00000 n\\n0007292835 00000 n\\n0007293000 00000 n\\n0001668071 00000 n\\n0007293429 00000 n\\n0007293492 00000 n\\n0007293658 00000 n\\n0007293778 00000 n\\n0007293841 00000 n\\n0007293904 00000 n\\n0007293967 00000 n\\n0007294030 00000 n\\n0007294093 00000 n\\n0007294156 00000 n\\n0007294219 00000 n\\n0007294282 00000 n\\n0007294345 00000 n\\n0007294408 00000 n\\n0007294471 00000 n\\n0007294534 00000 n\\n0001674060 00000 n\\n0002285572 00000 n\\n0007294701 00000 n\\n0007294871 00000 n\\n0007295099 00000 n\\n0001671550 00000 n\\n0007295255 00000 n\\n0007295318 00000 n\\n0007295381 00000 n\\n0007295444 00000 n\\n0002273140 00000 n\\n0002952301 00000 n\\n0002967587 00000 n\\n0003419178 00000 n\\n0007295508 00000 n\\n0007295675 00000 n\\n0007295844 00000 n\\n0007296013 00000 n\\n0007296195 00000 n\\n0007296377 00000 n\\n0007296546 00000 n\\n0007296715 00000 n\\n0007296883 00000 n\\n0007297052 00000 n\\n0007297280 00000 n\\n0002964733 00000 n\\n0007297476 00000 n\\n0007297539 00000 n\\n0007297603 00000 n\\n0007297666 00000 n\\n0007297728 00000 n\\n0003404990 00000 n\\n0003828834 00000 n\\n0007297792 00000 n\\n0007297961 00000 n\\n0007298129 00000 n\\n0007298297 00000 n\\n0007298465 00000 n\\n0007298631 00000 n\\n0007298796 00000 n\\n0003843022 00000 n\\n0007298993 00000 n\\n0007299056 00000 n\\n0007299119 00000 n\\n0007299182 00000 n\\n0007299245 00000 n\\n0003849689 00000 n\\n0004316376 00000 n\\n0004572233 00000 n\\n0004805422 00000 n\\n0007299414 00000 n\\n0007299583 00000 n\\n0007299825 00000 n\\n0003847768 00000 n\\n0007299997 00000 n\\n0007300060 00000 n\\n0007300123 00000 n\\n0007300185 00000 n\\n0007300249 00000 n\\n0007300313 00000 n\\n0007300377 00000 n\\n0004307433 00000 n\\n0004563290 00000 n\\n0004796479 00000 n\\n0005062644 00000 n\\n0005108449 00000 n\\n0007300440 00000 n\\n0007300609 00000 n\\n0007300777 00000 n\\n0007300945 00000 n\\n0007301108 00000 n\\n0007301270 00000 n\\n0007301433 00000 n\\n0007301596 00000 n\\n0007301757 00000 n\\n0007301918 00000 n\\n0007302083 00000 n\\n0005079569 00000 n\\n0007302248 00000 n\\n0007302311 00000 n\\n0007302477 00000 n\\n0007302639 00000 n\\n0007302801 00000 n\\n0007302963 00000 n\\n0007303126 00000 n\\n0007303289 00000 n\\n0007303453 00000 n\\n0007303617 00000 n\\n0007303781 00000 n\\n0007303949 00000 n\\n0007304117 00000 n\\n0007304280 00000 n\\n0007304443 00000 n\\n0007304605 00000 n\\n0007304767 00000 n\\n0007304942 00000 n\\n0007305117 00000 n\\n0007305279 00000 n\\n0007305441 00000 n\\n0007305624 00000 n\\n0007305807 00000 n\\n0007305982 00000 n\\n0007306157 00000 n\\n0007306318 00000 n\\n0007306481 00000 n\\n0007306644 00000 n\\n0007306803 00000 n\\n0007306981 00000 n\\n0007307159 00000 n\\n0007307341 00000 n\\n0007307522 00000 n\\n0007307685 00000 n\\n0007307848 00000 n\\n0007308011 00000 n\\n0007308174 00000 n\\n0007308336 00000 n\\n0007308497 00000 n\\n0007308662 00000 n\\n0007308827 00000 n\\n0007308990 00000 n\\n0007309153 00000 n\\n0007309333 00000 n\\n0007309513 00000 n\\n0007309684 00000 n\\n0007309855 00000 n\\n0007309992 00000 n\\n0005082652 00000 n\\n0007310485 00000 n\\n0007310548 00000 n\\n0007310707 00000 n\\n0007310827 00000 n\\n0007310890 00000 n\\n0007310953 00000 n\\n0007311016 00000 n\\n0007311079 00000 n\\n0007311142 00000 n\\n0007311205 00000 n\\n0007311268 00000 n\\n0007311331 00000 n\\n0007311394 00000 n\\n0007311457 00000 n\\n0007311520 00000 n\\n0007311583 00000 n\\n0007311646 00000 n\\n0007311709 00000 n\\n0007311772 00000 n\\n0007311835 00000 n\\n0007311897 00000 n\\n0007311960 00000 n\\n0007312022 00000 n\\n0007312186 00000 n\\n0007312348 00000 n\\n0007312511 00000 n\\n0007312674 00000 n\\n0007312840 00000 n\\n0007313005 00000 n\\n0007313180 00000 n\\n0007313356 00000 n\\n0007313524 00000 n\\n0007313692 00000 n\\n0007313859 00000 n\\n0007314026 00000 n\\n0007314177 00000 n\\n0005085553 00000 n\\n0007314454 00000 n\\n0007314517 00000 n\\n0007314717 00000 n\\n0007314779 00000 n\\n0007314842 00000 n\\n0007314905 00000 n\\n0007314968 00000 n\\n0007315031 00000 n\\n0007315094 00000 n\\n0007315255 00000 ã™é½³\\x12\\nÑÈ\\nh·Ûö\\x07\\x01\\xa0²k×®k�Ö××\\x17\\x0fÊÝ»w\\x0fv™˜˜ˆï8444Øc||<\\x0eŒŒŒô\\x0eŒ��Å�}ûöõ\\x0eŒŽŽÆ�ðvï@x¯8\\x10>Zï@øwã@ø|z\\x07Âg\\n\\x07ÂW4X\\'\\x0eLNNÖ\\x0eÄ:wàÀ�Ú�åååj`jjªv`ii©\\x1a8xð`íÀââb50==];°°°P\\nÌÌÌÔ\\x0e\\n:t¨\\x1a˜��\\xad\\n˜ŸŸ¯\\x06æææj\\x07â/ã…7j\\x07Â;V\\x03áCÕ\\x0e„\\x7fº\\x1a\\x08ŸLí@øä«�ðåÔ\\x0e„/¿\\x1a\\x08/Hí@x\\x01«�ð’Ö\\x0e„oA5\\x10¾)µ\\x03á›\\x18{uí@x\\nâ#Q; \\x1a¢!\\x1a¢!\\x1a¢±V4öîÝ\\x1bïµÂÛ¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑènVñµ\\x12\\nÑ\\x10\\n§†hˆ†hˆ†h4ˆÆððplV\\x03\\x03\\x03¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑèÞ\\x1f¬~\\x1a(\\x1a¢!\\x1a[üÔ¨Ý\\x1f\\x14\\nÑ\\x10\\n§Æ–�ÆØôkŸûÑ\\x7fü—»\\x7f\\x7fÙÿøým�üÇÿ}áŸÇˆÆªß³\\x12\\nÑÈ�F\\x7f\\x7f¿ýA¨lß¾}ÛÚì\\x0fªjþ/Æÿà‹†hˆ†hˆFN4ì\\x0fŠ†h85DC4DC4Dc£¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1a\\x1b\\x15\\nûƒ¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑ°?(\\x1a¢áÔØ¤h,Tí@ø§«�ðÉÔ\\x0e„O¾\\x1a\\x08_Ní@øò«�ð‚Ô\\x0e„\\x17°\\x1a\\x08/ií@ø\\x16T\\x03á›R;\\x10¾‰±W×\\x0e„Ç >\\x12µ\\x03¢!\\x1a¢!\\x1a¢!\\x1akEcïÞ½ñ^+¼-\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îf\\x15_+Ñ\\x10\\nÑpjˆ†hˆ†hˆFƒh\\n\\x0f\\x0fÇf500 \\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îýÁê§�¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�fÑXõ{V¢!\\x1aùÑèïï·?\\x08•íÛ·o[›ýAUÍÿÅø\\x1f|Ñ\\x10\\nÑ\\x10\\nÑÈ‰†ýAÑ\\x10\\n§†hˆ†hˆ†hlT4ì\\x0fŠ†h85DC4DC4Dc£¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4NúhØ\\x1f\\x04€èØ\\x7f\\x7f°Õjù+êþTô\\x1bþŠº\\'_4DÃ“/\\x1a¢Ñ4\\x1a�N\\'Þk…·EC4DÃ“/\\x1a¢áÉ\\x17\\nÑh\\n�îf\\x15\\x7fº$\\x1a¢!\\x1a\\x06DC4<ù¢!\\x1a\\n‰Ú;+Ñ\\x10\\nÑ0 \\x1a¢a@4D£Á3Ó½?¸ªY‰†h85œ\\x1a¢!\\x1a\\x06DC4ÖõÌôÞY‰†hd\\x0e´Ûmûƒ°–p8ÆƒrÏž=^\\x10€Æº\\x7fb\\x18\\x7fË\\nÍ\\n@³\\x02Ð¬Ð¬Ø<Ýk\\x11ö\\x07\\x01à\\x18\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xP¶Z\\xad#G‹cGÖ°E\\x06VVV6j ¼a`S\\x07â74\\x7fÀ“/\\x1a¢áÉ\\x17\\nÑXo4:�N¼×\\no‹†hˆ†\\'_4DÃ“/\\x1a¢Ñ8\\x1aÝÍ*þÄP4DC4\\nˆ†hxòEC4\\x1a<\\x12µwV¢!\\x1a¢a@4DÃ€hˆFƒg¦û·ÜW5+Ñ\\x10\\n§†SC4DÃ€hˆÆºž™Þ;+Ñ\\x10�Ì�v»m\\x7f\\x10*»víºöh}}}ñ\\xa0Ü½{÷`—‰‰‰øŽCCCƒ=ÆÇÇãÀÈÈHïÀØØX\\nØ·o_ïÀèèh\\n\\x08o÷\\x0e„÷Š\\x03á£õ\\x0e„\\x7f7\\x0e„Ï§w |æq |EƒuâÀäädí@¬s\\x07\\x0e\\n¨\\nX^^®\\x06¦¦¦j\\x07–––ª�ƒ\\x07\\x0fÖ\\x0e,..V\\x03ÓÓÓµ\\x03\\nÕÀÌÌLíÀ¡C‡ª�ÙÙÙÚ�ùùùj`nn®v þ2^x£v ¼c5\\x10>Tí@ø§«�ðÉÔ\\x0e„O¾\\x1a\\x08_Ní@øò«�ð‚Ô\\x0e„\\x17°\\x1a\\x08/ií@ø\\x16T\\x03á›R;\\x10¾‰±W×\\x0e„Ç >\\x12µ\\x03¢!\\x1a¢!\\x1a¢!\\x1akEcïÞ½ñ^+¼-\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îf\\x15_+Ñ\\x10\\nÑpjˆ†hˆ†hˆFƒh\\n\\x0f\\x0fÇf500 \\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îýÁê§�¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�fÑXõ{V¢!\\x1aùÑèïï·?\\x08•íÛ·o[›ýAUÍÿÅø\\x1f|Ñ\\x10\\nÑ\\x10\\nÑÈ‰†ýAÑ\\x10\\n§†hˆ†hˆ†hlT4ì\\x0fŠ†h85DC4DC4Dc£¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4NúhØ\\x1f\\x04€èØ\\x7f\\x7f°Õjù+êþTô\\x1bþŠº\\'_4DÃ“/\\x1a¢Ñ4\\x1a�N\\'Þk…·EC4DÃ“/\\x1a¢áÉ\\x17\\nÑh\\n�îf\\x15\\x7fº$\\x1a¢!\\x1a\\x06DC4<ù¢!\\x1a\\n‰Ú;+Ñ\\x10\\nÑ0 \\x1a¢a@4D£Á3Ó½?¸ªY‰†h85œ\\x1a¢!\\x1a\\x06DC4ÖõÌôÞY‰†hd\\x0e´Ûmûƒ°–p8ÆƒrÏž=^\\x10€Æº\\x7fb\\x18\\x7fË\\nÍ\\n@³\\x02Ð¬Ð¬Ø<Ýk\\x11ö\\x07\\x01à\\x18\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xP¶Z\\xad#G‹cGÖ°E\\x06VVV6j ¼a`S\\x07â74\\x7fÀ“/\\x1a¢áÉ\\x17\\nÑXo4:�N¼×\\no‹†hˆ†\\'_4DÃ“/\\x1a¢Ñ8\\x1aÝÍ*þÄP4DC4\\nˆ†hxòEC4\\x1a<\\x12µwV¢!\\x1a¢a@4DÃ€hˆFƒg¦û·ÜW5+Ñ\\x10\\n§†SC4DÃ€hˆÆºž™Þ;+Ñ\\x10�Ì�v»m\\x7f\\x10*»víºöh}}}ñ\\xa0Ü½{÷`—‰‰‰øŽCCCƒ=ÆÇÇãÀÈÈHïÀØØX\\nØ·o_ïÀèèh\\n\\x08o÷\\x0e„÷Š\\x03á£õ\\x0e„\\x7f7\\x0e„Ï§w |æq |EƒuâÀäädí@¬s\\x07\\x0e\\n¨\\nX^^®\\x06¦¦¦j\\x07–––ª�ƒ\\x07\\x0fÖ\\x0e,..V\\x03ÓÓÓµ\\x03\\nÕÀÌÌLíÀ¡C‡ª�ÙÙÙÚ�ùùùj`nn®v þ2^x£v ¼c5\\x10>Tí@ø§«�ðÉÔ\\x0e„O¾\\x1a\\x08_Ní@øò«�ð‚Ô\\x0e„\\x17°\\x1a\\x08/ií@ø\\x16T\\x03á›R;\\x10¾‰±W×\\x0e„Ç >\\x12µ\\x03¢!\\x1a¢!\\x1a¢!\\x1akEcïÞ½ñ^+¼-\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îf\\x15_+Ñ\\x10\\nÑpjˆ†hˆ†hˆFƒh\\n\\x0f\\x0fÇf500 \\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\n�îýÁê§�¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�fÑXõ{V¢!\\x1aùÑèïï·?\\x08•íÛ·o[›ýAUÍÿÅø\\x1f|Ñ\\x10\\nÑ\\x10\\nÑÈ‰†ýAÑ\\x10\\n§†hˆ†hˆ†hlT4ì\\x0fŠ†h85DC4DC4Dc£¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4NúhØ\\x1f\\x04€èØ\\x7f\\x7f°Õjù+êþTô\\x1bþŠº\\'_4DÃ“/\\x1a¢Ñ4\\x1a�N\\'Þk…·EC4DÃ“/\\x1a¢áÉ\\x17\\nÑh\\n�îf\\x15\\x7fº$\\x1a¢!\\x1a\\x06DC4<ù¢!\\x1a\\n‰Ú;+Ñ\\x10\\nÑ0 \\x1a¢a@4D£Á3Ó½?¸ªY‰†h85œ\\x1a¢!\\x1a\\x06DC4ÖõÌôÞY‰†hd\\x0e´Ûmûƒ°–p8ÆƒrÏž=^\\x10€Æº\\x7fb\\x18\\x7fË\\nÍ\\n@³\\x02Ð¬Ð¬Ø<Ýk\\x11ö\\x07\\x01à\\x18\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07e«Õ:r´8vd\\n[d`eee£\\x06Â\\x1b\\x066u ~Có\\x07<ù¢!\\x1až|Ñ\\x10�õF£ÓéÄ{\\xadð¶hˆ†hxòEC4<ù¢!\\x1a�£ÑÝ¬âO\\nEC4DÃ€hˆ†\\'_4D£Á#Q{g%\\x1a¢!\\x1a\\x06DC4\\nˆ†h4xfº\\x7fË}U³\\x12\\nÑpj85DC4\\nˆ†h¬ë™é½³\\x12\\nÑÈ\\nh·Ûö\\x07\\x01\\xa0²k×®k�Ö××\\x17\\x0fÊÝ»w\\x0fv™˜˜ˆï8444Øc||<\\x0eŒŒŒô\\x0eŒ��Å�}ûöõ\\x0eŒŽŽÆ�ðvï@x¯8\\x10>Zï@øwã@ø|z\\x07Âg\\n\\x07ÂW4X\\'\\x0eLNNÖ\\x0eÄ:wàÀ�Ú�åååj`jjªv`ii©\\x1a8xð`íÀââb50==];°°°P\\nÌÌÌÔ\\x0e\\n:t¨\\x1a˜��\\xad\\n˜ŸŸ¯\\x06æææj\\x07â/ã…7j\\x07Â;V\\x03áCÕ\\x0e„\\x7fº\\x1a\\x08ŸLí@øä«�ðåÔ\\x0e„/¿\\x1a\\x08/Hí@x\\x01«�ð’Ö\\x0e„oA5\\x10¾)µ\\x03á›\\x18{uí@x\\nâ#Q; \\x1a¢!\\x1a¢!\\x1a¢±V4öîÝ\\x1bïµÂÛ¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑènVñµ\\x12\\nÑ\\x10\\n§†hˆ†hˆ†h4ˆÆððplV\\x03\\x03\\x03¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑèÞ\\x1f¬~\\x1a(\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\x16�U¿g%\\x1a¢‘\\x1f�þþ~ûƒPÙ¾}û¶µÙ\\x1fTÕü_ŒÿÁ\\x17\\nÑ\\x10\\nÑ\\x10�œhØ\\x1f\\x14\\nÑpjˆ†hˆ†hˆÆFEÃþ\\xa0hˆ†SC4DC4DC46*\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07Eã¤�†ýAˆŽý÷\\x07[\\xad–¿¢îOE¿á¯¨{òEC4<ù¢!\\x1aM£Ñétâ½Vx[4DC4<ù¢!\\x1až|Ñ\\x10�ÆÑènVñ§K¢!\\x1a¢a@4DÃ“/\\x1a¢Ñà‘¨½³\\x12\\nÑ\\x10\\n\\x03¢!\\x1a\\x06DC4\\x1a<3Ýûƒ«š•hˆ†SÃ©!\\x1a¢a@4Dc]ÏLï�•hˆFæ@»Ý¶?\\x08k\\t‡c<(÷ìÙã\\x05\\x01h¬û\\'†ñ·Ü\\x01Ð¬4+Í\\nÍŠÍÓ½\\x16a\\x7f\\x10ŽqPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07e«Õ:r´8vd\\n[d`eee£\\x06Â\\x1b\\x066u ~Có\\x07<ù¢!\\x1až|Ñ\\x10�õF£ÓéÄ{\\xadð¶hˆ†hxòEC4<ù¢!\\x1a�£ÑÝ¬âO\\nEC4DÃ€hˆ†\\'_4D£Á#Q{g%\\x1a¢!\\x1a\\x06DC4\\nˆ†h4xfº\\x7fË}U³\\x12\\nÑpj85DC4\\n ˆ†h¬ë™é½³\\x12\\nÑÈ\\nh·Ûö\\x07\\x01\\xa0²k×®k�Ö××\\x17\\x0fÊÝ»w\\x0fv™˜˜ˆï8444Øc||<\\x0eŒŒŒô\\x0eŒ��Å�}ûöõ\\x0eŒŽŽÆ�ðvï@x¯8\\x10>Zï@øwã@ø|z\\x07Âg\\n\\x07ÂW4X\\'\\x0eLNNÖ\\x0eÄ:wàÀ�Ú�åååj`jjªv`ii©\\x1a8xð`íÀââb50==];°°°P\\nÌÌÌÔ\\x0e\\n:t¨\\x1a˜��\\xad\\n˜ŸŸ¯\\x06æææj\\x07â/ã…7j\\x07Â;V\\x03áCÕ\\x0e„\\x7fº\\x1a\\x08ŸLí@øä«�ðåÔ\\x0e„/¿\\x1a\\x08/Hí@x\\x01«�ð’Ö\\x0e„oA5\\x10¾)µ\\x03á›\\x18{uí@x\\nâ#Q; \\x1a¢!\\x1a¢!\\x1a¢±V4öîÝ\\x1bïµÂÛ¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑènVñµ\\x12\\nÑ\\x10\\n§†hˆ†hˆ†h4ˆÆððplV\\x03\\x03\\x03¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑèÞ\\x1f¬~\\x1a(\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\x16�U¿g%\\x1a¢‘\\x1f�þþ~ûƒPÙ¾}û¶µÙ\\x1fTÕü_ŒÿÁ\\x17\\nÑ\\x10\\nÑ\\x10�œhØ\\x1f\\x14\\nÑpjˆ†hˆ†hˆÆFEÃþ\\xa0hˆ†SC4DC4DC46*\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07Eã¤�†ýAˆŽý÷\\x07[\\xad–¿¢îOE¿á¯¨{òEC4<ù¢!\\x1aM£Ñétâ½Vx[4DC4<ù¢!\\x1až|Ñ\\x10�ÆÑènVñ§K¢!\\x1a¢a@4DÃ“/\\x1a¢Ñà‘¨½³\\x12\\nÑ\\x10\\n\\x03¢!\\x1a\\x06DC4\\x1a<3Ýûƒ«š•hˆ†SÃ©!\\x1a¢a@4Dc]ÏLï�•hˆFæ@»Ý¶?\\x08k\\t‡c<(÷ìÙã\\x05\\x01h¬û\\'†ñ·Ü\\x01Ð¬4+Í\\nÍŠÍÓ½\\x16a\\x7f\\x10ŽqPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07e«Õ:r´8vd\\n[d`eee£\\x06Â\\x1b\\x066u ~Có\\x07<ù¢!\\x1až|Ñ\\x10�õF£ÓéÄ{\\xadð¶hˆ†hxòEC4<ù¢!\\x1a�£ÑÝ¬âO\\nEC4DÃ€hˆ†\\'_4D£Á#Q{g%\\x1a¢!\\x1a\\x06DC4\\nˆ†h4xfº\\x7fË}U³\\x12\\nÑpj85DC4\\nˆ†h¬ë™é½³\\x12\\nÑÈ\\nh·Ûö\\x07\\x01\\xa0²k×®k�Ö××\\x17\\x0fÊÝ»w\\x0fv™˜˜ˆï8444Øc||<\\x0eŒŒŒô\\x0eŒ��Å�}ûöõ\\x0eŒŽŽÆ�ðvï@x¯8\\x10>Zï@øwã@ø|z\\x07Âg\\n\\x07ÂW4X\\'\\x0eLNNÖ\\x0eÄ:wàÀ�Ú�åååj`jjªv`ii©\\x1a8xð`íÀââb50==];°°°P\\nÌÌÌÔ\\x0e\\n:t¨\\x1a˜��\\xad\\n˜ŸŸ¯\\x06æææj\\x07â/ã…7j\\x07Â;V\\x03áCÕ\\x0e„\\x7fº\\x1a\\x08ŸLí@øä«�ðåÔ\\x0e„/¿\\x1a\\x08/Hí@x\\x01«�ð’Ö\\x0e„oA5\\x10¾)µ\\x03á›\\x18{uí@x\\nâ#Q; \\x1a¢!\\x1a¢!\\x1a¢±V4öîÝ\\x1bïµÂÛ¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑènVñµ\\x12\\nÑ\\x10\\n§†hˆ†hˆ†h4ˆÆððplV\\x03\\x03\\x03¢!\\x1a¢áÔ\\x10\\nÑ\\x10\\nÑ\\x10�ÆÑèÞ\\x1f¬~\\x1a(\\x1a¢!\\x1aN\\nÑ\\x10\\nÑ\\x10\\nÑh\\x16�U¿g%\\x1a¢‘\\x1f�þþ~ûƒPÙ¾}û¶µÙ\\x1fTÕü_ŒÿÁ\\x17\\nÑ\\x10\\nÑ\\x10�œhØ\\x1f\\x14\\nÑpjˆ†hˆ†hˆÆFEÃþ\\xa0hˆ†SC4DC4DC46*\\x1aö\\x07EC4œ\\x1a¢!\\x1a¢!\\x1a¢a\\x7fP4DÃ©!\\x1a¢!\\x1a¢!\\x1aö\\x07Eã¤�†ýAˆŽý÷\\x07[\\xad–¿¢îOE¿á¯¨{òEC4<ù¢!\\x1aM£Ñétâ½Vx[4DC4<ù¢!\\x1až|Ñ\\x10�ÆÑènVñ§K¢!\\x1a¢a@4DÃ“/\\x1a¢Ñà‘¨½³\\x12\\nÑ\\x10\\n\\x03¢!\\x1a\\x06DC4\\x1a<3Ýûƒ«š•hˆ†SÃ©!\\x1a¢a@4Dc]ÏLï�•hˆFæ@»Ý¶?\\x08k\\t‡c<(÷ìÙã\\x05\\x01h¬û\\'†ñ·Ü\\x01Ð¬4+Í\\nÍŠÍÓ½\\x16a\\x7f\\x10ŽqPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08‰\\x07¥ýA€\\nîµ4+Í\\n@³\\x02@³¢0ûƒ�xPÚ\\x1f\\x04Èá^\\n@³\\x02Ð¬4+4+\\n³?\\x08°Êìììï~÷»\\x1füà\\x07_þò—¯¼òÊw¿ûÝç�wÞ\\x19gœqÚi§�sÎ9\\x17\\\\pÁ%—\\\\Ò××wÃ\\n7|ë[ßzðÁ\\x07Ÿzê [MÈðÈH“Wyñæ]ò\\x06gÆô“^ú%0¶vŒ®‡Ëfao±cú6§ë•’[R\\'?850Ö®¾Í.åY “›2î\\x01åùÁÜbE«x÷ª_Uæ\\x07—�wÈÊ\\nŽ‰ü g\\x13\\x12\\x12’——×týçææ^wÝu&3qíÚµãZHÖ¯_¯$n™””d\\x0f\\x1b%ùAsp_\\n€Ê\\n€Ê\\n˜…ü ••:\\x05—È\\x0f²rk$\\x1f\\x17\\nÉ>Ÿf5Ú[þY÷\\x04ý\\x06ÛæH\\x0e×ó4®ŠbÛ�RQ–…óƒ«4â\\x0e«tN¾ú#l—\\x1fŒŒŽ1y•—î<,o\\x103öQé¥Kyu’tÛ«4·\\n\\x1a�y¨™]²9\\x07â�gýÖì£…e•vµwd\\\\)3¿ãKãz)\\n÷õôØ¥W–tÞ½f†Êüàò\\t\\x0eYYÁ1‘\\x1f\\x04‹{óÍ7•dâŠ‹‹\\x19+i\\x10®½öZ“cµ\\x7fÿ~{Ø(É\\x0f\\x02˜ƒûZTVTVÀ,ä\\x07©¬T\\n\\\\*ùAVn\\ná“(ýG4«ÑN\\t\\xad{‚g6ÚæHŽÔó4®Š\\x12Û�RqŽ…óƒ«½Ä\\nÚð\\x19‹ö!pòÇ¶ZþgãâM^åUûÃå\\n.Œû-\\x12èwô¢1I×ËsW±æNyãè±�4¿«\\x16�veyHâÁØ¬²J›\\x06`E—OáÓ\\x1bvylO…á¾AS\\x0e(ìs÷ºÙ*óƒ+&9de\\x05ÇD~\\x10,nÆŒ\\x19JòƒçÎ�c¬$\\x03\\x06\\n09VK–,±‡�’ü €9¸¯\\x05@e\\x05@e\\x05ÌB~�ÊJ�Â\\nòƒ¬Ü\\x1aÚ\\nÁ‘¬ù¨¹\\nøÑi\\x7fœ]ÀO6‹°\\n�.¾ô•e¶\\x1f¢’\\\\ËNËSkÆÉ{Ózµfõÿ8wS�aÙí1È:\\ns1ÅäUÞp,VÞàâ¸\\x07�\\n.f\\x17¯\\nM>–�\\x13�v%[ÓAÞøìØ~\\\\ekî\\nW,ñ<ÄÔ±÷É{Þçñt�¿™å>lØ\"¥¿7¾gý|uùÁ½+\\x7fvÈÊ\\nŽ‰ü XÜ‘#G”ä\\x07÷îÝËXI>ûì3“c5mÚ4{Ø(É\\x0f\\x02˜ƒûZTVTVÀ,ä\\x07©¬Ô)Ê\\x12Ìœ\\x15osÕœqåÊ�dï˜f8æ…\\x19Õ1\\x015Ï[´¡à™âK¯\\xad°ýø”æ[øùƒk½å½UxÝÎê\\x1f¶èx€Ç\\nÆ1)×Üþ�Ûdë|tüå\\n“WÙÿôeyƒ”q½å½]Ê+Iñ¼WÞøÌØÇ¸ÊÖÜ;J+,ðHÄ´±=ä={Œþ®ö?Vin\\nè²Ôec¤Â>\\x037-R™\\x1f\\\\5Õ!++8&òƒ`q©©©JòƒÒnÎXIFŽ\\nir¬|}}ía£$?\\x08`\\x0eîk\\x01PY\\x01PY\\x01³�\\x1f¤²R§8‡ç\\x0f²r\\x7f³æ£ºG’\\nÅ%j\\x12Çæˆ/½®ÒöÇVvÅÂÏ\\x1f\\\\ç+\\x08:yÝÅ,ødñ‰{]¶N\\x18ýÕ\\x11��\\x1b=\\x06¿ã:¥³Ë\\x0eë|tbFžÉ«\\n\\x14-È\\x18^öî%ï-§¨<Öó~yã¨q\\x03¸ÊÖÜ;´º*ó;N\\x1bÛ]Þs\\x17\\x17ÿ�\\n¯þ\\x16þÕ´\\n9z”4W§\\x07Å)ìsï\\x16¿†s‚åšÛÄùÁÕ¿:de\\x05ÇD~\\x10,\\x7f³¡¸XI~põêÕŒ•ÄÃÃÃäXMš4É\\n6Jòƒæà¾\\x16•\\x15•\\x150\\nùA*+uJóÈ\\x0f²r\\x7f“x´zÜ�µ¦Á›Õz=—¨I\\nŸ\\'¾ôU:Û\\x1f[E±…óƒëÇË{+òjÇ,øtñ‰Î.;êüXç£/å\\x16™¼ÊÁñÙò\\x06\\x19Þ=å½\\x15—k#<\\x1f’7Ž\\n7�«lÍ½Co‰oìŒ±Ýä=\\x1b&ç“®K†¹�\\x7fÈeµá\\x1f7†]RØgÐ¶åõ%\\x07ÿÏÍ»¿Ë²>.ë.iº\\nòƒk¦;de\\x05ÇD~\\x10,N§Ó)É\\x0f®ZµŠ±’üðÃ\\x0f&ÇjÑ¢Eö°Q’\\x1f\\x040\\x07÷µ¨¬¨¬€YÈ\\x0fRY©SV ˜9+Þæª9éÊM>^½á‹êE/Vï÷\\xad®(áú4•\\x13\\nêÉ\\x0fVÙþØ*K-;-Oo˜(ïíÊ˜»™\\x056Ì\\x0ff\\\\)3y•O§äË\\x1bdŽë.ïMW¥?æÙ_Þø´÷S\\\\e‡Û;2Çt\\xad/?Xç\\'$>GaŸAþ«êË\\x0f~âækè-ÉS�[Ü»n¦CVVpLä\\x07\\x01ÀâŠŠŠ”ä\\x07ýýý\\x19+ÉçŸ\\x7fnr¬¶lÙb\\x0f\\x1b%ùAsp_\\n€Ê\\n€Ê\\n˜…ü ••:å…ä\\x07Y¹°¶ÐEâKo\\x0f\\x0f|Ô–[8?¸i²¼·¼1�˜\\x05®›¢l•\\x1fÌ-®0y•c3\\x04»CÚ¸žÂ\\x0e\\x0fz>-o|Êûo\\\\e‡Û;²\\x14ç\\x07“s”fÌ÷\\x05¬\\xad/?8Ôm¢¡·‹žÝå¯\\x06\\xad›å�•\\x15\\n\\x13ùA°¸¤¤$%ùÁàà`ÆJ2pà@“c\\x15\\x16\\x16f\\x0f\\x1b%ùAsp_\\n€Ê\\n€Ê\\n˜…\\x14\\x12••:\\x15%ä\\x07Y¹°¶“Kì÷Òë*EÇv«êþNoš\"ï0gl\\x17fABVQ�@Ö—KC\\xad´+•Ušœ�)¹%{=ž©Óàç©ã…\\n\\x06z½(ï-Üç\\x19®²Ãí\\nÙcîQ˜\\x1f,×ê\\x14ö¹\\x7f×Æúòƒï»þlè-Þó>ù«û6ÌqÈÊ\\nŽ‰ü X\\\\PP�’ü`vv6cUYYÙªU«†\\x07Jj\\xa0Óélu„ä\\x07\\x01,…ûZTVTVÀ,\\x16�{À‰*«Ê2òƒv¶rÉ\\x0f:�ðeòë®ÓØÇ—v•N0\\'ÇüUu\\x7f§·ü\"ï0kì½Ì\\x02½^ÿ…_¨1�ÕmtÀ¡Ø,ë|ti…Îä—ONQù÷n.µ_½¢i3túNa‡;Æ¾!ï-Ìç9®²Ãí\\n9ŠóƒÊû<°gk}ùÁ·]§\\x1az‹ó¼_ðüÁ�ó\\n²²‚c\"?\\x08\\x167aÂ\\x04“áÁ»îº‹�’ìÚµËäX½øâ‹v²Q’\\x1f\\x040\\x07÷µ¨¬¨¬€YŽN«û{×§V0*TV¦i+È\\x0fÚ\\x12ùAç$}?Ë®{¥¦µ]\\n›^/˜“coSÝ_Ä–_å\\nfŒëÁ,�”kuS\\x03cßš}ôŸ+Ã�^°ÞƒW´º*“_>¥\\x15ºÎ.Û\\'�\\n^¨i#½”èÙý\\n×i¯N?,ìp«÷ûòÞNú>Ï%v¸½#wL\\'‹ç\\x07\\x0f\\x05m¯/?8Äuš¡·\\x18Ï\\x07\\x04Ï\\x1fÜ´À!++8&òƒ`q/¾ø¢ÉLÜûï¿Ï@IÞyç\\n“c5uêT;Ù(É\\x0f\\x02˜ƒûZTVTVÀ,\\x05—«§ôüã—®§?\\\\]’Ë¨PY™¦Ó’\\x1f´%òƒÎéô*ùu¯°“ü\\xa0pZŽ»Cug‘ÛfÈ;L\\x1bw?³À†ôÂ”èŸ¿|¤6†`WW—m�»,7üùï¿\\n\\x12v¸aü§òÞB}_d¨\\nnïÈ\\nÓÑòùÁý;ëË\\x0f¾æ:ÃÐ[´goA~pó\"‡¬¬à˜È\\x0f\\x02€eeff^sÍ5&3q\\n\\x17.d¬ÂÃÃ[¶lÙð@Ýxã�yyyv²Q’\\x1f\\x040\\x07÷µ¨¬¨¬€¹r\\x13ª·\\x7f_½ðùê�ÿ\\xad¾’ÊxPY)\"L‘�\\x1f´\\x1aòƒÎ)b\\xadüº—kn·ßiéÝFug‘þ3å\\n^ö~€Y`ÿ_>òÈØ\\xa0)\\x07„�\\xad�ôµ¼·\\x13ã_b˜\\nnïÈ÷ê\\xa0$?ØÛk·ò>�\\nÜS_~ð\\x15×Y†\\x0eÏxö•¿º\\x7fë\\x12‡¬¬à˜È\\x0f\\x02€eùúúš\\n\\x0f^wÝu¹¹Îþ¿|U^^Þ§O\\x1f“cõõ×_ÛÏFI~\\x10À\\nÜ×\\x02\\xa0²\\x02\\xa0²\\x02\\xa0²²\\nòƒv5øä\\x07�Aäzùu/³çü\\xa0O;õçº}¶¼Ã\\x14Ÿ¾Ì\\x02ûÿò‘§Æž™¼_ØÙê)ßË{;>á\\x15†¹‰Ìuÿ¨ÎhoõxÙ\"=\\x17xÝ-¿”ÿY}ªÎLØrê²ò>�\\n\\x0eª/?ø¢ë\\\\C‡‘ž\\x0f\\nòƒÛ–:je\\x05\\x07D~\\x10,¨¨¨èöÛo7™‰{ï½÷œ|\\xa0ôzýG\\x1f}dr\\xa0Zµj•˜˜h?\\x1b%ùAsp_\\n€Ê\\n€Ê\\n€ÊÊ6È\\x0fÚÕà“\\x1ft\\x06g6Ê¯{‰æNû�–¾íUw\\x16\\x190WÞa²ïÃÌ\\x02ûÿò‘ç\\x07Ÿœ¸OØÙÊénòÞŽMx•an\"¯¹Î¨3Ú\\x1f»M°LÅâÕ^~)�Äewu\\n0NƒG½÷–Vè”÷\\x19|ô@}ùÁA®ó\\n}žö|Xþê\\x01ÿeŽZYÁ\\x01‘\\x1f\\x04\\nš8qb\\n\\x05BBBœy”´Zí—_~©d\\xa0æÌ™cW\\x1b%ùAsp_\\n€Ê\\n€Ê\\n€ÊÊ6\\x04ùÁw¸j6\\x1b|òƒÎàìfùu/¶çüàø\\x0eª;‹Ú9_Þa¢o?f�ý\\x7fùÈóƒ\\x1f/\\x14ÿªðŠÙãä½…L|�an\"Òµ\\x185zdÍCK½n©Ð´ö\\x19ý�ô7\\x16éùÀ¸¿Ë/¥^¯ßu&íõ™Gz{íþdñ‰”Ü’FõyìØ‘úòƒO».2L\\xadpÏG\\x04ùÁí+\\nµ²‚#îÌä\\x07\\x01ÀB²³³ï¸ã\\x0e“™¸gžyÆ™G)99ù¹çžS\\x12\\n|á…\\x17¤bÌ®6Jòƒæà¾\\x16•\\x15•\\x15••m�\\x1f´«Á\\'?è\\nÎm•_÷BM\\x1bû�–\\x13:ªîìÌ®…ò\\x0e/NèÏ,°ÿ/Ÿ\\x1f×EÔÉ\\x0f\\n½�-ìlù‚Ÿå½\\x05O\\x1aÂ07\\x11Ãåèá²ù5×\\x19=]6\\x1aþÑ\"=ÿc|Ý\\x07†ÎqÿØøªºß]?\\nz¬¾üà“®~†ƒ\\x0fõì\\'È\\x0fîXå¨•\\x15\\n\\x10ùA°”¡C‡*‰Å\\n9rÄ9Ç\\'##ÃÅÅåæ›oV2J½zõÊËË³·�’ü €9¸¯\\x05@e\\x05@e\\x05@ee\\x1bä\\x07íjðÉ\\x0f:ƒèíòë~EÓÖ~§åÄÎª;;³{‘¼Ãø‰O0\\nìÿË\\',)·»ûNcxðåi‡+´UÂÎ–ûÍ‘÷vtÒ[\\ns\\x13‘?\\x1aÒRùÁ\\'ÇïÝêñ²ñ\"&{Þ;Àe™™}††‡Ö—\\n9\\x15q(6+(:ã¸çãòW\\x0f\\x06¬qÔÊ\\n\\x0eˆü X„¿¿¿’XÜ�!N÷?4Q^^\\n\\x18\\x18øÍ7ß\\\\\\x7fýõ-”¹ûî»SRRìp£$?\\x08`\\x0eîk\\x01PY\\x01PY\\x01PYÙ\\x06ùA\\x1bšÐ©îà¯û„Qiþb\\x02äë®Àžóƒ“º¨îìL\\xa0Ÿ¼Ã\\n\\x13ÿÆ,°»«,\\n/\\x1fKÈùrièóS\\x0fºmŽÊ/', 'score': 0.64719695, 'raw_content': None}, {'title': '[2412.19437v1] DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/abs/2412.19437v1', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. arXiv:2412.19437v1 arXiv author ID DeepSeek-V3 Technical Report We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.19437 Bibliographic and Citation Tools Connected Papers Toggle\", 'score': 0.6466616, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-', 'score': 0.62303346, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437v1', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total ... DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022 Figure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.', 'score': 0.5425973, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/html/2412.19437v1', 'content': 'Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention\\xa0(MLA)\\xa0(DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE\\xa0(Dai et\\xa0al., 2024) for cost-effective training. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy\\xa0(Wang et\\xa0al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models.', 'score': 0.47446132, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/html/2412.19437', 'content': 'Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention\\xa0(MLA)\\xa0(DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE\\xa0(Dai et\\xa0al., 2024) for cost-effective training. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy\\xa0(Wang et\\xa0al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models.', 'score': 0.47446132, 'raw_content': None}, {'title': 'DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open ...', 'url': 'https://arxiv.org/abs/2402.03300', 'content': 'cs arXiv:2402.03300 Help | Advanced Search arXiv author ID DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2402.03300 [cs.CL] (or arXiv:2402.03300v3 [cs.CL] for this version) From: Zhihong Shao [view email] Access Paper: cs.CL cs cs.AI References & Citations Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? Help', 'score': 0.40098447, 'raw_content': None}, {'title': 'DeepSeek LLM: Scaling Open-Source Language Models with Longtermism', 'url': 'https://arxiv.org/abs/2401.02954', 'content': 'cs arXiv:2401.02954 arXiv author ID DeepSeek LLM: Scaling Open-Source Language Models with Longtermism The rapid development of open-source large language models (LLMs) has been truly remarkable. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2401.02954 [cs.CL] (or arXiv:2401.02954v1 [cs.CL] for this version) cs.CL cs cs.AI Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle', 'score': 0.36765668, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2402.03300\n",
      "> RSP [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/abs/2402.03300v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.19437\n",
      "> RSP [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2401.02954\n",
      "> RSP [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/abs/2401.02954v1)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat', 'url': 'https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/', 'content': 'DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch | VentureBeat DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B.', 'score': 0.8727061, 'raw_content': None}, {'title': \"DeepSeek's new AI model appears to be one of the best 'open ...\", 'url': 'https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/', 'content': \"DeepSeek's new AI model appears to be one of the best 'open' challengers yet | TechCrunch DeepSeek's new AI model appears to be one of the best 'open' challengers yet | TechCrunch DeepSeek’s new AI model appears to be one of the best ‘open’ challengers yet The model, DeepSeek V3, was developed by the AI firm DeepSeek and was released on Wednesday under a permissive license that allows developers to download and modify it for most applications, including commercial ones. According to DeepSeek’s internal benchmark testing, DeepSeek V3 outperforms both downloadable, “openly” available models and “closed” AI models that can only be accessed through an API.\", 'score': 0.86108357, 'raw_content': None}, {'title': 'DeepSeek V3: New Open AI Model Surpasses Rivals and ... - WinBuzzer', 'url': 'https://winbuzzer.com/2024/12/27/deepseek-v3-new-open-ai-model-surpasses-rivals-and-challenges-gpt-4o-xcxwbn/', 'content': 'DeepSeek V3: New Open AI Model Surpasses Rivals and Challenges GPT-4o - WinBuzzer DeepSeek V3: New Open AI Model Surpasses Rivals and Challenges GPT-4o DeepSeek V3 is an open-source AI model that outperformes Meta’s Llama 3.1 and comes close to OpenAI’s GPT-4o in key benchmarks. DeepSeek V3’s technical advancements place it among the most powerful AI systems to, rivaling both open-source competitors like Meta’s Llama 3.1 and proprietary models like OpenAI’s GPT-4o. DeepSeek V3’s benchmark results showcase its exceptional capabilities across a broad spectrum of tasks, solidifying its position as a leader among open-source AI models. Related: DeepSeek AI Open Sources VL2 Series of Vision Language Models', 'score': 0.8593928, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-', 'score': 0.8479807, 'raw_content': None}, {'title': 'Introducing DeepSeek-V3 | DeepSeek API Docs', 'url': 'https://api-docs.deepseek.com/news/news1226', 'content': '🚀 Introducing DeepSeek-V3 | DeepSeek API Docs DeepSeek API Docs DeepSeek Platform Your First API Call Introducing DeepSeek-V3 2024/12/26 DeepSeek-V2.5-1210 Release 2024/12/10 DeepSeek-V2.5 Release 2024/09/05 Context Caching is Available 2024/08/02 New API Features 2024/07/25 API Reference API Guides Context Caching API Status Page Introducing DeepSeek-V3 2024/12/26 🚀 Introducing DeepSeek-V3 ⚡ 60 tokens/second (3x faster than V2!) Model 👉 https://github.com/deepseek-ai/DeepSeek-V3 Paper 👉 https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf 💰 API Pricing Update\\u200b 🎉 Until Feb 8: same as V2! | Input (cache miss) | Input (cache hit) | Output | Look forward to multimodal support and other cutting-edge features in the DeepSeek ecosystem. Previous Error CodesNext 🚀 DeepSeek V2.5: The Grand Finale 🎉 🎉 What’s new in V3 💰 API Pricing Update Copyright © 2024 DeepSeek, Inc.', 'score': 0.780947, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "Paper Link\n",
      "👁️\n",
      "1. Introduction\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \n",
      "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \n",
      "Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \n",
      "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \n",
      "Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\n",
      "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\n",
      "In addition, its training process is remarkably stable. \n",
      "Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "2. Model Summary\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n",
      "We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n",
      "    It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.\n",
      "This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.\n",
      "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n",
      "3. Model Downloads\n",
      "| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n",
      "| :------------: | :------------: | :------------: | :------------: | :------------: |\n",
      "| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n",
      "| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n",
      "NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.\n",
      "To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6:\n",
      "How_to Run_Locally\n",
      ".\n",
      "For developers looking to dive deeper, we recommend exploring\n",
      "README_WEIGHTS.md\n",
      "for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n",
      "4. Evaluation Results\n",
      "Base Model\n",
      "Standard Benchmarks\n",
      "|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n",
      "|---|-------------------|----------|--------|-------------|---------------|---------|\n",
      "| | Architecture | - | MoE | Dense | Dense | MoE |\n",
      "| | # Activated Params | - | 21B | 72B | 405B | 37B |\n",
      "| | # Total Params | - | 236B | 72B | 405B | 671B |\n",
      "| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n",
      "| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n",
      "| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n",
      "| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n",
      "| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n",
      "| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n",
      "| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n",
      "| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n",
      "| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n",
      "| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n",
      "| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n",
      "| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n",
      "| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n",
      "| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n",
      "| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n",
      "| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n",
      "| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n",
      "| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n",
      "| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n",
      "| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n",
      "| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n",
      "| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n",
      "| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n",
      "| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n",
      "| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n",
      "| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n",
      "| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n",
      "| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n",
      "| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n",
      "| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n",
      "| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n",
      "| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n",
      "Note: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\n",
      "For more evaluation details, please check our paper.\n",
      "Context Window\n",
      "Evaluation results on the\n",
      "Needle In A Haystack\n",
      "(NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to\n",
      "128K\n",
      ".\n",
      "Chat Model\n",
      "Standard Benchmarks (Models larger than 67B)\n",
      "| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n",
      "|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n",
      "| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n",
      "| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n",
      "| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n",
      "| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n",
      "| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n",
      "| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n",
      "| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n",
      "| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n",
      "| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n",
      "| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n",
      "| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n",
      "| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n",
      "| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n",
      "| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n",
      "| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n",
      "| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n",
      "| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n",
      "| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n",
      "| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n",
      "| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n",
      "| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n",
      "| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n",
      "| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n",
      "| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n",
      "| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n",
      "\n",
      "Note: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n",
      "Open Ended Generation Evaluation\n",
      "| Model | Arena-Hard | AlpacaEval 2.0 |\n",
      "|-------|------------|----------------|\n",
      "| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n",
      "| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n",
      "| LLaMA-3.1 405B | 69.3 | 40.5 |\n",
      "| GPT-4o-0513 | 80.4 | 51.1 |\n",
      "| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n",
      "| DeepSeek-V3 | **85.5** | **70.0** |\n",
      "\n",
      "Note: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n",
      "5. Chat Website & API Platform\n",
      "You can chat with DeepSeek-V3 on DeepSeek's official website:\n",
      "chat.deepseek.com\n",
      "We also provide OpenAI-Compatible API at DeepSeek Platform:\n",
      "platform.deepseek.com\n",
      "6. How to Run Locally\n",
      "DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n",
      "DeepSeek-Infer Demo\n",
      ": We provide a simple and lightweight demo for FP8 and BF16 inference.\n",
      "SGLang\n",
      ": Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n",
      "LMDeploy\n",
      ": Enables efficient FP8 and BF16 inference for local and cloud deployment.\n",
      "TensorRT-LLM\n",
      ": Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n",
      "vLLM\n",
      ": Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n",
      "AMD GPU\n",
      ": Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n",
      "Huawei Ascend NPU\n",
      ": Supports running DeepSeek-V3 on Huawei Ascend devices.\n",
      "Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n",
      "Here is an example of converting FP8 weights to BF16:\n",
      "shell\n",
      "cd inference\n",
      "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n",
      "NOTE: Huggingface's Transformers has not been directly supported yet.\n",
      "6.1 Inference with DeepSeek-Infer Demo (example only)\n",
      "Model Weights & Demo Code Preparation\n",
      "First, clone our DeepSeek-V3 GitHub repository:\n",
      "shell\n",
      "git clone https://github.com/deepseek-ai/DeepSeek-V3.git\n",
      "Navigate to the\n",
      "inference\n",
      "folder and install dependencies listed in\n",
      "requirements.txt\n",
      ".\n",
      "shell\n",
      "cd DeepSeek-V3/inference\n",
      "pip install -r requirements.txt\n",
      "Download the model weights from HuggingFace, and put them into\n",
      "/path/to/DeepSeek-V3\n",
      "folder.\n",
      "Model Weights Conversion\n",
      "Convert HuggingFace model weights to a specific format:\n",
      "shell\n",
      "python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n",
      "Run\n",
      "Then you can chat with DeepSeek-V3:\n",
      "shell\n",
      "torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n",
      "Or batch inference on a given file:\n",
      "shell\n",
      "torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n",
      "6.2 Inference with SGLang (recommended)\n",
      "SGLang\n",
      "currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n",
      "Notably,\n",
      "SGLang v0.4.1\n",
      "fully supports running DeepSeek-V3 on both\n",
      "NVIDIA and AMD GPUs\n",
      ", making it a highly versatile and robust solution.\n",
      "Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n",
      "6.3 Inference with LMDeploy (recommended)\n",
      "LMDeploy\n",
      ", a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n",
      "For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n",
      "6.4 Inference with TRT-LLM (recommended)\n",
      "TensorRT-LLM\n",
      "now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3.\n",
      "6.5 Inference with vLLM (recommended)\n",
      "vLLM\n",
      "v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers\n",
      "pipeline parallelism\n",
      "allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the\n",
      "vLLM instructions\n",
      ". Please feel free to follow\n",
      "the enhancement plan\n",
      "as well.\n",
      "6.6 Recommended Inference Functionality with AMD GPUs\n",
      "In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the\n",
      "SGLang instructions\n",
      ".\n",
      "6.7 Recommended Inference Functionality with Huawei Ascend NPUs\n",
      "The\n",
      "MindIE\n",
      "framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the\n",
      "instructions here\n",
      ".\n",
      "7. License\n",
      "This code repository is licensed under\n",
      "the MIT License\n",
      ". The use of DeepSeek-V3 Base/Chat models is subject to\n",
      "the Model License\n",
      ". DeepSeek-V3 series (including Base and Chat) supports commercial use.\n",
      "8. Citation\n",
      "@misc{deepseekai2024deepseekv3technicalreport,\n",
      "      title={DeepSeek-V3 Technical Report}, \n",
      "      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},\n",
      "      year={2024},\n",
      "      eprint={2412.19437},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.CL},\n",
      "      url={https://arxiv.org/abs/2412.19437}, \n",
      "}\n",
      "9. Contact\n",
      "If you have any questions, please raise an issue or contact us at\n",
      "service@deepseek.com\n",
      ".\n",
      "> RSP * Outperforms leading open-source models and rivals closed models.\n",
      "* Utilizes a unique Mixture-of-Experts architecture with 671B parameters.\n",
      "* Features an economical training process requiring only 2.788M GPU hours.\n",
      "* Implements innovative Multi-head Latent Attention for efficient inference.\n",
      "* Introduces an auxiliary-loss-free load balancing strategy.\n",
      "* Achieves remarkable performance across extensive benchmarks, especially in math and code tasks.\n",
      "* Supports a context window length of up to 128K tokens.\n",
      "* Open-source with permissive licensing for commercial use.\n",
      "* Allows for easy local deployment using various optimized frameworks.\n",
      "* Pioneers advancements in FP8 mixed precision training for large-scale models.\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=PowerInfer/SmallThinker-3B-Preview\n",
      "> RSP {'model_id': 'PowerInfer/SmallThinker-3B-Preview', 'created_at': '12 December 2024 at 11:56:09 UTC', 'downloads': 6996, 'likes': 288, 'trending_score': 217, 'description': \"---\\ndatasets:\\n- PowerInfer/QWQ-LONGCOT-500K\\n- PowerInfer/LONGCOT-Refine-500K\\nbase_model:\\n- Qwen/Qwen2.5-3B-Instruct\\npipeline_tag: text-generation\\nlanguage:\\n- en\\nlibrary_name: transformers\\n---\\n# SmallThinker-3B-preview\\n\\nWe introduce **SmallThinker-3B-preview**, a new model fine-tuned from the [Qwen2.5-3b-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) model. \\n\\n## Benchmark Performance\\n\\n| Model | AIME24 | AMC23 | GAOKAO2024_I | GAOKAO2024_II | MMLU_STEM | AMPS_Hard | math_comp |\\n|---------|--------|-------|--------------|---------------|-----------|-----------|-----------|\\n| Qwen2.5-3B-Instruct | 6.67 | 45 | 50 | 35.8 | 59.8 | - | - |\\n| SmallThinker | 16.667 | 57.5 | 64.2 | 57.1 | 68.2 | 70 | 46.8 |\\n| GPT-4o | 9.3 | - | - | - | 64.2 | 57 | 50 |\\n\\nLimitation: Due to SmallThinker's current limitations in instruction following, for math_comp we adopt a more lenient evaluation method where only correct answers are required, without constraining responses to follow the specified AAAAA format.\\n\\nColab Link: [Colab](https://colab.research.google.com/drive/182q600at0sVw7uX0SXFp6bQI7pyjWXQ2?usp=sharing)\\n## Intended Use Cases\\n\\nSmallThinker is designed for the following use cases:\\n\\n1.  **Edge Deployment:** Its small size makes it ideal for deployment on resource-constrained devices.\\n2.  **Draft Model for QwQ-32B-Preview:** SmallThinker can serve as a fast and efficient draft model for the larger QwQ-32B-Preview model. From my test, in llama.cpp we can get 70% speedup (from 40 tokens/s to 70 tokens/s).\\n\\n## Training Details\\n\\nThe model was trained using 8 H100 GPUs with a global batch size of 16. The specific configuration is as follows:\\n\\nThe SFT (Supervised Fine-Tuning) process was conducted in two phases:\\n\\n1. First Phase:\\n   - Used only the PowerInfer/QWQ-LONGCOT-500K dataset\\n   - Trained for 1.5 epochs\\n```\\n### model\\nmodel_name_or_path: /home/syx/Qwen2.5-3B-Instruct\\n\\n### method\\nstage: sft\\ndo_train: true\\nfinetuning_type: full\\ndeepspeed: examples/deepspeed/ds_z3_config.json\\n\\n### dataset\\ndataset: o1-v2\\ntemplate: qwen\\nneat_packing: true\\ncutoff_len: 16384\\noverwrite_cache: true\\npreprocessing_num_workers: 16\\n\\n### output\\noutput_dir: saves/qwen2-01-qat/full/sft\\nlogging_steps: 1\\nsave_steps: 1000\\nplot_loss: true\\noverwrite_output_dir: true\\n```\\n2. Second Phase:\\n   - Combined training with PowerInfer/QWQ-LONGCOT-500K and PowerInfer/LONGCOT-Refine datasets\\n   - Continued training for 2 additional epochs\\n```\\n### model\\nmodel_name_or_path: saves/qwen2-01-qat/full/sft/checkpoint-24000\\n\\n### method\\nstage: sft\\ndo_train: true\\nfinetuning_type: full\\ndeepspeed: examples/deepspeed/ds_z3_config.json\\n\\n### dataset\\ndataset: o1-v2, o1-v3\\ntemplate: qwen\\nneat_packing: true\\ncutoff_len: 16384\\noverwrite_cache: true\\npreprocessing_num_workers: 16\\n\\n### output\\noutput_dir: saves/qwen2-01-qat/full/sft\\nlogging_steps: 1\\nsave_steps: 1000\\nplot_loss: true\\noverwrite_output_dir: true\\n```\\n\\n## Limitations & Disclaimer\\n\\nPlease be aware of the following limitations:\\n\\n*   **Language Limitation:** The model has only been trained on English-language datasets, hence its capabilities in other languages are still lacking.\\n*   **Limited Knowledge:** Due to limited SFT data and the model's relatively small scale, its reasoning capabilities are constrained by its knowledge base.\\n*   **Unpredictable Outputs:** The model may produce unexpected outputs due to its size and probabilistic generation paradigm. Users should exercise caution and validate the model's responses.\\n*   **Repetition Issue:** The model tends to repeat itself when answering high-difficulty questions. Please increase the `repetition_penalty` to mitigate this issue.\"}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=PowerInfer/SmallThinker-3B-Preview\n",
      "> RSP [{'title': 'SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency', 'url': 'https://pub.towardsai.net/smallthinker-3b-a-small-thinking-model-revolutionizing-ai-efficiency-f528cf7d6906', 'content': 'SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency | by Md Monsur ali | Jan, 2025 | Towards AI SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency How SmallThinker 3B delivers big results with minimal resources, making it the perfect choice for edge computing and mobile AI applications. Published in Towards AI With the introduction of the SmallThinker-3B Preview, PowerInfer seeks to address this balance by offering a compact yet potent model tailored for diverse applications. SmallThinker-3B-Preview is a compact yet powerful AI model developed by PowerInfer, designed to deliver high-quality inference while minimizing computational overhead. Hosted on Hugging Face, the model is easily accessible to developers, researchers, and AI enthusiasts. Follow Published in Towards AI ----------------------- Follow Follow 368 Followers Follow', 'score': 0.90771407, 'raw_content': None}, {'title': 'SmallThinker 3B Preview By PowerInfer: Benchmarks, Features and ...', 'url': 'https://llm.extractum.io/model/PowerInfer/SmallThinker-3B-Preview,6YvWQRdkYbb2o0HqvU7LTJ', 'content': 'SmallThinker 3B Preview by PowerInfer »\\xa0 All LLMs \\xa0»\\xa0 PowerInfer \\xa0»\\xa0 SmallThinker 3B Preview \\xa0\\xa0URL Share it on  Base model:qwen/qwen2.5-3b-ins... Model Card on HF 🤗: https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\\xa0 SmallThinker 3B Preview Benchmarks LLM Name    SmallThinker 3B Preview Repository 🤗    https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\\xa0 Base Model(s)   \\xa0\\xa0Qwen/Qwen2.5-3B-Instruct \\xa0\\xa0Qwen/Qwen2.5-3B-Instruct Model Size  3b Best Alternatives to SmallThinker 3B Preview Qwen2.5 3B Instruct 32K / 6.2\\u2009GB    558811  130 Calme 3.2 Instruct 3B   32K / 11\\u2009GB 3210    0 Qwen2.5 Coder 3B Instruct   32K / 6.2\\u2009GB    66636   24 Qwen2.5 3B RP Thinker V2    32K / 6.8\\u2009GB    98  1 Calme 3.1 Instruct 3B   32K / 11\\u2009GB 2677    2 \"73.2\") means that the model is better than PowerInfer/SmallThinker-3B-Preview. Rank the SmallThinker 3B Preview Capabilities', 'score': 0.8302782, 'raw_content': None}, {'title': 'Testing SmallThinker 3B Preview by PowerInfer - YouTube', 'url': 'https://www.youtube.com/watch?v=OVNnXQp_wNU', 'content': \"In this video, I tested SmallThinker 3B Preview by PowerInfer, an AI model which is currently trending on HuggingFace.It has a good concept, but it's very in\", 'score': 0.81097376, 'raw_content': None}, {'title': 'PowerInfer/SmallThinker-3B-Preview at main - Hugging Face', 'url': 'https://huggingface.co/PowerInfer/SmallThinker-3B-Preview/tree/main', 'content': 'PowerInfer/SmallThinker-3B-Preview at main *   config.json *   generation_config.json 16 Bytes LFSupdate weight 14 days ago 1.67 MB LFSupdate weight 14 days ago 4.96 GB LFSupdate weight 14 days ago *   model.safetensors.index.json 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago Pickle imports What is a pickle import? 1.06 kB LFSupdate weight 14 days ago *   tokenizer.json *   tokenizer_config.json *   trainer_state.json \"transformers.training_args.OptimizerNames\", \"transformers.trainer_utils.HubStrategy\", \"transformers.trainer_pt_utils.AcceleratorConfig\", \"transformers.trainer_utils.SchedulerType\", \"transformers.trainer_utils.IntervalStrategy\" 6.58 kB LFSupdate weight 14 days ago', 'score': 0.7343678, 'raw_content': None}, {'title': 'bartowski/SmallThinker-3B-Preview-GGUF - Hugging Face', 'url': 'https://huggingface.co/bartowski/SmallThinker-3B-Preview-GGUF', 'content': 'Filename Quant type File Size Split Description; SmallThinker-3B-Preview-f32.gguf: f32: 13.59GB: false: Full F32 weights. SmallThinker-3B-Preview-f16.gguf: f16', 'score': 0.572078, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=PowerInfer/SmallThinker-3B-Preview\n",
      "> RSP [{'title': 'PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU', 'url': 'https://arxiv.org/pdf/2312.12456', 'content': 'PowerInfer introduces a high-speed Large Language Model inference engine on a PC with a consumer-grade GPU, reducing memory and data transfer costs.', 'score': 0.41634628, 'raw_content': None}, {'title': 'PDF', 'url': 'https://arxiv.org/pdf/2312.12456v1.pdf', 'content': 'arXiv:2312.12456v1 [cs.LG] 16 Dec 2023 PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU Yixin Song, Zeyu Mi∗, Haotong Xie and Haibo Chen Institute of Parallel and Distributed Systems (IPADS),ShanghaiJiao Tong University', 'score': 0.3864398, 'raw_content': None}, {'title': '[2312.12456] PowerInfer: Fast Large Language Model Serving with a ...', 'url': 'http://export.arxiv.org/abs/2312.12456', 'content': 'cs > arXiv:2312.12456 cs.LG cs cs.OS This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. For the OPT-30B model, PowerInfer achieves performance comparable to that of a high-end server-grade A100 GPU, reaching 82% of its token generation rate on a single consumer-grade RTX 4090 GPU. Subjects:   Machine Learning (cs.LG); Operating Systems (cs.OS) Cite\\xa0as:    arXiv:2312.12456 [cs.LG] (or arXiv:2312.12456v2 [cs.LG] for this version)', 'score': 0.331672, 'raw_content': None}, {'title': 'PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU', 'url': 'https://arxiv.org/html/2312.12456v1', 'content': 'PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. However, these improvements are smaller compared to those on a higher-end PC (PC-High), primarily due to the 11GB GPU memory limitation of PC-Low. This limitation affects the number of neurons that can be allocated to the GPU, particularly for models with around 30B parameters or more, leading to a greater dependence on the CPU for processing a larger number of activated neurons.', 'score': 0.3156732, 'raw_content': None}, {'title': 'PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU', 'url': 'https://arxiv.org/pdf/2312.12456v2', 'content': 'PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU Yixin Song, Zeyu Mi, Haotong Xie and Haibo Chen Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University', 'score': 0.27899086, 'raw_content': None}, {'title': '[2312.12456] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU', 'url': 'https://arxiv.org/abs/2312.12456', 'content': 'cs arXiv:2312.12456 Help | Advanced Search arXiv author ID Help pages This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. Subjects:   Machine Learning (cs.LG); Operating Systems (cs.OS) Cite as:    arXiv:2312.12456 [cs.LG] (or arXiv:2312.12456v1 [cs.LG] for this version) From: Zeyu Mi [view email] Access Paper: cs.LG cs Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? arXiv Operational Status ', 'score': 0.25215882, 'raw_content': None}, {'title': 'PowerInfer-2 : Fast Large Language Model Inference on a Smartphone', 'url': 'https://arxiv.org/html/2406.06282v3', 'content': 'In contrast, high-end PCs’ discrete GPUs efficiently handle both dense and sparse matrix operations compared to CPUs. This hardware limitation significantly impacts the effectiveness of PowerInfer and LLMFlash’s sparse computation approaches on smartphones, as sparse computations are forced to run on CPUs. Such CPU-only execution not only underutilizes the NPU’s computational capabilities but also fails to fully exploit the high memory bandwidth available in smartphones’ unified memory architecture (UMA). (a) The prefill phase uses an NPU-centric workflow that leverages NPU for computation; (b) The decoding phase employs a CPU-NPU hybrid workflow for FFN computation where NPU handles dense computations for hot neurons while CPU cores process sparse computations for cold neurons, with their processing ratio automatically adjusting to match the dynamic sparsity patterns caused by varying batch sizes.', 'score': 0.21727978, 'raw_content': None}, {'title': 'PowerInfer-2 : Fast Large Language Model Inference on a Smartphone', 'url': 'https://arxiv.org/html/2406.06282', 'content': 'In contrast, high-end PCs’ discrete GPUs efficiently handle both dense and sparse matrix operations compared to CPUs. This hardware limitation significantly impacts the effectiveness of PowerInfer and LLMFlash’s sparse computation approaches on smartphones, as sparse computations are forced to run on CPUs. Such CPU-only execution not only underutilizes the NPU’s computational capabilities but also fails to fully exploit the high memory bandwidth available in smartphones’ unified memory architecture (UMA). (a) The prefill phase uses an NPU-centric workflow that leverages NPU for computation; (b) The decoding phase employs a CPU-NPU hybrid workflow for FFN computation where NPU handles dense computations for hot neurons while CPU cores process sparse computations for cold neurons, with their processing ratio automatically adjusting to match the dynamic sparsity patterns caused by varying batch sizes.', 'score': 0.21727978, 'raw_content': None}, {'title': 'PowerInfer-2: Fast Large Language Model Inference on a Smartphone', 'url': 'https://arxiv.org/pdf/2406.06282v3', 'content': 'PowerInfer-2: Fast Large Language Model Inference on a Smartphone Zhenliang Xue*, Yixin Song*, Zeyu Mi , Xinrui Zheng, Yubin Xia, and Haibo Chen', 'score': 0.20032328, 'raw_content': None}, {'title': 'PowerInfer-2: Fast Large Language Model Inference on a Smartphone', 'url': 'https://arxiv.org/pdf/2406.06282v1', 'content': 'PowerInfer-2: Fast Large Language Model Inference on a Smartphone Zhenliang Xue*, Yixin Song*, Zeyu Mi , Le Chen, Yubin Xia, and Haibo Chen Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University', 'score': 0.1839162, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2406.06282\n",
      "> RSP [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](http://arxiv.org/abs/2406.06282v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2312.12456\n",
      "> RSP [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](http://arxiv.org/abs/2312.12456v2)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency', 'url': 'https://pub.towardsai.net/smallthinker-3b-a-small-thinking-model-revolutionizing-ai-efficiency-f528cf7d6906', 'content': 'SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency | by Md Monsur ali | Jan, 2025 | Towards AI SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency How SmallThinker 3B delivers big results with minimal resources, making it the perfect choice for edge computing and mobile AI applications. Published in Towards AI With the introduction of the SmallThinker-3B Preview, PowerInfer seeks to address this balance by offering a compact yet potent model tailored for diverse applications. SmallThinker-3B-Preview is a compact yet powerful AI model developed by PowerInfer, designed to deliver high-quality inference while minimizing computational overhead. Hosted on Hugging Face, the model is easily accessible to developers, researchers, and AI enthusiasts. Follow Published in Towards AI ----------------------- Follow Follow 368 Followers Follow', 'score': 0.90771407, 'raw_content': None}, {'title': 'SmallThinker 3B Preview By PowerInfer: Benchmarks, Features and ...', 'url': 'https://llm.extractum.io/model/PowerInfer/SmallThinker-3B-Preview,6YvWQRdkYbb2o0HqvU7LTJ', 'content': 'SmallThinker 3B Preview by PowerInfer »\\xa0 All LLMs \\xa0»\\xa0 PowerInfer \\xa0»\\xa0 SmallThinker 3B Preview \\xa0\\xa0URL Share it on  Base model:qwen/qwen2.5-3b-ins... Model Card on HF 🤗: https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\\xa0 SmallThinker 3B Preview Benchmarks LLM Name    SmallThinker 3B Preview Repository 🤗    https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\\xa0 Base Model(s)   \\xa0\\xa0Qwen/Qwen2.5-3B-Instruct \\xa0\\xa0Qwen/Qwen2.5-3B-Instruct Model Size  3b Best Alternatives to SmallThinker 3B Preview Qwen2.5 3B Instruct 32K / 6.2\\u2009GB    558811  130 Calme 3.2 Instruct 3B   32K / 11\\u2009GB 3210    0 Qwen2.5 Coder 3B Instruct   32K / 6.2\\u2009GB    66636   24 Qwen2.5 3B RP Thinker V2    32K / 6.8\\u2009GB    98  1 Calme 3.1 Instruct 3B   32K / 11\\u2009GB 2677    2 \"73.2\") means that the model is better than PowerInfer/SmallThinker-3B-Preview. Rank the SmallThinker 3B Preview Capabilities', 'score': 0.8302782, 'raw_content': None}, {'title': 'Testing SmallThinker 3B Preview by PowerInfer - YouTube', 'url': 'https://www.youtube.com/watch?v=OVNnXQp_wNU', 'content': \"In this video, I tested SmallThinker 3B Preview by PowerInfer, an AI model which is currently trending on HuggingFace.It has a good concept, but it's very in\", 'score': 0.81097376, 'raw_content': None}, {'title': 'PowerInfer/SmallThinker-3B-Preview at main - Hugging Face', 'url': 'https://huggingface.co/PowerInfer/SmallThinker-3B-Preview/tree/main', 'content': 'PowerInfer/SmallThinker-3B-Preview at main *   config.json *   generation_config.json 16 Bytes LFSupdate weight 14 days ago 1.67 MB LFSupdate weight 14 days ago 4.96 GB LFSupdate weight 14 days ago *   model.safetensors.index.json 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago 16 kB LFSupdate weight 14 days ago Pickle imports What is a pickle import? 1.06 kB LFSupdate weight 14 days ago *   tokenizer.json *   tokenizer_config.json *   trainer_state.json \"transformers.training_args.OptimizerNames\", \"transformers.trainer_utils.HubStrategy\", \"transformers.trainer_pt_utils.AcceleratorConfig\", \"transformers.trainer_utils.SchedulerType\", \"transformers.trainer_utils.IntervalStrategy\" 6.58 kB LFSupdate weight 14 days ago', 'score': 0.7343678, 'raw_content': None}, {'title': 'bartowski/SmallThinker-3B-Preview-GGUF - Hugging Face', 'url': 'https://huggingface.co/bartowski/SmallThinker-3B-Preview-GGUF', 'content': 'Filename Quant type File Size Split Description; SmallThinker-3B-Preview-f32.gguf: f32: 13.59GB: false: Full F32 weights. SmallThinker-3B-Preview-f16.gguf: f16', 'score': 0.572078, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "datasets:\n",
      "- PowerInfer/QWQ-LONGCOT-500K\n",
      "- PowerInfer/LONGCOT-Refine-500K\n",
      "base_model:\n",
      "- Qwen/Qwen2.5-3B-Instruct\n",
      "pipeline_tag: text-generation\n",
      "language:\n",
      "- en\n",
      "library_name: transformers\n",
      "SmallThinker-3B-preview\n",
      "We introduce\n",
      "SmallThinker-3B-preview\n",
      ", a new model fine-tuned from the\n",
      "Qwen2.5-3b-Instruct\n",
      "model.\n",
      "Benchmark Performance\n",
      "| Model | AIME24 | AMC23 | GAOKAO2024_I | GAOKAO2024_II | MMLU_STEM | AMPS_Hard | math_comp |\n",
      "|---------|--------|-------|--------------|---------------|-----------|-----------|-----------|\n",
      "| Qwen2.5-3B-Instruct | 6.67 | 45 | 50 | 35.8 | 59.8 | - | - |\n",
      "| SmallThinker | 16.667 | 57.5 | 64.2 | 57.1 | 68.2 | 70 | 46.8 |\n",
      "| GPT-4o | 9.3 | - | - | - | 64.2 | 57 | 50 |\n",
      "Limitation: Due to SmallThinker's current limitations in instruction following, for math_comp we adopt a more lenient evaluation method where only correct answers are required, without constraining responses to follow the specified AAAAA format.\n",
      "Colab Link:\n",
      "Colab\n",
      "Intended Use Cases\n",
      "SmallThinker is designed for the following use cases:\n",
      "Edge Deployment:\n",
      "Its small size makes it ideal for deployment on resource-constrained devices.\n",
      "Draft Model for QwQ-32B-Preview:\n",
      "SmallThinker can serve as a fast and efficient draft model for the larger QwQ-32B-Preview model. From my test, in llama.cpp we can get 70% speedup (from 40 tokens/s to 70 tokens/s).\n",
      "Training Details\n",
      "The model was trained using 8 H100 GPUs with a global batch size of 16. The specific configuration is as follows:\n",
      "The SFT (Supervised Fine-Tuning) process was conducted in two phases:\n",
      "First Phase:\n",
      "Used only the PowerInfer/QWQ-LONGCOT-500K dataset\n",
      "Trained for 1.5 epochs\n",
      "```\n",
      "model\n",
      "model_name_or_path: /home/syx/Qwen2.5-3B-Instruct\n",
      "method\n",
      "stage: sft\n",
      "do_train: true\n",
      "finetuning_type: full\n",
      "deepspeed: examples/deepspeed/ds_z3_config.json\n",
      "dataset\n",
      "dataset: o1-v2\n",
      "template: qwen\n",
      "neat_packing: true\n",
      "cutoff_len: 16384\n",
      "overwrite_cache: true\n",
      "preprocessing_num_workers: 16\n",
      "output\n",
      "output_dir: saves/qwen2-01-qat/full/sft\n",
      "logging_steps: 1\n",
      "save_steps: 1000\n",
      "plot_loss: true\n",
      "overwrite_output_dir: true\n",
      "2. Second Phase:\n",
      "   - Combined training with PowerInfer/QWQ-LONGCOT-500K and PowerInfer/LONGCOT-Refine datasets\n",
      "   - Continued training for 2 additional epochs\n",
      "model\n",
      "model_name_or_path: saves/qwen2-01-qat/full/sft/checkpoint-24000\n",
      "method\n",
      "stage: sft\n",
      "do_train: true\n",
      "finetuning_type: full\n",
      "deepspeed: examples/deepspeed/ds_z3_config.json\n",
      "dataset\n",
      "dataset: o1-v2, o1-v3\n",
      "template: qwen\n",
      "neat_packing: true\n",
      "cutoff_len: 16384\n",
      "overwrite_cache: true\n",
      "preprocessing_num_workers: 16\n",
      "output\n",
      "output_dir: saves/qwen2-01-qat/full/sft\n",
      "logging_steps: 1\n",
      "save_steps: 1000\n",
      "plot_loss: true\n",
      "overwrite_output_dir: true\n",
      "```\n",
      "Limitations & Disclaimer\n",
      "Please be aware of the following limitations:\n",
      "Language Limitation:\n",
      "The model has only been trained on English-language datasets, hence its capabilities in other languages are still lacking.\n",
      "Limited Knowledge:\n",
      "Due to limited SFT data and the model's relatively small scale, its reasoning capabilities are constrained by its knowledge base.\n",
      "Unpredictable Outputs:\n",
      "The model may produce unexpected outputs due to its size and probabilistic generation paradigm. Users should exercise caution and validate the model's responses.\n",
      "Repetition Issue:\n",
      "The model tends to repeat itself when answering high-difficulty questions. Please increase the\n",
      "repetition_penalty\n",
      "to mitigate this issue.\n",
      "> RSP * Compact size ideal for resource-constrained devices\n",
      "* High-quality inference with minimal computational overhead\n",
      "* Designed for edge deployment and mobile applications\n",
      "* Accelerated performance compared to larger models (up to 70% speedup)\n",
      "* Fine-tuned on specialized datasets for improved accuracy\n",
      "* Strong benchmark performance surpassing some larger models\n",
      "* Versatile applications as a draft model for larger models\n",
      "* Trained with advanced techniques like Supervised Fine-Tuning (SFT)\n",
      "* Accessible on platforms like Hugging Face for easy integration\n",
      "* Holds potential for further enhancement with more extensive datasets and training\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=deepseek-ai/DeepSeek-V3-Base\n",
      "> RSP {'model_id': 'deepseek-ai/DeepSeek-V3-Base', 'created_at': '25 December 2024 at 12:52:06 UTC', 'downloads': 8663, 'likes': 1180, 'trending_score': 188, 'description': '<!-- markdownlint-disable first-line-h1 -->\\n<!-- markdownlint-disable html -->\\n<!-- markdownlint-disable no-duplicate-header -->\\n\\n<div align=\"center\">\\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\\n</div>\\n<hr>\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n<div align=\"center\" style=\"line-height: 1;\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\\n  </a>\\n</div>\\n\\n\\n<p align=\"center\">\\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>👁️</a>\\n</p>\\n\\n\\n## 1. Introduction\\n\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \\nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \\nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \\nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \\nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\\nIn addition, its training process is remarkably stable. \\nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/benchmark.png\">\\n</p>\\n\\n## 2. Model Summary\\n\\n---\\n\\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\\n\\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \\n    It can also be used for speculative decoding for inference acceleration. \\n\\n---\\n\\n**Pre-Training: Towards Ultimate Training Efficiency**\\n\\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \\n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \\n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \\n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\\n\\n---\\n\\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\\n\\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\\n\\n---\\n\\n\\n## 3. Model Downloads\\n\\n<div align=\"center\">\\n\\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\\n| :------------: | :------------: | :------------: | :------------: | :------------: |\\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\\n\\n</div>\\n\\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\\n\\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\\n\\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\\n\\n## 4. Evaluation Results\\n### Base Model\\n#### Standard Benchmarks\\n\\n<div align=\"center\">\\n\\n\\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\\n|---|-------------------|----------|--------|-------------|---------------|---------|\\n| | Architecture | - | MoE | Dense | Dense | MoE |\\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\\n| | # Total Params | - | 236B | 72B | 405B | 671B |\\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\\n\\n</div>\\n\\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\\nFor more evaluation details, please check our paper. \\n\\n#### Context Window\\n<p align=\"center\">\\n  <img width=\"80%\" src=\"figures/niah.png\">\\n</p>\\n\\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \\n\\n### Chat Model\\n#### Standard Benchmarks (Models larger than 67B)\\n<div align=\"center\">\\n\\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\\n\\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\\n\\n</div>\\n\\n\\n####  Open Ended Generation Evaluation\\n\\n<div align=\"center\">\\n\\n\\n\\n| Model | Arena-Hard | AlpacaEval 2.0 |\\n|-------|------------|----------------|\\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\\n| LLaMA-3.1 405B | 69.3 | 40.5 |\\n| GPT-4o-0513 | 80.4 | 51.1 |\\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\\n| DeepSeek-V3 | **85.5** | **70.0** |\\n\\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\\n</div>\\n\\n\\n## 5. Chat Website & API Platform\\nYou can chat with DeepSeek-V3 on DeepSeek\\'s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\\n\\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\\n\\n## 6. How to Run Locally\\n\\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\\n\\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\\n\\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\\n\\nHere is an example of converting FP8 weights to BF16:\\n\\n```shell\\ncd inference\\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\\n```\\n\\n**NOTE: Huggingface\\'s Transformers has not been directly supported yet.**\\n\\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\\n\\n#### Model Weights & Demo Code Preparation\\n\\nFirst, clone our DeepSeek-V3 GitHub repository:\\n\\n```shell\\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\\n```\\n\\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\\n\\n```shell\\ncd DeepSeek-V3/inference\\npip install -r requirements.txt\\n```\\n\\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\\n\\n#### Model Weights Conversion\\n\\nConvert HuggingFace model weights to a specific format:\\n\\n```shell\\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\\n```\\n\\n#### Run\\n\\nThen you can chat with DeepSeek-V3:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\\n```\\n\\nOr batch inference on a given file:\\n\\n```shell\\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\\n```\\n\\n### 6.2 Inference with SGLang (recommended)\\n\\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\\n\\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\\n\\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\\n\\n### 6.3 Inference with LMDeploy (recommended)\\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\\n\\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\\n\\n\\n### 6.4 Inference with TRT-LLM (recommended)\\n\\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \\n\\n### 6.5 Inference with vLLM (recommended)\\n\\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\\n\\n### 6.6 Recommended Inference Functionality with AMD GPUs\\n\\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\\n\\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\\n\\n\\n## 7. License\\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\\n\\n## 8. Citation\\n```\\n@misc{deepseekai2024deepseekv3technicalreport,\\n      title={DeepSeek-V3 Technical Report}, \\n      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},\\n      year={2024},\\n      eprint={2412.19437},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.19437}, \\n}\\n```\\n\\n## 9. Contact\\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\\n'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=deepseek-ai/DeepSeek-V3-Base\n",
      "> RSP [{'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3-Base achieves the best performance on most benchmarks, especially on math and code tasks. 4.4.2.Evaluation Results. In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B', 'score': 0.7463059, 'raw_content': None}, {'title': 'Paper page - DeepSeek-V3 Technical Report - Hugging Face', 'url': 'https://huggingface.co/papers/2412.19437', 'content': 'Paper page - DeepSeek-V3 Technical Report DeepSeek-V3 Technical Report Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. Comment· Sign up or log in to comment Models citing this paper 3 #### deepseek-ai/DeepSeek-V3-Base Updated 4 days ago • 6.64k • 1.1k #### deepseek-ai/DeepSeek-V3 Updated 4 days ago • 45.5k • 988 #### bullerwins/DeepSeek-V3-split Updated about 9 hours ago Datasets citing this paper 0 Spaces citing this paper 7 #### Deepseek Papers Collection Deepseek papers collection • 14 items • Updated 4 days ago • 8 #### LLM Technical Report Collection 22 items • Updated 3 days ago • 2 #### Papers Collection 40 items • Updated 1 day ago Models Datasets Spaces Pricing Docs', 'score': 0.73868835, 'raw_content': None}, {'title': 'deepseek-ai/DeepSeek-V3-Base', 'url': 'https://simonwillison.net/2024/Dec/25/deepseek-v3/', 'content': 'deepseek-ai/DeepSeek-V3-Base deepseek-ai/DeepSeek-V3-Base (via) No model card or announcement yet, but this new model release from Chinese AI lab DeepSeek (an arm of Chinese hedge fund High-Flyer) looks very significant. The new model is apparently available to some people via both chat.deepseek.com and the DeepSeek API as part of a staged rollout. I never know if I can believe models or not (the first time I asked \"what model are you?\" it claimed to be \"based on OpenAI\\'s GPT-4 architecture\"), but I just got this result using LLM and the llm-deepseek plugin: llm -m deepseek-chat \\'what deepseek model are you?\\' Trying out QvQ - Qwen\\'s new visual reasoning model - 24th December 2024', 'score': 0.7190094, 'raw_content': None}, {'title': 'DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat', 'url': 'https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/', 'content': 'DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch | VentureBeat DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B.', 'score': 0.7161595, 'raw_content': None}, {'title': 'deepseek-ai/DeepSeek-V3 - GitHub', 'url': 'https://github.com/deepseek-ai/DeepSeek-V3', 'content': 'At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights. SGLang: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes. AMD GPU: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes. LMDeploy, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. The use of DeepSeek-V3 Base/Chat models is subject to the Model License.', 'score': 0.68344116, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=deepseek-ai/DeepSeek-V3-Base\n",
      "> RSP [{'title': '[2412.19437] DeepSeek-V3 Technical Report - export.arxiv.org', 'url': 'http://export.arxiv.org/abs/2412.19437', 'content': 'cs > arXiv:2412.19437 cs.CL cs cs.AI We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite\\xa0as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) Which authors of this paper are endorsers?', 'score': 0.6055431, 'raw_content': None}, {'title': '[2412.19437] DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/abs/2412.19437', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. arXiv:2412.19437 arXiv author ID DeepSeek-V3 Technical Report We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.19437 Bibliographic and Citation Tools Connected Papers Toggle\", 'score': 0.5097406, 'raw_content': None}, {'title': '[2412.19437v1] DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/abs/2412.19437v1', 'content': \"Change to arXiv's privacy policy The arXiv Privacy Policy has changed. arXiv:2412.19437v1 arXiv author ID DeepSeek-V3 Technical Report We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. Cite as:    arXiv:2412.19437 [cs.CL] (or arXiv:2412.19437v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2412.19437 Bibliographic and Citation Tools Connected Papers Toggle\", 'score': 0.50388235, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total ... on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the', 'score': 0.49392182, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/html/2412.19437v1', 'content': 'Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention\\xa0(MLA)\\xa0(DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE\\xa0(Dai et\\xa0al., 2024) for cost-effective training. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy\\xa0(Wang et\\xa0al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models.', 'score': 0.43842506, 'raw_content': None}, {'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/html/2412.19437', 'content': 'Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention\\xa0(MLA)\\xa0(DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE\\xa0(Dai et\\xa0al., 2024) for cost-effective training. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy\\xa0(Wang et\\xa0al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models.', 'score': 0.43842506, 'raw_content': None}, {'title': 'DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open ...', 'url': 'https://arxiv.org/abs/2402.03300', 'content': 'cs arXiv:2402.03300 Help | Advanced Search arXiv author ID DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2402.03300 [cs.CL] (or arXiv:2402.03300v3 [cs.CL] for this version) From: Zhihong Shao [view email] Access Paper: cs.CL cs cs.AI References & Citations Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? Help', 'score': 0.31459826, 'raw_content': None}, {'title': 'DeepSeek LLM: Scaling Open-Source Language Models with Longtermism', 'url': 'https://arxiv.org/abs/2401.02954', 'content': 'cs arXiv:2401.02954 arXiv author ID DeepSeek LLM: Scaling Open-Source Language Models with Longtermism The rapid development of open-source large language models (LLMs) has been truly remarkable. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2401.02954 [cs.CL] (or arXiv:2401.02954v1 [cs.CL] for this version) cs.CL cs cs.AI Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle', 'score': 0.30731693, 'raw_content': None}, {'title': 'DeepSeek-VL: Towards Real-World Vision-Language Understanding', 'url': 'http://export.arxiv.org/abs/2403.05525', 'content': 'cs > arXiv:2403.05525 cs.AI cs DeepSeek-VL: Towards Real-World Vision-Language Understanding We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a vision-language chatbot in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language benchmarks at the same model size while maintaining robust performance on language-centric benchmarks. Subjects:   Artificial Intelligence (cs.AI) Cite\\xa0as:    arXiv:2403.05525 [cs.AI] (or arXiv:2403.05525v2 [cs.AI] for this version) Which authors of this paper are endorsers?', 'score': 0.28048873, 'raw_content': None}, {'title': 'DeepSeek-VL2: Mixture-of-Experts Vision-Language Models', 'url': 'https://web3.arxiv.org/pdf/2412.10302', 'content': 'DeepSeek-AI Abstract We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two ... The pre-trained SigLIP operates at a base resolution of 384 ×384. To', 'score': 0.27226803, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.10302\n",
      "> RSP [DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding](http://arxiv.org/abs/2412.10302v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2401.02954\n",
      "> RSP [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/abs/2401.02954v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2403.05525\n",
      "> RSP [DeepSeek-VL: Towards Real-World Vision-Language Understanding](http://arxiv.org/abs/2403.05525v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2412.19437\n",
      "> RSP [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2402.03300\n",
      "> RSP [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/abs/2402.03300v3)\n",
      "< REQ Calling OpenAI gpt-4o-mini model system_prompt=Given the information below, summarize the large machine learning model competitive characteristics (how it differentiates from any other model) using no more than 10 single-level bullets. Only output these bullets, not any extra text.\n",
      "\n",
      "Example of the required output:\n",
      "* Characteristic 1\n",
      "* Characteristic 2\n",
      "* Characteristic 3\n",
      "..., message=# WEB SEARCH RESULTS\n",
      "[{'title': 'DeepSeek-V3 Technical Report - arXiv.org', 'url': 'https://arxiv.org/pdf/2412.19437', 'content': 'DeepSeek-V3-Base achieves the best performance on most benchmarks, especially on math and code tasks. 4.4.2.Evaluation Results. In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B', 'score': 0.7463059, 'raw_content': None}, {'title': 'Paper page - DeepSeek-V3 Technical Report - Hugging Face', 'url': 'https://huggingface.co/papers/2412.19437', 'content': 'Paper page - DeepSeek-V3 Technical Report DeepSeek-V3 Technical Report Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. Comment· Sign up or log in to comment Models citing this paper 3 #### deepseek-ai/DeepSeek-V3-Base Updated 4 days ago • 6.64k • 1.1k #### deepseek-ai/DeepSeek-V3 Updated 4 days ago • 45.5k • 988 #### bullerwins/DeepSeek-V3-split Updated about 9 hours ago Datasets citing this paper 0 Spaces citing this paper 7 #### Deepseek Papers Collection Deepseek papers collection • 14 items • Updated 4 days ago • 8 #### LLM Technical Report Collection 22 items • Updated 3 days ago • 2 #### Papers Collection 40 items • Updated 1 day ago Models Datasets Spaces Pricing Docs', 'score': 0.73868835, 'raw_content': None}, {'title': 'deepseek-ai/DeepSeek-V3-Base', 'url': 'https://simonwillison.net/2024/Dec/25/deepseek-v3/', 'content': 'deepseek-ai/DeepSeek-V3-Base deepseek-ai/DeepSeek-V3-Base (via) No model card or announcement yet, but this new model release from Chinese AI lab DeepSeek (an arm of Chinese hedge fund High-Flyer) looks very significant. The new model is apparently available to some people via both chat.deepseek.com and the DeepSeek API as part of a staged rollout. I never know if I can believe models or not (the first time I asked \"what model are you?\" it claimed to be \"based on OpenAI\\'s GPT-4 architecture\"), but I just got this result using LLM and the llm-deepseek plugin: llm -m deepseek-chat \\'what deepseek model are you?\\' Trying out QvQ - Qwen\\'s new visual reasoning model - 24th December 2024', 'score': 0.7190094, 'raw_content': None}, {'title': 'DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat', 'url': 'https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/', 'content': 'DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch | VentureBeat DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch Chinese AI startup DeepSeek, known for challenging leading AI vendors with its innovative open-source technologies, today released a new ultra-large model: DeepSeek-V3. According to benchmarks shared by DeepSeek, the offering is already topping the charts, outperforming leading open-source models, including Meta’s Llama 3.1-405B, and closely matching the performance of closed models from Anthropic and OpenAI. Despite the economical training, DeepSeek-V3 has emerged as the strongest open-source model in the market. The company ran multiple benchmarks to compare the performance of the AI and noted that it convincingly outperforms leading open models, including Llama-3.1-405B and Qwen 2.5-72B.', 'score': 0.7161595, 'raw_content': None}, {'title': 'deepseek-ai/DeepSeek-V3 - GitHub', 'url': 'https://github.com/deepseek-ai/DeepSeek-V3', 'content': 'At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights. SGLang: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes. AMD GPU: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes. LMDeploy, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. The use of DeepSeek-V3 Base/Chat models is subject to the Model License.', 'score': 0.68344116, 'raw_content': None}]\n",
      "\n",
      "# README FILE (MODEL CARD)\n",
      "Paper Link\n",
      "👁️\n",
      "1. Introduction\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \n",
      "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \n",
      "Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \n",
      "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \n",
      "Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\n",
      "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\n",
      "In addition, its training process is remarkably stable. \n",
      "Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "2. Model Summary\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n",
      "We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n",
      "    It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.\n",
      "This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.\n",
      "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n",
      "3. Model Downloads\n",
      "| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n",
      "| :------------: | :------------: | :------------: | :------------: | :------------: |\n",
      "| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n",
      "| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n",
      "NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.\n",
      "To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6:\n",
      "How_to Run_Locally\n",
      ".\n",
      "For developers looking to dive deeper, we recommend exploring\n",
      "README_WEIGHTS.md\n",
      "for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n",
      "4. Evaluation Results\n",
      "Base Model\n",
      "Standard Benchmarks\n",
      "|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n",
      "|---|-------------------|----------|--------|-------------|---------------|---------|\n",
      "| | Architecture | - | MoE | Dense | Dense | MoE |\n",
      "| | # Activated Params | - | 21B | 72B | 405B | 37B |\n",
      "| | # Total Params | - | 236B | 72B | 405B | 671B |\n",
      "| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n",
      "| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n",
      "| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n",
      "| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n",
      "| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n",
      "| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n",
      "| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n",
      "| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n",
      "| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n",
      "| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n",
      "| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n",
      "| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n",
      "| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n",
      "| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n",
      "| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n",
      "| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n",
      "| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n",
      "| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n",
      "| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n",
      "| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n",
      "| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n",
      "| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n",
      "| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n",
      "| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n",
      "| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n",
      "| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n",
      "| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n",
      "| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n",
      "| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n",
      "| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n",
      "| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n",
      "| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n",
      "Note: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\n",
      "For more evaluation details, please check our paper.\n",
      "Context Window\n",
      "Evaluation results on the\n",
      "Needle In A Haystack\n",
      "(NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to\n",
      "128K\n",
      ".\n",
      "Chat Model\n",
      "Standard Benchmarks (Models larger than 67B)\n",
      "| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n",
      "|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n",
      "| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n",
      "| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n",
      "| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n",
      "| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n",
      "| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n",
      "| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n",
      "| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n",
      "| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n",
      "| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n",
      "| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n",
      "| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n",
      "| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n",
      "| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n",
      "| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n",
      "| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n",
      "| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n",
      "| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n",
      "| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n",
      "| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n",
      "| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n",
      "| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n",
      "| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n",
      "| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n",
      "| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n",
      "| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n",
      "\n",
      "Note: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n",
      "Open Ended Generation Evaluation\n",
      "| Model | Arena-Hard | AlpacaEval 2.0 |\n",
      "|-------|------------|----------------|\n",
      "| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n",
      "| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n",
      "| LLaMA-3.1 405B | 69.3 | 40.5 |\n",
      "| GPT-4o-0513 | 80.4 | 51.1 |\n",
      "| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n",
      "| DeepSeek-V3 | **85.5** | **70.0** |\n",
      "\n",
      "Note: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n",
      "5. Chat Website & API Platform\n",
      "You can chat with DeepSeek-V3 on DeepSeek's official website:\n",
      "chat.deepseek.com\n",
      "We also provide OpenAI-Compatible API at DeepSeek Platform:\n",
      "platform.deepseek.com\n",
      "6. How to Run Locally\n",
      "DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n",
      "DeepSeek-Infer Demo\n",
      ": We provide a simple and lightweight demo for FP8 and BF16 inference.\n",
      "SGLang\n",
      ": Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n",
      "LMDeploy\n",
      ": Enables efficient FP8 and BF16 inference for local and cloud deployment.\n",
      "TensorRT-LLM\n",
      ": Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n",
      "vLLM\n",
      ": Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n",
      "AMD GPU\n",
      ": Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n",
      "Huawei Ascend NPU\n",
      ": Supports running DeepSeek-V3 on Huawei Ascend devices.\n",
      "Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n",
      "Here is an example of converting FP8 weights to BF16:\n",
      "shell\n",
      "cd inference\n",
      "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n",
      "NOTE: Huggingface's Transformers has not been directly supported yet.\n",
      "6.1 Inference with DeepSeek-Infer Demo (example only)\n",
      "Model Weights & Demo Code Preparation\n",
      "First, clone our DeepSeek-V3 GitHub repository:\n",
      "shell\n",
      "git clone https://github.com/deepseek-ai/DeepSeek-V3.git\n",
      "Navigate to the\n",
      "inference\n",
      "folder and install dependencies listed in\n",
      "requirements.txt\n",
      ".\n",
      "shell\n",
      "cd DeepSeek-V3/inference\n",
      "pip install -r requirements.txt\n",
      "Download the model weights from HuggingFace, and put them into\n",
      "/path/to/DeepSeek-V3\n",
      "folder.\n",
      "Model Weights Conversion\n",
      "Convert HuggingFace model weights to a specific format:\n",
      "shell\n",
      "python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n",
      "Run\n",
      "Then you can chat with DeepSeek-V3:\n",
      "shell\n",
      "torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n",
      "Or batch inference on a given file:\n",
      "shell\n",
      "torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n",
      "6.2 Inference with SGLang (recommended)\n",
      "SGLang\n",
      "currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n",
      "Notably,\n",
      "SGLang v0.4.1\n",
      "fully supports running DeepSeek-V3 on both\n",
      "NVIDIA and AMD GPUs\n",
      ", making it a highly versatile and robust solution.\n",
      "Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n",
      "6.3 Inference with LMDeploy (recommended)\n",
      "LMDeploy\n",
      ", a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n",
      "For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n",
      "6.4 Inference with TRT-LLM (recommended)\n",
      "TensorRT-LLM\n",
      "now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3.\n",
      "6.5 Inference with vLLM (recommended)\n",
      "vLLM\n",
      "v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers\n",
      "pipeline parallelism\n",
      "allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the\n",
      "vLLM instructions\n",
      ". Please feel free to follow\n",
      "the enhancement plan\n",
      "as well.\n",
      "6.6 Recommended Inference Functionality with AMD GPUs\n",
      "In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the\n",
      "SGLang instructions\n",
      ".\n",
      "6.7 Recommended Inference Functionality with Huawei Ascend NPUs\n",
      "The\n",
      "MindIE\n",
      "framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the\n",
      "instructions here\n",
      ".\n",
      "7. License\n",
      "This code repository is licensed under\n",
      "the MIT License\n",
      ". The use of DeepSeek-V3 Base/Chat models is subject to\n",
      "the Model License\n",
      ". DeepSeek-V3 series (including Base and Chat) supports commercial use.\n",
      "8. Citation\n",
      "@misc{deepseekai2024deepseekv3technicalreport,\n",
      "      title={DeepSeek-V3 Technical Report}, \n",
      "      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},\n",
      "      year={2024},\n",
      "      eprint={2412.19437},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.CL},\n",
      "      url={https://arxiv.org/abs/2412.19437}, \n",
      "}\n",
      "9. Contact\n",
      "If you have any questions, please raise an issue or contact us at\n",
      "service@deepseek.com\n",
      ".\n",
      "> RSP * Outperforms existing leading open-source models in benchmarks.\n",
      "* Strong performance on specialized tasks like math and code generation.\n",
      "* Efficient training requiring only 2.788M H800 GPU hours.\n",
      "* Utilizes a Mixture-of-Experts (MoE) architecture with 671B total parameters.\n",
      "* Features innovative Multi-Token Prediction (MTP) training objective.\n",
      "* Pioneers an auxiliary-loss-free load balancing strategy.\n",
      "* Demonstrates stability throughout the training process without significant loss spikes.\n",
      "* Offers competitive performance against closed-source models from major companies.\n",
      "* Supports both FP8 and BF16 inference modes for versatile deployment.\n",
      "* Capable of processing up to 128K context length for extensive input handling.\n",
      "< REQ Retrieving model information from HuggingFace Hub model_id=black-forest-labs/FLUX.1-dev\n",
      "> RSP {'model_id': 'black-forest-labs/FLUX.1-dev', 'created_at': '31 July 2024 at 21:13:44 UTC', 'downloads': 1166389, 'likes': 7800, 'trending_score': 182, 'description': '---\\nlanguage:\\n- en\\nlicense: other\\nlicense_name: flux-1-dev-non-commercial-license\\nlicense_link: LICENSE.md\\nextra_gated_prompt: By clicking \"Agree\", you agree to the [FluxDev Non-Commercial License Agreement](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)\\n  and acknowledge the [Acceptable Use Policy](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/POLICY.md).\\ntags:\\n- text-to-image\\n- image-generation\\n- flux\\n---\\n\\n![FLUX.1 [dev] Grid](./dev_grid.jpg)\\n\\n`FLUX.1 [dev]` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.\\nFor more information, please read our [blog post](https://blackforestlabs.ai/announcing-black-forest-labs/).\\n\\n# Key Features\\n1. Cutting-edge output quality, second only to our state-of-the-art model `FLUX.1 [pro]`.\\n2. Competitive prompt following, matching the performance of closed source alternatives .\\n3. Trained using guidance distillation, making `FLUX.1 [dev]` more efficient.\\n4. Open weights to drive new scientific research, and empower artists to develop innovative workflows.\\n5. Generated outputs can be used for personal, scientific, and commercial purposes as described in the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).\\n\\n# Usage\\nWe provide a reference implementation of `FLUX.1 [dev]`, as well as sampling code, in a dedicated [github repository](https://github.com/black-forest-labs/flux).\\nDevelopers and creatives looking to build on top of `FLUX.1 [dev]` are encouraged to use this as a starting point.\\n\\n## API Endpoints\\nThe FLUX.1 models are also available via API from the following sources\\n- [bfl.ml](https://docs.bfl.ml/) (currently `FLUX.1 [pro]`)\\n- [replicate.com](https://replicate.com/collections/flux)\\n- [fal.ai](https://fal.ai/models/fal-ai/flux/dev)\\n- [mystic.ai](https://www.mystic.ai/black-forest-labs/flux1-dev)\\n\\n## ComfyUI\\n`FLUX.1 [dev]` is also available in [Comfy UI](https://github.com/comfyanonymous/ComfyUI) for local inference with a node-based workflow.\\n\\n## Diffusers\\n\\nTo use `FLUX.1 [dev]` with the 🧨 diffusers python library, first install or upgrade diffusers\\n\\n```shell\\npip install -U diffusers\\n```\\n\\nThen you can use `FluxPipeline` to run the model\\n\\n```python\\nimport torch\\nfrom diffusers import FluxPipeline\\n\\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\\npipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\\n\\nprompt = \"A cat holding a sign that says hello world\"\\nimage = pipe(\\n    prompt,\\n    height=1024,\\n    width=1024,\\n    guidance_scale=3.5,\\n    num_inference_steps=50,\\n    max_sequence_length=512,\\n    generator=torch.Generator(\"cpu\").manual_seed(0)\\n).images[0]\\nimage.save(\"flux-dev.png\")\\n```\\n\\nTo learn more check out the [diffusers](https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux) documentation\\n\\n---\\n# Limitations\\n- This model is not intended or able to provide factual information.\\n- As a statistical model this checkpoint might amplify existing societal biases.\\n- The model may fail to generate output that matches the prompts.\\n- Prompt following is heavily influenced by the prompting-style.\\n\\n# Out-of-Scope Use\\nThe model and its derivatives may not be used\\n\\n- In any way that violates any applicable national, federal, state, local or international law or regulation.\\n- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\\n- To generate or disseminate verifiably false information and/or content with the purpose of harming others.\\n- To generate or disseminate personal identifiable information that can be used to harm an individual.\\n- To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\\n- To create non-consensual nudity or illegal pornographic content.\\n- For fully automated decision making that adversely impacts an individual\\'s legal rights or otherwise creates or modifies a binding, enforceable obligation.\\n- Generating or facilitating large-scale disinformation campaigns.\\n\\n# License\\nThis model falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).'}\n",
      "< REQ Retrieving model information on the web using Tavily model_id=black-forest-labs/FLUX.1-dev\n",
      "> RSP [{'title': 'Black Forest Labs - Frontier AI Lab', 'url': 'https://blackforestlabs.ai/', 'content': 'Black Forest Labs offers four variants of FLUX, a state-of-the-art image generation model. FLUX.1 [dev] is an open-weight model for non-commercial applications, available on HuggingFace and Replicate.', 'score': 0.89279664, 'raw_content': None}, {'title': 'FLUX 1.1 - BlackForestLabs', 'url': 'https://blackforestlabs.org/flux-1-1/', 'content': 'FLUX 1.1 - BlackForestLabs FLUX 1.1FLUX 1.1 Model FLUX 1.1 FLUX 1.1 [dev] is a 12-billion-parameter, rectified flow transformer designed to generate images from text prompts. High-quality output, just behind the performance of our flagship FLUX.1 [pro] model. Generated images are permitted for personal, scientific, and commercial use, as outlined in the FLUX.1 [dev] Non-Commercial License. FLUX 1.1 Model Usage FLUX.1 models are also accessible via API through: FLUX.1 [dev] is integrated with ComfyUI, offering a node-based workflow for local inference. To use FLUX.1 [dev] with the 🧨 diffusers library in Python, begin by installing or updating diffusers: pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16) image.save(\"flux-dev.png\") This model is covered by the FLUX.1 [dev] Non-Commercial License. FLUX 1.1FLUX 1.1 Model', 'score': 0.85509074, 'raw_content': None}, {'title': 'black-forest-labs/FLUX.1-dev - Hugging Face', 'url': 'https://huggingface.co/black-forest-labs/FLUX.1-dev', 'content': 'FLUX.1-dev is a 12 billion parameter rectified flow transformer that generates images from text descriptions. It is available via API, diffusers, and Comfy UI, and has a non-commercial license agreement.', 'score': 0.84858394, 'raw_content': None}, {'title': 'FLUX.1 Dev | FLUX Dev AI Image Generator by Black Forest Labs', 'url': 'https://flux1ai.com/dev', 'content': 'FLUX.1 Dev is a guidance-distilled text-to-image model that can create high-quality images from text prompts. It is free for non-commercial use and has an open-weight architecture for research and development.', 'score': 0.82425016, 'raw_content': None}, {'title': 'README.md · black-forest-labs/FLUX.1-dev at main - Hugging Face', 'url': 'https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/README.md', 'content': 'FLUX.1 [dev] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. For more information, please read our blog post.. Key Features Cutting-edge output quality, second only to our state-of-the-art model FLUX.1 [pro].; Competitive prompt following, matching the performance of closed source alternatives .', 'score': 0.81489426, 'raw_content': None}]\n",
      "< REQ Retrieving model information on arxiv documents using Tavily model_id=black-forest-labs/FLUX.1-dev\n",
      "> RSP [{'title': 'I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow ...', 'url': 'https://arxiv.org/html/2410.07536v1', 'content': 'In this paper, we use a self-trained Lumina-Next-2K model and the open-source Flux.1-dev\\xa0Black Forest Labs (2024) model as representative rectified flow transformers (RFTs) to validate the general effectiveness of I-Max for RFTs. Lumina-Next-2K is a 2K generative model derived from the open-source Lumina-Next model\\xa0Zhuo et\\xa0al. In this section, using GPT-4o preference assessments, we compare the model’s generation results at its native resolution with high-resolution images generated using I-Max. As shown in Fig. 7, we can observe that for a certain range of scaling factors, the extrapolated results even achieve over ', 'score': 0.56517935, 'raw_content': None}, {'title': 'Abstract - arXiv.org', 'url': 'https://arxiv.org/pdf/2410.22775', 'content': 'FLUX: FLUX family models [21] are newly introduced T2I models developed by Black Forest Lab [20]. To the best of our knowledge, no formal technical report is available about this model. ... FLUX-Dev Aug 2024 - 512×512 - T5 & CLIP-based FLUX-Schnell Aug 2024 - 1024×1024 - T5 & CLIP-based', 'score': 0.511644, 'raw_content': None}, {'title': 'SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion ...', 'url': 'https://arxiv.org/html/2411.05007v2', 'content': 'To our knowledge, we are the first to successfully apply 4-bit post-training quantization to both the weights and activations of diffusion models, and achieve measured speedup on NVIDIA GPUs. On the latest 12B FLUX.1, we largely preserve the image quality and reduce the memory footprint of the original BF16 model by 3.5× and deliver a 3.0× speedup over the NF4 weight-only quantized baseline, measured on a 16GB laptop-level RTX4090 GPU. Among these works, only MixDQ\\xa0(Zhao et\\xa0al., 2024c) and ViDiT-Q\\xa0(Zhao et\\xa0al., 2024b) implement low-bit inference engines and report measured 8-bit speedup on GPUs. In this work, we push the boundary further by quantizing diffusion models to 4 bits, supporting both the integer or floating-point data types, compatible with the UNet backbone\\xa0(Ho et\\xa0al., 2020) and recent DiT architecture\\xa0(Peebles & Xie, 2023).', 'score': 0.36711195, 'raw_content': None}, {'title': 'SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion ...', 'url': 'https://arxiv.org/html/2411.05007v1', 'content': 'To our knowledge, we are the first to successfully apply 4-bit post-training quantization to both the weights and activations of diffusion models, and achieve measured speedup on NVIDIA GPUs. On the latest 12B FLUX.1, we largely preserve the image quality and reduce the memory footprint of the original BF16 model by 3.5× and deliver a 3.0× speedup over the NF4 weight-only quantized baseline, measured on a 16GB laptop-level RTX4090 GPU. Among these works, only MixDQ\\xa0(Zhao et\\xa0al., 2024c) and ViDiT-Q\\xa0(Zhao et\\xa0al., 2024b) implement low-bit inference engines and report measured 8-bit speedup on GPUs. In this work, we push the boundary further by quantizing diffusion models to 4 bits, supporting both the integer or floating-point data types, compatible with the UNet backbone\\xa0(Ho et\\xa0al., 2020) and recent DiT architecture\\xa0(Peebles & Xie, 2023).', 'score': 0.36711195, 'raw_content': None}, {'title': 'Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study', 'url': 'https://arxiv.org/html/2411.13588v1', 'content': 'While prior research has highlighted the presence of high similarity in activation values between adjacent diffusion steps (referred to as redundancy) and proposed various caching mechanisms to mitigate computational overhead, the exploration of redundancy in existing literature remains limited, with findings often not generalizable across different DiT models. Our experimental analysis reveals substantial variations in the distribution of redundancy across diffusion steps among different DiT models. We systematically explore redundancy across diffusion steps in a diverse range of prominent DiT models, including FLUX.1-dev, Pixart-Alpha, Stable-Diffusion-3, CogVideoX-5B, Open-Sora, Latte-1, and Mochi-1-preview. By contrast, in the domain of DiT models, L2C\\xa0[8], Tgate\\xa0[9], and PAB\\xa0[10] explores the redundancy distribution across diffusion steps and propose caching mechanism to accelerate the diffusion process. Consequently, we can infer that the number of diffusion steps does not significantly alter the redundancy distribution in DiT models.', 'score': 0.33541664, 'raw_content': None}, {'title': '1.58-bit FLUX - arXiv.org', 'url': 'https://arxiv.org/html/2412.18653v1', 'content': 'In this work, we focus on post-training 1.58-bit quantization of FLUX, a state-of-the-art open-source text-to-image (T2I) model. [2023a]   Yefei He, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang.Efficientdm: Efficient quantization-aware fine-tuning of low-bit diffusion models.arXiv preprint arXiv:2310.03270, 2023a. [2024a] Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et\\xa0al.Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation.arXiv preprint arXiv:2406.02540, 2024a. [2024b] Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, and Yu Wang.Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization.arXiv preprint arXiv:2405.17873, 2024b.', 'score': 0.33105537, 'raw_content': None}, {'title': 'Training-free Regional Prompting for Diffusion Transformers - arXiv.org', 'url': 'https://arxiv.org/html/2411.02395v1', 'content': 'In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. In addition to improving the base model, some recent studies\\xa0[16, 8, 17, 18, 19] have proposed to handle compositional control by providing spatial conditions (layout/box) and training a control module as a plugin on top of the base model, or to manipulate the attention score map using region masks in a training-free manner. RPG\\xa0[8] employs the Multi-modal Large Language Model (MLLM)\\xa0[20] as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions, and proposes complementary regional diffusion to enable region-wise compositional generation. We propose a training-free regional prompting method for FLUX.1, enabling fine-grained compositional generation for transformer-based models with swift and responsive image generation.', 'score': 0.2515378, 'raw_content': None}, {'title': 'Diffusion Beats Autoregressive: An Evaluation of Compositional ...', 'url': 'https://arxiv.org/html/2410.22775v1', 'content': 'Recently, a new open-source diffusion-based T2I model, FLUX, has been introduced, demonstrating strong performance in high-quality image generation. Recent advancements in computational resources and data scaling have led to the development of substantial text-to-image (T2I) models, from diffusion-based [16] models such as Stable Diffusion [37, 31] and DALL-E [34, 33, 4] to autoregressive-based ones such as LlamaGen [41], which are capable of producing high-quality and realistic images from textual prompts. While DALL-E1 [34] utilizes a discrete variational auto-encoder [43] model to generate image tokens from textual tokens, DALL-E2 [33] first uses a pre-trained CLIP-based model to prepare the text embeddings from the input prompt, which is then fed to a diffusion or autoregressive model to produce an image embedding.', 'score': 0.13990116, 'raw_content': None}, {'title': 'Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject ...', 'url': 'https://arxiv.org/html/2411.15466v1', 'content': 'In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Figure 1:Given a single reference image, our Diptych Prompting performs zero-shot subject-driven text-to-image generation through diptych inpainting. In our approach, we reinterpret the task as a diptych inpainting process: the left panel contains a reference image of the subject as a visual cue, and the right panel is generated through inpainting based on a text prompt describing the diptych with the desired context. In this paper, we proposed Diptych Prompting, an inpainting-based approach for zero-shot subject-driven text-to-image generation.', 'score': 0.02876438, 'raw_content': None}, {'title': 'OminiControl: Minimal and Universal Control for Diffusion Transformer', 'url': 'https://arxiv.org/html/2411.15098v1', 'content': 'We present a parameter-efficient method for enabling image-conditioned control in Diffusion Transformer (DiT) models, achieving both spatially aligned and non-spatially aligned control within a unified framework. We evaluate our method on two categories of conditional generation tasks: spatially aligned tasks (including Canny-to-image, depth-to-image, masked-based inpainting, and colorization) and subject-driven generation. effectively controls the generation process for both spatially-aligned tasks like canny-to-image generation and non-spatially-aligned tasks like subject-driven generation, enabling flexible control over the condition’s influence. As shown in Table\\xa03, our experiments show that increasing the LoRA rank generally improves model performance, with rank 16 achieving the best results across multiple aspects: image quality (measured by FID and SSIM), condition control capability (measured by F1 Score), while maintaining competitive text-image consistency (measured by CLIP-Score).', 'score': 0.027608924, 'raw_content': None}]\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.05007\n",
      "> RSP [SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models](http://arxiv.org/abs/2411.05007v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.13588\n",
      "> RSP [Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study](http://arxiv.org/abs/2411.13588v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.15098\n",
      "> RSP [OminiControl: Minimal and Universal Control for Diffusion Transformer](http://arxiv.org/abs/2411.15098v3)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.15466\n",
      "> RSP [Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator](http://arxiv.org/abs/2411.15466v1)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2410.07536\n",
      "> RSP [I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow](http://arxiv.org/abs/2410.07536v2)\n",
      "< REQ Retrieving paper information using Arxiv API arxiv_id=2411.02395\n"
     ]
    }
   ],
   "source": [
    "models_info = large_models_market_analysis_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c972d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_c972d_level0_col0\" class=\"col_heading level0 col0\" >Model Name</th>\n",
       "      <th id=\"T_c972d_level0_col1\" class=\"col_heading level0 col1\" >Created At</th>\n",
       "      <th id=\"T_c972d_level0_col2\" class=\"col_heading level0 col2\" >Total Downloads</th>\n",
       "      <th id=\"T_c972d_level0_col3\" class=\"col_heading level0 col3\" >Total Likes</th>\n",
       "      <th id=\"T_c972d_level0_col4\" class=\"col_heading level0 col4\" >Trending Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row0_col0\" class=\"data row0 col0\" >deepseek-ai/DeepSeek-V3</td>\n",
       "      <td id=\"T_c972d_row0_col1\" class=\"data row0 col1\" >25 December 2024 at 12:52:23 UTC</td>\n",
       "      <td id=\"T_c972d_row0_col2\" class=\"data row0 col2\" >74084</td>\n",
       "      <td id=\"T_c972d_row0_col3\" class=\"data row0 col3\" >1411</td>\n",
       "      <td id=\"T_c972d_row0_col4\" class=\"data row0 col4\" >611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row1_col0\" class=\"data row1 col0\" >PowerInfer/SmallThinker-3B-Preview</td>\n",
       "      <td id=\"T_c972d_row1_col1\" class=\"data row1 col1\" >12 December 2024 at 11:56:09 UTC</td>\n",
       "      <td id=\"T_c972d_row1_col2\" class=\"data row1 col2\" >6996</td>\n",
       "      <td id=\"T_c972d_row1_col3\" class=\"data row1 col3\" >288</td>\n",
       "      <td id=\"T_c972d_row1_col4\" class=\"data row1 col4\" >217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row2_col0\" class=\"data row2 col0\" >deepseek-ai/DeepSeek-V3-Base</td>\n",
       "      <td id=\"T_c972d_row2_col1\" class=\"data row2 col1\" >25 December 2024 at 12:52:06 UTC</td>\n",
       "      <td id=\"T_c972d_row2_col2\" class=\"data row2 col2\" >8663</td>\n",
       "      <td id=\"T_c972d_row2_col3\" class=\"data row2 col3\" >1180</td>\n",
       "      <td id=\"T_c972d_row2_col4\" class=\"data row2 col4\" >188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row3_col0\" class=\"data row3 col0\" >black-forest-labs/FLUX.1-dev</td>\n",
       "      <td id=\"T_c972d_row3_col1\" class=\"data row3 col1\" >31 July 2024 at 21:13:44 UTC</td>\n",
       "      <td id=\"T_c972d_row3_col2\" class=\"data row3 col2\" >1166389</td>\n",
       "      <td id=\"T_c972d_row3_col3\" class=\"data row3 col3\" >7799</td>\n",
       "      <td id=\"T_c972d_row3_col4\" class=\"data row3 col4\" >181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row4_col0\" class=\"data row4 col0\" >hexgrad/Kokoro-82M</td>\n",
       "      <td id=\"T_c972d_row4_col1\" class=\"data row4 col1\" >26 December 2024 at 00:20:08 UTC</td>\n",
       "      <td id=\"T_c972d_row4_col2\" class=\"data row4 col2\" >1479</td>\n",
       "      <td id=\"T_c972d_row4_col3\" class=\"data row4 col3\" >269</td>\n",
       "      <td id=\"T_c972d_row4_col4\" class=\"data row4 col4\" >168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row5_col0\" class=\"data row5 col0\" >meta-llama/Llama-3.3-70B-Instruct</td>\n",
       "      <td id=\"T_c972d_row5_col1\" class=\"data row5 col1\" >26 November 2024 at 16:08:47 UTC</td>\n",
       "      <td id=\"T_c972d_row5_col2\" class=\"data row5 col2\" >416929</td>\n",
       "      <td id=\"T_c972d_row5_col3\" class=\"data row5 col3\" >1516</td>\n",
       "      <td id=\"T_c972d_row5_col4\" class=\"data row5 col4\" >127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row6_col0\" class=\"data row6 col0\" >StephanST/WALDO30</td>\n",
       "      <td id=\"T_c972d_row6_col1\" class=\"data row6 col1\" >02 October 2024 at 14:20:40 UTC</td>\n",
       "      <td id=\"T_c972d_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_c972d_row6_col3\" class=\"data row6 col3\" >163</td>\n",
       "      <td id=\"T_c972d_row6_col4\" class=\"data row6 col4\" >102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row7_col0\" class=\"data row7 col0\" >nomic-ai/modernbert-embed-base</td>\n",
       "      <td id=\"T_c972d_row7_col1\" class=\"data row7 col1\" >29 December 2024 at 23:51:30 UTC</td>\n",
       "      <td id=\"T_c972d_row7_col2\" class=\"data row7 col2\" >4837</td>\n",
       "      <td id=\"T_c972d_row7_col3\" class=\"data row7 col3\" >135</td>\n",
       "      <td id=\"T_c972d_row7_col4\" class=\"data row7 col4\" >89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row8_col0\" class=\"data row8 col0\" >cognitivecomputations/Dolphin3.0-Llama3.1-8B</td>\n",
       "      <td id=\"T_c972d_row8_col1\" class=\"data row8 col1\" >29 December 2024 at 18:37:00 UTC</td>\n",
       "      <td id=\"T_c972d_row8_col2\" class=\"data row8 col2\" >242</td>\n",
       "      <td id=\"T_c972d_row8_col3\" class=\"data row8 col3\" >82</td>\n",
       "      <td id=\"T_c972d_row8_col4\" class=\"data row8 col4\" >82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c972d_row9_col0\" class=\"data row9 col0\" >stabilityai/stable-diffusion-3.5-large</td>\n",
       "      <td id=\"T_c972d_row9_col1\" class=\"data row9 col1\" >22 October 2024 at 07:29:57 UTC</td>\n",
       "      <td id=\"T_c972d_row9_col2\" class=\"data row9 col2\" >127483</td>\n",
       "      <td id=\"T_c972d_row9_col3\" class=\"data row9 col3\" >1807</td>\n",
       "      <td id=\"T_c972d_row9_col4\" class=\"data row9 col4\" >66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x130bba750>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_comparison_table(models_info).style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# deepseek-ai/DeepSeek-V3\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [DeepSeek v3 - Advanced AI & LLM Model Online](https://deepseekv3.org/)\n",
       "* [DeepSeek-V3 Technical Report - arXiv.org](https://arxiv.org/pdf/2412.19437)\n",
       "* [DeepSeek V3: Advanced AI Language Model with 671B Parameters](https://www.deepseekv3.com/en)\n",
       "* [Introducing DeepSeek-V3 | DeepSeek API Docs](https://api-docs.deepseek.com/news/news1226)\n",
       "* [DeepSeek Online - Try DeepSeek V3 Free | No Registration Required](https://www.deepseekv3.net/)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[DeepSeek-VL: Towards Real-World Vision-Language Understanding](http://arxiv.org/abs/2403.05525v2)']\n",
       "* ['[DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)']\n",
       "* ['[DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/abs/2401.02954v1)']\n",
       "\n",
       "# PowerInfer/SmallThinker-3B-Preview\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [SmallThinker 3B: A Small Thinking Model Revolutionizing AI Efficiency](https://pub.towardsai.net/smallthinker-3b-a-small-thinking-model-revolutionizing-ai-efficiency-f528cf7d6906)\n",
       "* [smallthinker:3b](https://registry.ollama.com/library/smallthinker:3b)\n",
       "* [Testing SmallThinker 3B Preview by PowerInfer - YouTube](https://www.youtube.com/watch?v=OVNnXQp_wNU)\n",
       "* [SmallThinker-由Qwen 2.5 3B而来的，全新小型推理模型](https://www.ilinkandlink.com/2025/01/02/smallthinker/)\n",
       "* [SmallThinker 3B Preview By PowerInfer: Benchmarks, Features and ...](https://llm.extractum.io/model/PowerInfer/SmallThinker-3B-Preview,6YvWQRdkYbb2o0HqvU7LTJ)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[PowerInfer-2: Fast Large Language Model Inference on a Smartphone](http://arxiv.org/abs/2406.06282v3)']\n",
       "* ['[PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](http://arxiv.org/abs/2312.12456v2)']\n",
       "\n",
       "# deepseek-ai/DeepSeek-V3-Base\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [DeepSeek-V3, ultra-large open-source AI, outperforms ... - VentureBeat](https://venturebeat.com/ai/deepseek-v3-ultra-large-open-source-ai-outperforms-llama-and-qwen-on-launch/)\n",
       "* [DeepSeek-V3 Technical Report - arXiv.org](https://arxiv.org/pdf/2412.19437)\n",
       "* [Paper page - DeepSeek-V3 Technical Report - Hugging Face](https://huggingface.co/papers/2412.19437)\n",
       "* [README_WEIGHTS.md · deepseek-ai/DeepSeek-V3-Base at main - Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base/blob/main/README_WEIGHTS.md)\n",
       "* [deepseek-ai/DeepSeek-V3 - GitHub](https://github.com/deepseek-ai/DeepSeek-V3)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding](http://arxiv.org/abs/2412.10302v1)']\n",
       "* ['[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/abs/2402.03300v3)']\n",
       "* ['[DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)']\n",
       "* ['[DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/abs/2401.02954v1)']\n",
       "\n",
       "# black-forest-labs/FLUX.1-dev\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [black-forest-labs/FLUX.1-dev at main - Hugging Face](https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main)\n",
       "* [FLUX 1.1 - BlackForestLabs](https://blackforestlabs.org/flux-1-1/)\n",
       "* [README.md · black-forest-labs/FLUX.1-dev at main - Hugging Face](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/README.md)\n",
       "* [FLUX.1 Dev | FLUX Dev AI Image Generator by Black Forest Labs](https://flux1ai.com/dev)\n",
       "* [black-forest-labs/FLUX.1-dev - Hugging Face](https://huggingface.co/black-forest-labs/FLUX.1-dev)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models](http://arxiv.org/abs/2411.05007v2)']\n",
       "* ['[Schedule On the Fly: Diffusion Time Prediction for Faster and Better Image Generation](http://arxiv.org/abs/2412.01243v1)']\n",
       "* ['[OminiControl: Minimal and Universal Control for Diffusion Transformer](http://arxiv.org/abs/2411.15098v3)']\n",
       "* ['[I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow](http://arxiv.org/abs/2410.07536v2)']\n",
       "* ['[Training-free Regional Prompting for Diffusion Transformers](http://arxiv.org/abs/2411.02395v1)']\n",
       "* ['[Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models](http://arxiv.org/abs/2410.22775v1)']\n",
       "\n",
       "# hexgrad/Kokoro-82M\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [Hugging Face - The AI community building the future.](https://hf.wing.moe/)\n",
       "* [GitHub - remsky/Kokoro-FastAPI: Dockerized FastAPI wrapper for Kokoro ...](https://github.com/remsky/Kokoro-FastAPI)\n",
       "* [hexgrad/Kokoro-82M · [TODO] FP16 Inference - Hugging Face](https://huggingface.co/hexgrad/Kokoro-82M/discussions/4)\n",
       "* [Kokoro 82M Installation - Best TTS Model to Run on Google Colab](https://www.youtube.com/watch?v=up-ZG35uuvQ)\n",
       "* [Models - Hugging Face](https://hf.wing.moe/models)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis](http://arxiv.org/abs/2205.15439v2)']\n",
       "* ['[A Survey of Resource-efficient LLM and Multimodal Foundation Models](http://arxiv.org/abs/2401.08092v2)']\n",
       "* ['[HEMM: Holistic Evaluation of Multimodal Foundation Models](http://arxiv.org/abs/2407.03418v1)']\n",
       "* ['[Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech](http://arxiv.org/abs/2105.06337v2)']\n",
       "* ['[PGTask: Introducing the Task of Profile Generation from Dialogues](http://arxiv.org/abs/2304.06634v2)']\n",
       "* ['[Enhancing Inflation Nowcasting with LLM: Sentiment Analysis on News](http://arxiv.org/abs/2410.20198v1)']\n",
       "* ['[StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models](http://arxiv.org/abs/2306.07691v2)']\n",
       "* ['[Extending Whisper with prompt tuning to target-speaker ASR](http://arxiv.org/abs/2312.08079v2)']\n",
       "\n",
       "# meta-llama/Llama-3.3-70B-Instruct\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [meta / llama-3.3-70b-instruct - docs.api.nvidia.com](https://docs.api.nvidia.com/nim/reference/meta-llama-3_3-70b-instruct)\n",
       "* [Llama 3.3 | Model Cards and Prompt formats](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/)\n",
       "* [Meta's new Llama 3.3 70B Instruct model now available on watsonx.ai - IBM](https://www.ibm.com/new/announcements/meta-s-new-llama-3-3-70b-instruct-model-now-available-on-watsonx-ai)\n",
       "* [Llama 3.3 70B Instruct - API, Providers, Stats | OpenRouter](https://openrouter.ai/meta-llama/llama-3.3-70b-instruct)\n",
       "* [unsloth/Llama-3.3-70B-Instruct - Hugging Face](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[MGH Radiology Llama: A Llama 3 70B Model for Radiology](http://arxiv.org/abs/2408.11848v2)']\n",
       "* ['[Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging: A Comprehensive Evaluation](http://arxiv.org/abs/2406.14971v1)']\n",
       "* ['[The Llama 3 Herd of Models](http://arxiv.org/abs/2407.21783v3)']\n",
       "* ['[Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data](http://arxiv.org/abs/2409.00096v1)']\n",
       "* ['[LiveBench: A Challenging, Contamination-Free LLM Benchmark](http://arxiv.org/abs/2406.19314v1)']\n",
       "* ['[Confidential Computing on NVIDIA Hopper GPUs: A Performance Benchmark Study](http://arxiv.org/abs/2409.03992v4)']\n",
       "\n",
       "# StephanST/WALDO30\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [Stephan Sturges on LinkedIn: WALDO 3.0 is coming along nicely! This ...](https://www.linkedin.com/posts/stephanst_waldo-30-is-coming-along-nicely-this-release-activity-7194234634816081922-SeGA)\n",
       "* [WALDO/Readme.md at master · stephansturges/WALDO · GitHub](https://github.com/stephansturges/WALDO/blob/master/Readme.md)\n",
       "* [WALDO30 | AI Model Details](https://www.aimodels.fyi/models/huggingFace/waldo30-stephanst)\n",
       "* [Waldo30 - use with Halio - General - Hailo Community](https://community.hailo.ai/t/waldo30-use-with-halio/4832)\n",
       "* [stephansturges/WALDO - GitHub](https://github.com/stephansturges/WALDO)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[Dark Matter (H)eats Young Planets](http://arxiv.org/abs/2309.02495v3)']\n",
       "* ['[Lagrangian Neural Networks](http://arxiv.org/abs/2003.04630v2)']\n",
       "* ['[Kernel Methods for Interferometric Imaging](http://arxiv.org/abs/2412.01908v1)']\n",
       "* ['[OPT: Open Pre-trained Transformer Language Models](http://arxiv.org/abs/2205.01068v4)']\n",
       "* ['[A Broad-line, Low-luminosity Active Galactic Nucleus at ${z=7.3}$ Anchoring a Large Galaxy Overdensity](http://arxiv.org/abs/2411.11534v1)']\n",
       "* ['[Applications of machine learning in gravitational wave research with current interferometric detectors](http://arxiv.org/abs/2412.15046v1)']\n",
       "* ['[MSA-3D: Metallicity Gradients in Galaxies at $z\\\\sim1$ with JWST/NIRSpec Slit-stepping Spectroscopy](http://arxiv.org/abs/2409.01616v3)']\n",
       "* ['[Panning for gold with the Neil Gehrels Swift Observatory: an optimal strategy for finding the counterparts to gravitational wave events](http://arxiv.org/abs/2411.05072v2)']\n",
       "* ['[JWST Imaging of Edge-on Protoplanetary Disks. IV. Mid-infrared Dust Scattering in the HH 30 disk](http://arxiv.org/abs/2412.07523v1)']\n",
       "\n",
       "# nomic-ai/modernbert-embed-base\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [[Hands-on] RAG on Docs Using ModernBERT - by Avi Chawla](https://blog.dailydoseofds.com/p/hands-on-rag-on-docs-using-modernbert)\n",
       "* [Nomic AI Launches ModernBERT-Embed-Base, Trained on... | DeepNewz](https://deepnewz.com/ai-modeling/nomic-ai-launches-modernbert-embed-base-trained-on-235-million-documents-256-f4acadfc)\n",
       "* [Install ModernBERT Embed Locally - Great New RAG Model](https://www.youtube.com/watch?v=HcVav0IqZlk)\n",
       "* [ModernBERT: A new improved BERT for text embeddings](https://medium.com/data-science-in-your-pocket/modernbert-a-new-improved-bert-for-text-embeddings-538239202527)\n",
       "* [ModernBERT — A modernized BERT for NLP tasks | UnfoldAI](https://unfoldai.com/modernbert/)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[CoRNStack: High-Quality Contrastive Data for Better Code Ranking](http://arxiv.org/abs/2412.01007v2)']\n",
       "* ['[Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models](http://arxiv.org/abs/2405.05374v1)']\n",
       "* ['[Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](http://arxiv.org/abs/2412.13663v2)']\n",
       "* ['[Nomic Embed: Training a Reproducible Long Context Text Embedder](http://arxiv.org/abs/2402.01613v1)']\n",
       "\n",
       "# cognitivecomputations/Dolphin3.0-Llama3.1-8B\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [Dolphin3.0 Llama3.1 8B by cognitivecomputations](https://llm.extractum.io/model/cognitivecomputations/Dolphin3.0-Llama3.1-8B,7rToucpFJjB0d8WeBFJ57p)\n",
       "* [dolphin-llama3](https://ollama.com/library/dolphin-llama3)\n",
       "* [nchapman/dolphin3.0-llama3:8b](https://ollama.com/nchapman/dolphin3.0-llama3:8b)\n",
       "* [Cognitivecomputations/Dolphin3.0-Llama3.1-8B uncensored - Hacker News](https://news.ycombinator.com/item?id=42607271)\n",
       "* [Dolphin 3.0 Released (Llama 3.1 + 3.2 + Qwen 2.5): A Local-First ...](https://www.marktechpost.com/2025/01/05/dolphin-3-0-released-llama-3-1-3-2-qwen-2-5-a-local-first-steerable-ai-model-that-puts-you-in-control-of-your-ai-stack-and-alignment/)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[Hermes 3 Technical Report](http://arxiv.org/abs/2408.11857v1)']\n",
       "* ['[Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders](http://arxiv.org/abs/2410.20526v1)']\n",
       "* [\"[Extending Llama-3's Context Ten-Fold Overnight](http://arxiv.org/abs/2404.19553v1)\"]\n",
       "* ['[The Llama 3 Herd of Models](http://arxiv.org/abs/2407.21783v3)']\n",
       "* ['[Llama 3 Meets MoE: Efficient Upcycling](http://arxiv.org/abs/2412.09952v1)']\n",
       "* ['[Applying Refusal-Vector Ablation to Llama 3.1 70B Agents](http://arxiv.org/abs/2410.10871v1)']\n",
       "* ['[Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction](http://arxiv.org/abs/2410.23692v1)']\n",
       "* ['[The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization](http://arxiv.org/abs/2408.15301v2)']\n",
       "* ['[EXAONE 3.0 7.8B Instruction Tuned Language Model](http://arxiv.org/abs/2408.03541v3)']\n",
       "\n",
       "# stabilityai/stable-diffusion-3.5-large\n",
       "\n",
       "TBD\n",
       "\n",
       "Mentioned in the following web pages:\n",
       "* [stabilityai/stable-diffusion-3.5-large · Hugging Face](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)\n",
       "* [ControlNets for Stable Diffusion 3.5 Large - Stability AI](https://stability.ai/news/sd3-5-large-controlnets)\n",
       "* [Stable Diffusion 3.5 Large - a Hugging Face Space by stabilityai](https://huggingface.co/spaces/stabilityai/stable-diffusion-3.5-large)\n",
       "* [GitHub - Stability-AI/sd3.5](https://github.com/Stability-AI/sd3.5)\n",
       "* [Introducing Stable Diffusion 3.5 - Stability AI](https://stability.ai/news/introducing-stable-diffusion-3-5)\n",
       "\n",
       "Mentioned in the following papers:\n",
       "* ['[Detecting AutoEncoder is Enough to Catch LDM Generated Images](http://arxiv.org/abs/2411.06441v1)']\n",
       "* ['[Schedule On the Fly: Diffusion Time Prediction for Faster and Better Image Generation](http://arxiv.org/abs/2412.01243v1)']\n",
       "* ['[IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models](http://arxiv.org/abs/2308.06721v1)']\n",
       "* ['[Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study](http://arxiv.org/abs/2411.13588v1)']\n",
       "* ['[Stable Diffusion is a Natural Cross-Modal Decoder for Layered AI-generated Image Compression](http://arxiv.org/abs/2412.12982v1)']\n",
       "* ['[DiffusionPipe: Training Large Diffusion Models with Efficient Pipelines](http://arxiv.org/abs/2405.01248v1)']\n",
       "* ['[DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models](http://arxiv.org/abs/2210.14896v4)']\n",
       "* ['[Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2](http://arxiv.org/abs/2210.00586v2)']\n",
       "* ['[Context-Aware Full Body Anonymization using Text-to-Image Diffusion Models](http://arxiv.org/abs/2410.08551v2)']\n",
       "* ['[Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training](http://arxiv.org/abs/2312.16204v3)']\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(get_models_overview(models_info)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-analysis-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
