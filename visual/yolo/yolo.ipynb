{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO model"
      ],
      "metadata": {
        "id": "El4SO5enFmc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20G3Vz-3FhQx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Convolutional block consisting of Conv2d + BatchNorm + LeakyReLU\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.leaky_relu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class YOLOv3(nn.Module):\n",
        "    def __init__(self, num_classes=20):\n",
        "        super(YOLOv3, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define the layers of YOLOv3 architecture\n",
        "        self.conv1 = ConvBlock(3, 32, 3, 1, 1)\n",
        "        self.conv2 = ConvBlock(32, 64, 3, 2, 1)\n",
        "        self.conv3 = ConvBlock(64, 128, 3, 2, 1)\n",
        "\n",
        "        # Stack more layers as per the YOLOv3 design\n",
        "        # For simplicity, I'll keep it small here\n",
        "        self.conv_final = nn.Conv2d(128, (num_classes + 5) * 3, 1, 1, 0)  # (B, C+5)*3 for bounding boxes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        out = self.conv_final(x)  # Output layer: (B, (C+5)*3, H, W)\n",
        "\n",
        "        # Output reshape to match bounding box output format\n",
        "        return out.view(out.size(0), -1, self.num_classes + 5)  # (B, N, 25)\n",
        "\n",
        "# Instantiate the model for 20 classes (e.g., Pascal VOC)\n",
        "model = YOLOv3(num_classes=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "xnFqlcwtKOC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pascal VOC class names to integer mapping\n",
        "VOC_CLASSES = [\n",
        "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
        "    'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "    'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
        "    'sheep', 'sofa', 'train', 'tvmonitor'\n",
        "]\n",
        "\n",
        "# Create a dictionary for class name to index mapping\n",
        "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(VOC_CLASSES)}"
      ],
      "metadata": {
        "id": "PMUaC5buT3zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yolo_loss(predictions, targets, num_classes=20, lambda_coord=5, lambda_noobj=0.5):\n",
        "    \"\"\"\n",
        "    A simplified YOLO loss function.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: Predicted bounding boxes, objectness, and class probabilities (tensor)\n",
        "    - targets: Ground truth bounding boxes and class labels (list of dictionaries)\n",
        "    - num_classes: Number of classes\n",
        "    - lambda_coord: Weight for the localization (coordinate) loss\n",
        "    - lambda_noobj: Weight for the no-object confidence loss\n",
        "\n",
        "    Returns:\n",
        "    - Total loss (localization + objectness + classification)\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack predictions\n",
        "    pred_bboxes = predictions[..., :4]  # Predicted bounding boxes (x, y, w, h)\n",
        "    pred_conf = predictions[..., 4]     # Predicted objectness score\n",
        "    pred_class = predictions[..., 5:]   # Predicted class probabilities\n",
        "\n",
        "    # Prepare the losses\n",
        "    coord_loss = 0\n",
        "    conf_loss = 0\n",
        "    class_loss = 0\n",
        "\n",
        "    mse_loss = nn.MSELoss()\n",
        "    bce_loss = nn.BCEWithLogitsLoss()\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    for i, target in enumerate(targets):  # Loop over each image in the batch\n",
        "        objects = target['object']  # Access all objects in the image\n",
        "        if isinstance(objects, dict):\n",
        "            objects = [objects]  # Ensure it's a list if there is only one object\n",
        "\n",
        "        for obj in objects:\n",
        "            # Extract bounding boxes (as floats), confidence, and class\n",
        "            gt_bboxes = torch.tensor([\n",
        "                float(bbox['xmin']),\n",
        "                float(bbox['ymin']),\n",
        "                float(bbox['xmax']),\n",
        "                float(bbox['ymax'])\n",
        "            ]).to(pred_bboxes.device)\n",
        "\n",
        "            gt_conf = 1.0  # Assume confidence is 1 (object exists)\n",
        "\n",
        "            # Use the class_to_idx mapping to get the integer label for the class name\n",
        "            gt_class = class_to_idx[obj['name']]\n",
        "\n",
        "            # Localization loss: Compare predicted and ground truth bounding boxes\n",
        "            coord_loss += lambda_coord * mse_loss(pred_bboxes[i], gt_bboxes)\n",
        "\n",
        "            # Objectness loss: Compare predicted and ground truth object confidence\n",
        "            gt_conf_tensor = torch.full_like(pred_conf[i], gt_conf).to(pred_conf.device)\n",
        "            conf_loss += bce_loss(pred_conf[i], gt_conf_tensor)\n",
        "\n",
        "            # Classification loss: Compare predicted and ground truth class probabilities\n",
        "            target_class = torch.zeros(1, num_classes, dtype=torch.long).to(pred_class.device)\n",
        "            target_class[0, gt_class] = 1\n",
        "            class_loss += ce_loss(pred_class[i].unsqueeze(0), target_class)\n",
        "\n",
        "    # Total loss = localization loss + confidence loss + classification loss\n",
        "    total_loss = coord_loss + lambda_noobj * conf_loss + class_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "kPpmZR_OSfUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, optimizer, loss_fn, num_epochs=10, device=\"cuda\"):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (images, targets) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Move each target dictionary to the device\n",
        "            for target in targets:\n",
        "                for key, value in target.items():\n",
        "                    if isinstance(value, torch.Tensor):\n",
        "                        target[key] = value.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 100 == 99:  # Print loss every 100 mini-batches\n",
        "                print(f\"[{epoch+1}, {i+1}] loss: {running_loss / 100:.3f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Custom collate function to handle batches with varying annotation sizes\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "\n",
        "    for sample in batch:\n",
        "        images.append(sample[0])  # Append image tensor\n",
        "        targets.append(sample[1]['annotation'])  # Append target annotations\n",
        "\n",
        "    images = torch.stack(images, 0)  # Stack images along the batch dimension\n",
        "    return images, targets\n",
        "\n",
        "# Define transforms to resize all images to the same size (e.g., 416x416)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((416, 416)),  # Resize all images to 416x416\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Load Pascal VOC dataset with resizing and the custom collate function\n",
        "train_dataset = datasets.VOCDetection('data/VOCdevkit/', year='2012', image_set='train',\n",
        "                                      download=True, transform=transform)\n",
        "\n",
        "# DataLoader with custom collate_fn\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, optimizer, yolo_loss, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIA__RYzKPjN",
        "outputId": "fcc8da06-7722-4cef-d41c-a64a161ad197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: data/VOCdevkit/VOCtrainval_11-May-2012.tar\n",
            "Extracting data/VOCdevkit/VOCtrainval_11-May-2012.tar to data/VOCdevkit/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([32448, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 100] loss: 8057353.633\n",
            "[1, 200] loss: 7179640.202\n",
            "[1, 300] loss: 6347760.053\n",
            "[1, 400] loss: 5512673.098\n",
            "[1, 500] loss: 4753820.412\n",
            "[1, 600] loss: 4167145.842\n",
            "[1, 700] loss: 3480778.879\n",
            "[2, 100] loss: 2905216.131\n",
            "[2, 200] loss: 2607282.801\n",
            "[2, 300] loss: 2364014.320\n",
            "[2, 400] loss: 2317555.264\n",
            "[2, 500] loss: 2256733.799\n",
            "[2, 600] loss: 2060348.269\n",
            "[2, 700] loss: 2243113.618\n",
            "[3, 100] loss: 2261567.931\n",
            "[3, 200] loss: 2237175.893\n",
            "[3, 300] loss: 2143934.901\n",
            "[3, 400] loss: 2183021.415\n",
            "[3, 500] loss: 2198932.078\n",
            "[3, 600] loss: 2219960.942\n",
            "[3, 700] loss: 2174068.716\n",
            "[4, 100] loss: 2281878.473\n",
            "[4, 200] loss: 2180962.293\n",
            "[4, 300] loss: 2251474.288\n",
            "[4, 400] loss: 2161326.547\n",
            "[4, 500] loss: 2146677.171\n",
            "[4, 600] loss: 2111387.093\n",
            "[4, 700] loss: 2234158.189\n",
            "[5, 100] loss: 2220832.331\n",
            "[5, 200] loss: 2187321.455\n",
            "[5, 300] loss: 2230120.970\n",
            "[5, 400] loss: 2197343.676\n",
            "[5, 500] loss: 2226908.906\n",
            "[5, 600] loss: 2234050.874\n",
            "[5, 700] loss: 2103988.904\n",
            "[6, 100] loss: 2156758.579\n",
            "[6, 200] loss: 2359540.915\n",
            "[6, 300] loss: 2185332.515\n",
            "[6, 400] loss: 2106160.446\n",
            "[6, 500] loss: 2216521.520\n",
            "[6, 600] loss: 2171616.935\n",
            "[6, 700] loss: 2194977.073\n",
            "[7, 100] loss: 2238225.303\n",
            "[7, 200] loss: 2099264.209\n",
            "[7, 300] loss: 2202145.250\n",
            "[7, 400] loss: 2190894.698\n",
            "[7, 500] loss: 2262093.859\n",
            "[7, 600] loss: 2275667.772\n",
            "[7, 700] loss: 2170089.331\n",
            "[8, 100] loss: 2221385.356\n",
            "[8, 200] loss: 2315640.163\n",
            "[8, 300] loss: 2292618.589\n",
            "[8, 400] loss: 2048731.409\n",
            "[8, 500] loss: 2173808.321\n",
            "[8, 600] loss: 2160952.467\n",
            "[8, 700] loss: 2161772.246\n",
            "[9, 100] loss: 2224588.272\n",
            "[9, 200] loss: 2227878.116\n",
            "[9, 300] loss: 2114703.618\n",
            "[9, 400] loss: 2202404.349\n",
            "[9, 500] loss: 2112594.739\n",
            "[9, 600] loss: 2154422.728\n",
            "[9, 700] loss: 2277700.604\n",
            "[10, 100] loss: 2175359.853\n",
            "[10, 200] loss: 2178258.525\n",
            "[10, 300] loss: 2271562.857\n",
            "[10, 400] loss: 2206830.271\n",
            "[10, 500] loss: 2297768.624\n",
            "[10, 600] loss: 2163218.249\n",
            "[10, 700] loss: 2090212.530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "01xkjoaC3ddD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "Y5EN9cjiKQs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_predictions(image, predictions):\n",
        "    \"\"\"Visualize bounding boxes and class predictions\"\"\"\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
        "\n",
        "    # Assuming predictions are in the format [x1, y1, x2, y2, class_probabilities]\n",
        "    for pred in predictions:\n",
        "        x1, y1, x2, y2, conf, cls = pred[:4]\n",
        "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1 - 10, f'Class: {cls} Conf: {conf:.2f}', color='white', backgroundcolor='red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Testing function\n",
        "def test(model, test_loader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, _) in enumerate(test_loader):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            # Post-process the predictions and visualize\n",
        "            for image, output in zip(images, outputs):\n",
        "                visualize_predictions(image, output)\n",
        "\n",
        "# Load Pascal VOC dataset with resizing and the custom collate function\n",
        "test_dataset = datasets.VOCDetection('data/VOCdevkit/', year='2012', image_set='test',\n",
        "                                      download=True, transform=transform)\n",
        "\n",
        "# Get a small sample (e.g., 10 images)\n",
        "small_sample_dataset = torch.utils.data.Subset(test_dataset, indices=range(10))\n",
        "\n",
        "# DataLoader with custom collate_fn\n",
        "test_loader = DataLoader(small_sample_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "test(model, test_loader)"
      ],
      "metadata": {
        "id": "wr4GoGR1KYGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}