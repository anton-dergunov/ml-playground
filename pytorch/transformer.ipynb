{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on the following resources:\n",
    "- YouTube video \"[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)\"\n",
    "- GitHub project [nanogpt-lecture](https://github.com/karpathy/ng-video-lecture)\n",
    "- GitHub project [nanoGPT](https://github.com/karpathy/nanoGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x130b08bb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading Tiny Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!wget -O ~/datasets/tinyshakespeare.txt -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path.home () /  \"datasets\" / \"tinyshakespeare.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "print(''.join(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # maximum context length for predictions\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    ix = torch.randint(low=0, high=(len(data) - block_size), size=(batch_size,)) # batch_size random integers in [low, high)\n",
    "    x = torch.stack([data[i: (i + block_size)] for i in ix])\n",
    "    y = torch.stack([data[(i + 1): (i + block_size + 1)] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(5, (2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.tensor([1, 2]),\n",
    "             torch.tensor([3, 4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54],\n",
      "        [57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1]])\n",
      "Targets:\n",
      "tensor([[58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39],\n",
      "        [43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45]])\n",
      "When input is tensor([52]) the target is 58\n",
      "When input is tensor([52, 58]) the target is 1\n",
      "When input is tensor([52, 58,  1]) the target is 58\n",
      "When input is tensor([52, 58,  1, 58]) the target is 46\n",
      "When input is tensor([52, 58,  1, 58, 46]) the target is 39\n",
      "When input is tensor([52, 58,  1, 58, 46, 39]) the target is 58\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is 1\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is 46\n",
      "When input is tensor([25]) the target is 17\n",
      "When input is tensor([25, 17]) the target is 27\n",
      "When input is tensor([25, 17, 27]) the target is 10\n",
      "When input is tensor([25, 17, 27, 10]) the target is 0\n",
      "When input is tensor([25, 17, 27, 10,  0]) the target is 21\n",
      "When input is tensor([25, 17, 27, 10,  0, 21]) the target is 1\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is 54\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is 39\n",
      "When input is tensor([57]) the target is 43\n",
      "When input is tensor([57, 43]) the target is 60\n",
      "When input is tensor([57, 43, 60]) the target is 43\n",
      "When input is tensor([57, 43, 60, 43]) the target is 52\n",
      "When input is tensor([57, 43, 60, 43, 52]) the target is 1\n",
      "When input is tensor([57, 43, 60, 43, 52,  1]) the target is 63\n",
      "When input is tensor([57, 43, 60, 43, 52,  1, 63]) the target is 43\n",
      "When input is tensor([57, 43, 60, 43, 52,  1, 63, 43]) the target is 39\n",
      "When input is tensor([60]) the target is 43\n",
      "When input is tensor([60, 43]) the target is 42\n",
      "When input is tensor([60, 43, 42]) the target is 8\n",
      "When input is tensor([60, 43, 42,  8]) the target is 0\n",
      "When input is tensor([60, 43, 42,  8,  0]) the target is 25\n",
      "When input is tensor([60, 43, 42,  8,  0, 25]) the target is 63\n",
      "When input is tensor([60, 43, 42,  8,  0, 25, 63]) the target is 1\n",
      "When input is tensor([60, 43, 42,  8,  0, 25, 63,  1]) the target is 45\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(train_data)\n",
    "print(\"Inputs:\")\n",
    "print(xb)\n",
    "print(\"Targets:\")\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):     # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target  = yb[b, t]\n",
    "        print(f\"When input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        #Â https://medium.com/@gautam.e/what-is-nn-embedding-really-de038baadd24\n",
    "        # https://stackoverflow.com/questions/75646273/what-is-the-difference-nn-embedding-and-nn-linear\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size)\n",
    "\n",
    "    # This model only uses the previous character to predict the next character\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (Batch, Time) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (Batch, Time, Channel)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "tensor([-0.0896,  1.8414, -1.4726,  1.1064, -1.9390,  0.6582, -0.6924,  1.5158,\n",
      "         2.4418, -1.1210,  0.6197, -3.0786, -0.7115,  0.2381, -0.3451,  0.1705,\n",
      "         0.0774,  1.3466, -0.3217, -0.2189, -0.2735,  0.3476,  1.8535, -0.0375,\n",
      "        -1.1209,  0.9318,  0.3525,  0.0450, -0.3756,  0.3991, -0.6068, -0.3524,\n",
      "         0.3962, -0.6525,  0.6501,  0.1894, -1.0641,  0.6243,  0.5673, -0.2119,\n",
      "        -0.3794, -0.8791,  0.5142, -1.5793,  1.2666,  0.7957,  0.1870,  0.6083,\n",
      "         0.5449,  0.2654,  0.6035, -0.6983, -1.1202,  1.3071,  0.8063,  0.4136,\n",
      "        -0.4379, -0.6720,  1.4559, -3.2106, -0.5489,  0.1024,  1.6736, -0.3724,\n",
      "        -0.2800], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out.shape)\n",
    "print(out[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the function to generate the text using this model. The model accepts `idx` in the format (Batch, Time) tensor of integers. The output is (Batch, Time, Channel or Embedding Dimension). This corresponds to the next character to generate (we apply `softmax`). Then we treat every channel as probability to select corresponding next word, so we use `torch.multinomial(probs, num_samples=1)` to select next word according to the multinomial probability distribution. Also note that we pass the whole sequence each time to the model during generation even though we are using the last character only. This is just the structure that we will use later. Also, we update the model to return the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # reshape it, since `cross_entropy` expects in shape (N, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6064, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sasnHFNQgu,UZoLQT&guaHefMF?esA;ACyrwTfGTpiRXeio,v'JwNSH 3vPcZuoVMY?eLwUg, yG?fNFNWVLcuYoEUwsPM&\n",
      "?X!H\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist())) # produces garbage, since the model is randomly initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using AdamW optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4172263145446777\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # increasing batch size from the previous value of 4\n",
    "max_iters = 10000\n",
    "\n",
    "for steps in range(max_iters):\n",
    "    xb, yb = get_batch(train_data)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # TODO Is this required?\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Y: od thi. chilleswshesses,\n",
      "Tystus y st d:\n",
      "Myoo th at, wi&efryo he:\n",
      "\n",
      "Y:\n",
      "Thokete t esu t wid re g toomer.\n",
      "ty w witrteyr IXELINRofe\n",
      "\n",
      "Whosat t fixcoud\n",
      "\n",
      "Mad caliedeathou prtsupetha.\n",
      "Th y GLAN utresavis tr: al pord en mit in ves.\n",
      "me thquf nom lt igo\n",
      "The the yme o b,\n",
      "NGHAnotereder torll, S: enoulaimod G c\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=300)[0].tolist())) # produces something a bit more reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
