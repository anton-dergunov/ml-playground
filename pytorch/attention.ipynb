{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates attention mechanism. Code and images are from:\n",
    "\n",
    "http://www.peterbloem.nl/blog/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra and useful functions in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.bmm(X, Y)` performs a batch matrix-matrix product. Inputs must be 3D-tensors, first dimention is a batch.\n",
    "\n",
    "If X is a $(b * n * m)$ tensor and Y is a $(b * m * p)$ tensor, output is a $(b * n * p)$ tensor.\n",
    "\n",
    "https://pytorch.org/docs/stable/torch.html#torch.bmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 4)\n",
    "y = torch.randn(10, 4, 5)\n",
    "res = torch.bmm(x, y)\n",
    "res.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.transpose(input, dim0, dim1)` (or `X.transpose(dim0, dim1)`) returns a tensor which is a trasposed version of `input`. Dimentions `dim0` and `dim1` are swapped.\n",
    "\n",
    "https://pytorch.org/docs/stable/torch.html#torch.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6354, -0.3138,  1.5790],\n",
       "        [-0.9388,  1.3939,  0.5190]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6354, -0.9388],\n",
       "        [-0.3138,  1.3939],\n",
       "        [ 1.5790,  0.5190]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(x, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.functional.softmax(input, dim)` applies softmax along dimension `dim`.\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.functional.html#softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5387, -1.3863,  0.1120,  0.2355],\n",
       "         [-2.0100,  0.8054,  0.2112,  0.4322],\n",
       "         [-1.9613,  0.5597, -0.1515, -0.5779]],\n",
       "\n",
       "        [[ 1.2681, -0.1804, -0.3403, -1.1587],\n",
       "         [-2.9798,  1.0406, -0.5751, -0.0330],\n",
       "         [-0.0475,  1.7537,  3.5292,  1.2491]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1410, 0.2304, 0.6112, 0.8013],\n",
       "         [0.7251, 0.4415, 0.6870, 0.6143],\n",
       "         [0.1286, 0.2326, 0.0246, 0.1386]],\n",
       "\n",
       "        [[0.8590, 0.7696, 0.3888, 0.1987],\n",
       "         [0.2749, 0.5585, 0.3130, 0.3857],\n",
       "         [0.8714, 0.7674, 0.9754, 0.8614]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6800, 0.0590, 0.3481, 0.3758],\n",
       "         [0.1561, 0.5280, 0.3844, 0.4575],\n",
       "         [0.1639, 0.4130, 0.2675, 0.1666]],\n",
       "\n",
       "        [[0.7797, 0.0884, 0.0201, 0.0658],\n",
       "         [0.0111, 0.2998, 0.0159, 0.2029],\n",
       "         [0.2092, 0.6117, 0.9640, 0.7313]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1814, 0.0777, 0.3476, 0.3933],\n",
       "         [0.0260, 0.4347, 0.2400, 0.2993],\n",
       "         [0.0425, 0.5285, 0.2595, 0.1694]],\n",
       "\n",
       "        [[0.6564, 0.1542, 0.1314, 0.0580],\n",
       "         [0.0115, 0.6417, 0.1275, 0.2193],\n",
       "         [0.0215, 0.1303, 0.7695, 0.0787]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic self-attention\n",
    "\n",
    "Input vectors: $x_1, x_2,.. x_t$. Output vectors: $y_1, y_2,.. y_t$.\n",
    "\n",
    "Self attention operation is simply a weighted average over all input vectors:\n",
    "\n",
    "$$\n",
    "y_i = \\sum_j w_{ij} x_j\n",
    "$$\n",
    "\n",
    "The simplies way for weight $w_{ij}$ is the dot product to which we apply softmax:\n",
    "\n",
    "$$\n",
    "w'_{ij} = x_i^T x_j \\\\\n",
    "w_{ij} = \\frac{\\exp{w'_{ij}}}{\\sum_j \\exp{w'_{ij}}}\n",
    "$$\n",
    "\n",
    "<img src=\"self-attention.svg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input is a sequence of $t$ vectors of dimension $k$, minimatch dimension $b$: a $(b * t * k)$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2688,  0.3804, -1.7762,  0.8495],\n",
       "         [-0.1935, -0.3447, -0.3844,  0.7467],\n",
       "         [ 1.3795, -0.3551,  0.0151, -1.9090]],\n",
       "\n",
       "        [[-0.3196,  1.8688, -0.8605,  0.5735],\n",
       "         [-0.2754, -0.9110, -0.9624, -1.8642],\n",
       "         [ 1.0176, -2.2407, -0.6599,  1.0171]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.0935,  1.1339, -1.4127],\n",
       "         [ 1.1339,  0.8616, -1.5759],\n",
       "         [-1.4127, -1.5759,  5.6737]],\n",
       "\n",
       "        [[ 4.6638, -1.8554, -3.3616],\n",
       "         [-1.8554,  5.3073,  0.5000],\n",
       "         [-3.3616,  0.5000,  7.5263]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
    "raw_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    0.9471,     0.0491,     0.0038],\n",
       "         [    0.5470,     0.4166,     0.0364],\n",
       "         [    0.0008,     0.0007,     0.9985]],\n",
       "\n",
       "        [[    0.9982,     0.0015,     0.0003],\n",
       "         [    0.0008,     0.9911,     0.0081],\n",
       "         [    0.0000,     0.0009,     0.9991]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(raw_weights, dim=2)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2504,  0.3420, -1.7010,  0.8338],\n",
       "         [ 0.1166,  0.0516, -1.1312,  0.7063],\n",
       "         [ 1.3775, -0.3544,  0.0133, -1.9048]],\n",
       "\n",
       "        [[-0.3191,  1.8633, -0.8606,  0.5700],\n",
       "         [-0.2650, -0.9196, -0.9599, -1.8390],\n",
       "         [ 1.0164, -2.2395, -0.6602,  1.0146]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.bmm(weights, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2504,  0.3420, -1.7010,  0.8338],\n",
       "         [ 0.1166,  0.0516, -1.1312,  0.7063],\n",
       "         [ 1.3775, -0.3544,  0.0133, -1.9048]],\n",
       "\n",
       "        [[-0.3191,  1.8633, -0.8606,  0.5700],\n",
       "         [-0.2650, -0.9196, -0.9599, -1.8390],\n",
       "         [ 1.0164, -2.2395, -0.6602,  1.0146]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def basic_self_attention(x):\n",
    "    weights = F.softmax(torch.bmm(x, x.transpose(1, 2)), dim=2)\n",
    "    y = y = torch.bmm(weights, x)\n",
    "    return y\n",
    "\n",
    "basic_self_attention(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete self-attention\n",
    "\n",
    "To give self-attention controllable parameters, we compute:\n",
    "* query $q_i$\n",
    "* key $k_j$\n",
    "* value $v_j$\n",
    "\n",
    "This terminology is derived from key-value stores: we use query to find a key and use its value.\n",
    "\n",
    "To compute them, we use $k * k$ weight matrices $W_q, W_k, W_v$.\n",
    "\n",
    "$$\n",
    "q_i = W_q x_i \\\\\n",
    "k_j = W_k x_j \\\\\n",
    "v_j = W_v x_j \\\\\n",
    "w'_{ij} = q_i^T k_j \\\\\n",
    "w_{ij} = softmax(w'_{ij}) \\\\\n",
    "y_i = \\sum_j w_{ij} v_j\n",
    "$$\n",
    "\n",
    "<img src=\"key-query-value.svg\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_q = torch.randn(4, 4)\n",
    "w_k = torch.randn(4, 4)\n",
    "w_v = torch.randn(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9703,  1.9240,  3.1712,  1.9942],\n",
       "         [-0.2578,  1.1100,  0.6023,  0.8200],\n",
       "         [ 1.8106, -2.0853,  2.7100,  0.7022]],\n",
       "\n",
       "        [[ 0.1233,  0.7111, -0.6614, -0.8625],\n",
       "         [ 2.4325,  0.7309,  1.6502,  1.1887],\n",
       "         [ 0.1435,  1.2382,  4.8879,  3.7766]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.matmul(x, w_q)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.matmul(x, w_k)\n",
    "v = torch.matmul(x, w_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 13.8004,   7.0465,  -7.8591],\n",
       "         [  1.6687,   0.9556,   0.5353],\n",
       "         [ 13.7186,   6.7265, -12.4250]],\n",
       "\n",
       "        [[  0.0856,   2.1770,  -3.7426],\n",
       "         [  1.4583,   2.7894,  10.4175],\n",
       "         [  1.9852,  -7.4579,  25.9419]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "raw_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    0.9988,     0.0012,     0.0000],\n",
       "         [    0.5519,     0.2705,     0.1777],\n",
       "         [    0.9991,     0.0009,     0.0000]],\n",
       "\n",
       "        [[    0.1097,     0.8879,     0.0024],\n",
       "         [    0.0001,     0.0005,     0.9994],\n",
       "         [    0.0000,     0.0000,     1.0000]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(raw_weights, dim=2)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2683,  0.3796, -1.7746,  0.8493],\n",
       "         [ 0.3411,  0.0536, -1.0815,  0.3316],\n",
       "         [ 0.2684,  0.3798, -1.7749,  0.8494]],\n",
       "\n",
       "        [[-0.2772, -0.6093, -0.9505, -1.5900],\n",
       "         [ 1.0168, -2.2396, -0.6601,  1.0157],\n",
       "         [ 1.0176, -2.2407, -0.6599,  1.0171]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.bmm(weights, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
