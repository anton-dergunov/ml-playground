{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximal Marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you want to find the most relevant news articles about `London` from a dataset. Here’s an example dataset containing news article titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = [\n",
    "    # Culture\n",
    "    \"The Revival of Ancient Traditions in Modern Society\",\n",
    "    \"Exploring the Intersection of Art and Technology\",\n",
    "    \"A Deep Dive into Indigenous Music Around the World\",\n",
    "    \"How Street Art is Shaping Urban Culture\",\n",
    "    \"Cultural Festivals You Can't Miss This Year\",\n",
    "    \"The Impact of Globalization on Local Cultures\",\n",
    "    \"Art Exhibitions That Will Transform Your Perspective\",\n",
    "    \"The Evolution of Theatre in London\",\n",
    "    \"The Influence of Eastern Philosophy in Western Culture\",\n",
    "    \"Exploring London's Cultural Scene\",\n",
    "\n",
    "    # Weather\n",
    "    \"Understanding the Science Behind Extreme Weather Events\",\n",
    "    \"How Climate Change is Affecting Global Weather Patterns\",\n",
    "    \"Preparing for Hurricane Season: What You Need to Know\",\n",
    "    \"The Future of Weather Forecasting: Innovations and Challenges\",\n",
    "    \"Heatwaves: Causes, Effects, and Mitigation Strategies\",\n",
    "    \"The Impact of El Niño and La Niña on Global Weather\",\n",
    "    \"Winter Storms: Preparing for the Unexpected\",\n",
    "    \"How Urbanization Affects Local Weather Patterns\",\n",
    "    \"Weather Patterns Affecting London This Summer\",\n",
    "    \"How London's Weather Has Changed Over the Decades\",\n",
    "\n",
    "    # World News\n",
    "    \"Global Leaders Convene to Discuss Climate Action\",\n",
    "    \"The Economic Impacts of the Latest Trade Agreements\",\n",
    "    \"Technological Advancements in Developing Nations\",\n",
    "    \"Elections Around the World: Key Outcomes and Implications\",\n",
    "    \"The Role of Social Media in Modern Revolutions\",\n",
    "    \"Global Health Initiatives: Progress and Challenges\",\n",
    "    \"Diplomatic Tensions and Their Global Ramifications\",\n",
    "    \"The Growing Influence of Social Media on Photography\",\n",
    "    \"London's Role in Global Climate Talks\",\n",
    "    \"Brexit and Its Impact on London's Economy\",\n",
    "\n",
    "    # Programming Languages\n",
    "    \"Top 10 Programming Languages to Learn in 2024\",\n",
    "    \"How Python Became the Go-To Language for Data Science\",\n",
    "    \"The Growing Popularity of Rust in Systems Programming\",\n",
    "    \"Comparing Functional and Object-Oriented Programming Paradigms\",\n",
    "    \"The Impact of Open Source on Programming Language Development\",\n",
    "    \"The Role of Swift in Apple's Ecosystem\",\n",
    "    \"Emerging Programming Languages to Keep an Eye On\",\n",
    "    \"Python Tips for Mastering Data Science\",\n",
    "    \"London's Tech Scene and Programming Trends\",\n",
    "    \"How London's Startups are Using AI\",\n",
    "\n",
    "    # Photography News\n",
    "    \"The Intersection of Photography and Virtual Reality\",\n",
    "    \"How Drones are Revolutionizing Aerial Photography\",\n",
    "    \"The Best New Cameras and Lenses of the Year\",\n",
    "    \"Exploring the World of Underwater Photography\",\n",
    "    \"The Art of Portrait Photography: Techniques and Tips\",\n",
    "    \"The Role of Post-Processing in Modern Photography\",\n",
    "    \"The Intersection of Photography and Virtual Reality\",\n",
    "    \"How to Build a Professional Photography Portfolio\",\n",
    "    \"Exploring London's Best Photography Spots\",\n",
    "    \"The Best Photography Exhibitions in London\",\n",
    "\n",
    "    # Things to Do in London\n",
    "    \"Historic Landmarks You Can't Miss in London\",\n",
    "    \"Family-Friendly Activities in London\",\n",
    "    \"Family-Friendly Events in London\",\n",
    "    \"The Best Parks and Green Spaces in London\",\n",
    "    \"Unique Shopping Experiences in London\",\n",
    "    \"Day Trips from London: Exploring the Countryside\",\n",
    "    \"Cultural Festivals in London This Year\",\n",
    "    \"Top Photography Spots in London\",\n",
    "    \"London's Tech Scene and Programming Trends\",\n",
    "    \"Best Photo Spots in London\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To approach this problem, one method is to use a [similarity measure](https://en.wikipedia.org/wiki/Similarity_measure) to rank documents by their relevance to a query. A common measure is [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) applied to [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) of the documents and the query. The algorithm follows these steps:\n",
    "\n",
    "1. Create word embedding representation of the documents and the query.\n",
    "2. Compute cosine similarity between the representations of the query and each document.\n",
    "3. Select `N` documents most similar to the query.\n",
    "\n",
    "We can use the [transformers](https://huggingface.co/docs/transformers/en/index) library to leverage a pretrained language model (such as the classic `bert-base-uncased`) to generate text embeddings.\n",
    "\n",
    "This function generates embeddings by averaging the hidden states of the model’s last layer for each input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list, model_name=\"bert-base-uncased\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(text_list, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    embeddings = embeddings.numpy()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is calculated using the formula $\\text{cosine\\_similarity}(A, B) = \\frac{A * B}{\\|A\\| \\|B\\|}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_vector1 * norm_vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute cosine similarities between the query and each document, we can combine the query and documents into a single list and then compute similarities between each pair of embeddings (that would also get us similarities between each document, but we would need that later in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(documents, query):\n",
    "    quiery_and_documents = [query] + documents\n",
    "    embeddings = get_embeddings(quiery_and_documents)\n",
    "    num_embeddings = embeddings.shape[0]\n",
    "    similarities = np.zeros((num_embeddings, num_embeddings))\n",
    "\n",
    "    for i in range(num_embeddings):\n",
    "        for j in range(num_embeddings):\n",
    "            similarities[i, j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "\n",
    "    return similarities\n",
    "\n",
    "article_similarities = compute_similarities(article_titles, query=\"London\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, rank the documents based on their similarity to the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_relevance(similarities):\n",
    "    return sorted(range(len(similarities) - 1),\n",
    "                  key=lambda i: similarities[0, i+1],\n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_selected(ranking, similarities, entities):\n",
    "    for i in ranking:\n",
    "        print(f\"{similarities[0, i+1]}\\t{entities[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 7 relevant news articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7103888392448425\tFamily-Friendly Activities in London\n",
      "0.7099422812461853\tThe Best Photography Exhibitions in London\n",
      "0.7076156735420227\tUnique Shopping Experiences in London\n",
      "0.7006320357322693\tFamily-Friendly Events in London\n",
      "0.6981644034385681\tBest Photo Spots in London\n",
      "0.6938719153404236\tTop Photography Spots in London\n",
      "0.6879022121429443\tExploring London's Cultural Scene\n"
     ]
    }
   ],
   "source": [
    "article_similarity_order = similarity_relevance(article_similarities)\n",
    "print_selected(article_similarity_order[:7], article_similarities, article_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These articles are relevant to London, but some are duplicates:\n",
    "- \"Family-Friendly Activities in London\" and \"Family-Friendly Events in London\"\n",
    "- \"Best Photo Spots in London\" and \"Top Photography Spots in London\"\n",
    "\n",
    "In many cases, retrieving both relevant and diverse documents is essential. For instance, when using [Retrieval-Augmented Generation](https://www.promptingguide.ai/techniques/rag) (RAG) in LLMs, the context window is limited, so we prefer selecting text snippets that are relevant but not duplicates. The [LangChain](https://www.langchain.com/) framework supports RAG and provides the [Maximal Marginal Relevance](https://aclanthology.org/X98-1025/) (MMR) technique.\n",
    "\n",
    "A document has **high marginal relevance** if it is both *relevant* to the query and contains *minimal similarity to previously selected documents*. **MMR** is defined as:\n",
    "\n",
    "$$\n",
    "MMR \\overset{def}{=} \\underset{D_i \\in R \\setminus S}{argmax}\n",
    "      [ \\lambda * Sim_1(D_i,Q) -\n",
    "        (1-\\lambda) * \\underset{D_j \\in S}{max}(Sim_2(D_i, D_j)) ]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- C - document collection\n",
    "- Q - query\n",
    "- R - ranked list of documents retrieved by the IR system ($ R \\subseteq C $)\n",
    "- S - subset of documents in R already provided to the user ($ S \\subseteq C $)\n",
    "- R \\\\ S - subset of documents not yet offered to the user\n",
    "- $ \\lambda $ - hyperparameter to prefer more relevant or more diverse documents\n",
    "\n",
    "Let's implement this technique. First, we select the most relevant document. Then we iteratively select the document that gives the maximum MMR score (most relevant one and most dissimilar to the documents that we have already selected), until we select the requested number of documents.\n",
    "\n",
    "We'll reuse the previously computed similarity matrix and now use similarities between each pair of documents. The document-document similarity metric can differ from the query-document similarity, but for simplicity, we'll use cosine similarity as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_marginal_relevance(similarities, num_to_select, lambda_param):\n",
    "    if similarities.shape[0] <= 1 or num_to_select <= 0:\n",
    "        return []\n",
    "    \n",
    "    most_similar = np.argmax(similarities[0, 1:])\n",
    "\n",
    "    selected = [most_similar]\n",
    "    candidates = set(range(len(similarities) - 1))\n",
    "    candidates.remove(most_similar)\n",
    "\n",
    "    while (len(selected) < num_to_select):\n",
    "        if not candidates:\n",
    "            break\n",
    "\n",
    "        mmr_scores = {}\n",
    "        for i in candidates:\n",
    "            mmr_scores[i] = (lambda_param * similarities[i+1, 0] -\n",
    "                (1 - lambda_param) * max([similarities[i+1, j+1] for j in selected]))\n",
    "\n",
    "        next_best = max(mmr_scores, key=mmr_scores.get)\n",
    "        selected.append(next_best)\n",
    "        candidates.remove(next_best)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x15724ca50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestMaximalMarginalRelevance(unittest.TestCase):\n",
    "    def test_basic_case(self):\n",
    "        similarities = np.array([\n",
    "            [1, 0.8, 0.6],\n",
    "            [0.8, 1, 0.5],\n",
    "            [0.6, 0.5, 1]\n",
    "        ])\n",
    "        result = maximal_marginal_relevance(similarities, 2, 0.5)\n",
    "        self.assertEqual(result, [0, 1])\n",
    "\n",
    "    def test_single_selection(self):\n",
    "        similarities = np.array([\n",
    "            [1, 0.8, 0.9],\n",
    "            [0.8, 1, 0.7],\n",
    "            [0.9, 0.7, 1]\n",
    "        ])\n",
    "        result = maximal_marginal_relevance(similarities, 1, 0.5)\n",
    "        self.assertEqual(result, [1])\n",
    "\n",
    "    def test_all_selection(self):\n",
    "        similarities = np.array([\n",
    "            [1, 0.8, 0.6, 0.4],\n",
    "            [0.8, 1, 0.5, 0.2],\n",
    "            [0.6, 0.5, 1, 0.1],\n",
    "            [0.4, 0.1, 0.2, 1]\n",
    "        ])\n",
    "        result = maximal_marginal_relevance(similarities, 3, 0.5)\n",
    "        self.assertEqual(result, [0, 2, 1])\n",
    "\n",
    "    def test_lambda_param(self):\n",
    "        similarities = np.array([\n",
    "            [1, 0.8, 0.6, 0.4],\n",
    "            [0.8, 1, 0.5, 0.2],\n",
    "            [0.6, 0.5, 1, 0.1],\n",
    "            [0.4, 0.1, 0.2, 1]\n",
    "        ])\n",
    "        result = maximal_marginal_relevance(similarities, 3, 0.9)\n",
    "        self.assertEqual(result, [0, 1, 2])\n",
    "\n",
    "    def test_empty_selection(self):\n",
    "        similarities = np.array([\n",
    "            [1, 0.8, 0.6],\n",
    "            [0.8, 1, 0.5],\n",
    "            [0.6, 0.5, 1]\n",
    "        ])\n",
    "        result = maximal_marginal_relevance(similarities, 0, 0.5)\n",
    "        self.assertEqual(result, [])\n",
    "\n",
    "    def test_no_similarities(self):\n",
    "        similarities = np.empty(0)\n",
    "        result = maximal_marginal_relevance(similarities, 2, 0.5)\n",
    "        self.assertEqual(result, [])\n",
    "\n",
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the top 7 relevant news articles using the MMR technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7103888392448425\tFamily-Friendly Activities in London\n",
      "0.685788631439209\tThe Evolution of Theatre in London\n",
      "0.6981644034385681\tBest Photo Spots in London\n",
      "0.6072949767112732\tWeather Patterns Affecting London This Summer\n",
      "0.5159177780151367\tPython Tips for Mastering Data Science\n",
      "0.6396927237510681\tThe Best Parks and Green Spaces in London\n",
      "0.7076156735420227\tUnique Shopping Experiences in London\n"
     ]
    }
   ],
   "source": [
    "article_mmr_order = maximal_marginal_relevance(article_similarities,\n",
    "                                               num_to_select = 7,\n",
    "                                               lambda_param = .5)\n",
    "print_selected(article_mmr_order, article_similarities, article_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new set of articles does not contain duplicates. However, there is an article not quite related to our query: \"Python Tips for Mastering Data Science\". This issue can be mitigated by selecting a better $ \\lambda $ value.\n",
    "\n",
    "Referring to the MMR formula again:\n",
    "\n",
    "- $ \\lambda = 1 $: Computes incrementally the standard relevance-ranked list\n",
    "- $ \\lambda = 0 $: Computes a maximal diversity ranking among documents in R\n",
    "- $ \\lambda \\in [0,1] $: Optimizes a linear combination of both criteria\n",
    "\n",
    "By setting a higher value of $ \\lambda = .7 $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7103888392448425\tFamily-Friendly Activities in London\n",
      "0.685788631439209\tThe Evolution of Theatre in London\n",
      "0.7099422812461853\tThe Best Photography Exhibitions in London\n",
      "0.7076156735420227\tUnique Shopping Experiences in London\n",
      "0.6879022121429443\tExploring London's Cultural Scene\n",
      "0.6938719153404236\tTop Photography Spots in London\n",
      "0.6432080268859863\tDay Trips from London: Exploring the Countryside\n"
     ]
    }
   ],
   "source": [
    "article_mmr_order_07 = maximal_marginal_relevance(article_similarities,\n",
    "                                                  num_to_select = 7,\n",
    "                                                  lambda_param = .7)\n",
    "print_selected(article_mmr_order_07, article_similarities, article_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all articles are related to `London`, and there are no duplicates. Increasing $ \\lambda $ further to .8, then we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7103888392448425\tFamily-Friendly Activities in London\n",
      "0.7099422812461853\tThe Best Photography Exhibitions in London\n",
      "0.7076156735420227\tUnique Shopping Experiences in London\n",
      "0.685788631439209\tThe Evolution of Theatre in London\n",
      "0.6879022121429443\tExploring London's Cultural Scene\n",
      "0.6938719153404236\tTop Photography Spots in London\n",
      "0.6981644034385681\tBest Photo Spots in London\n"
     ]
    }
   ],
   "source": [
    "article_mmr_order_08 = maximal_marginal_relevance(article_similarities,\n",
    "                                                  num_to_select = 7,\n",
    "                                                  lambda_param = .8)\n",
    "print_selected(article_mmr_order_08, article_similarities, article_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all articles are related to `London`, but we have a duplicate pair: \"Top Photography Spots in London\" and \"Best Photo Spots in London\". For this example, $ \\lambda = .7 $ works well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
